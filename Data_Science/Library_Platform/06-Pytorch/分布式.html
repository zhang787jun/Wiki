<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>torch.distributed  - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science">Data_Science</a>&nbsp;»&nbsp;<a href="/Wiki/#-Library_Platform">Library_Platform</a>&nbsp;»&nbsp;<a href="/Wiki/#-06-Pytorch">06-Pytorch</a>&nbsp;»&nbsp;torch.distributed </div>
</div>
<div class="clearfix"></div>
<div id="title">torch.distributed </div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#filename-ptdistpy">filename 'ptdist.py'</a></li>
<li><a href="#use-address-of-one-of-the-machines">Use address of one of the machines</a></li>
<li><a href="#rank-should-always-be-specified">rank should always be specified</a></li>
</ul>
</div>
<p>概述<br />
torch.distributed 包支持</p>
<p>Pytorch 中通过 torch.distributed 包提供分布式支持，包括 GPU 和 CPU 的分布式训练支持。Pytorch 分布式目前只支持 Linux。</p>
<p>在此之前，torch.nn.DataParallel 已经提供数据并行的支持，但是其不支持多机分布式训练，且底层实现相较于 distributed 的接口，有些许不足。</p>
<p>torch.distributed 的优势如下：</p>
<div class="hlcode"><pre><span class="err">每个进程对应一个独立的训练过程，且只对梯度等少量数据进行信息交换。</span>
</pre></div>


<p>在每次迭代中，每个进程具有自己的 optimizer ，并独立完成所有的优化步骤，进程内与一般的训练无异。</p>
<p>在各进程梯度计算完成之后，各进程需要将梯度进行汇总平均，然后再由 rank=0 的进程，将其 broadcast 到所有进程。之后，各进程用该梯度来更新参数。</p>
<p>由于各进程中的模型，初始参数一致 (初始时刻进行一次 broadcast)，而每次用于更新参数的梯度也一致，因此，各进程的模型参数始终保持一致。</p>
<p>而在 DataParallel 中，全程维护一个 optimizer，对各 GPU 上梯度进行求和，而在主 GPU 进行参数更新，之后再将模型参数 broadcast 到其他 GPU。</p>
<p>相较于 DataParallel，torch.distributed 传输的数据量更少，因此速度更快，效率更高。</p>
<div class="hlcode"><pre><span class="err">每个进程包含独立的解释器和</span> <span class="n">GIL</span><span class="err">。</span>
</pre></div>


<p>由于每个进程拥有独立的解释器和 GIL，消除了来自单个 Python 进程中的多个执行线程，模型副本或 GPU 的额外解释器开销和 GIL-thrashing ，因此可以减少解释器和 GIL 使用冲突。这对于严重依赖 Python runtime 的 models 而言，比如说包含 RNN 层或大量小组件的 models 而言，这尤为重要。<br />
RingAllReduce VS TreeAllReduce</p>
<p>本节详细原理解释，参考知乎。</p>
<p>Pytorch 1.x 的多机多卡计算模型并没有采用主流的 Parameter Server 结构，而是直接用了Uber Horovod 的形式，也是百度开源的 RingAllReduce 算法。</p>
<p>采用 PS 计算模型的分布式，通常会遇到网络的问题，随着 worker 数量的增加，其加速比会迅速的恶化，需要借助其他辅助技术。</p>
<p>由于某一个 GPU 需要接收其他所有 GPU 的梯度，并求平均以及 broadcast 回去，若 GPU 数量越大时，通信成本也就越高。其基本架构如下图所示。</p>
<p>而 Uber 的 Horovod，采用的 RingAllReduce 的计算方案，其特点是网络单次通信量不随着 worker(GPU) 的增加而增加，是一个恒定值。</p>
<p>在 RingALll 中，GPU 集群被组织成一个逻辑环，每个 GPU 只从左邻居接受数据、并发送数据给右邻居，即每次同步每个 gpu 只获得部分梯度更新，等一个完整的 Ring 完成，每个 GPU 都获得了完整的参数。</p>
<p>与 TreeAllReduce 不同，RingAllreduce 算法的每次通信成本是恒定的，与系统中 gpu 的数量无关，完全由系统中 gpu 之间最慢的连接决定。</p>
<p>其基本的结构和算法原理如下所示：</p>
<p>如上图所示的结构，在 5 次迭代之后，所有的 GPU 更新为了值之和。<br />
Pytorch 分布式使用流程<br />
基本概念</p>
<p>下面是分布式系统中常用的一些概念：</p>
<div class="hlcode"><pre><span class="n">group</span><span class="err">：</span>
</pre></div>


<p>即进程组。默认情况下，只有一个组，一个 job 即为一个组，也即一个 world。</p>
<p>当需要进行更加精细的通信时，可以通过 new_group 接口，使用 word 的子集，创建新组，用于集体通信等。</p>
<div class="hlcode"><pre><span class="n">world</span> <span class="n">size</span> <span class="err">：</span>
</pre></div>


<p>表示全局进程个数。</p>
<div class="hlcode"><pre><span class="n">rank</span><span class="err">：</span>
</pre></div>


<p>表示进程序号，用于进程间通讯，表征进程优先级。rank = 0 的主机为 master 节点。</p>
<div class="hlcode"><pre><span class="n">local_rank</span><span class="err">：</span>
</pre></div>


<p>进程内，GPU 编号，非显式参数，由 torch.distributed.launch 内部指定。比方说， rank = 3，local_rank = 0 表示第 3 个进程内的第 1 块 GPU。<br />
基本使用流程</p>
<p>Pytorch 中分布式的基本使用流程如下：</p>
<div class="hlcode"><pre><span class="err">在使用</span> <span class="n">distributed</span> <span class="err">包的任何其他函数之前，需要使用</span> <span class="n">init_process_group</span> <span class="err">初始化进程组，同时初始化</span> <span class="n">distributed</span> <span class="err">包。如果需要进行小组内集体通信，用</span> <span class="n">new_group</span> <span class="err">创建子分组创建分布式并行模型</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="n">device_ids</span><span class="p">)</span><span class="err">为数据集创建</span> <span class="n">Sampler</span><span class="err">使用启动工具</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">launch</span> <span class="err">在每个主机上执行一次脚本，开始训练使用</span> <span class="n">destory_process_group</span><span class="p">()</span> <span class="err">销毁进程组</span>
</pre></div>


<p>使用模板</p>
<p>下面以 TCP 初始化方式为例，共 3 太主机，每台主机 2 块 GPU，进行分布式训练。<br />
TCP 初始化方式<br />
代码</p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="kn">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="c"># ......</span>
<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s">&#39;PyTorch distributed training on cifar-10&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&#39;--rank&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s">&#39;rank of current process&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&#39;--word_size&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s">&quot;word size&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&#39;--init_method&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">&#39;tcp://127.0.0.1:23456&#39;</span><span class="p">,</span>
                    <span class="n">help</span><span class="o">=</span><span class="s">&quot;init-method&quot;</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c"># ......</span>
<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">word_size</span><span class="p">)</span>

<span class="c"># ......</span>
<span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="n">download</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">trainset</span><span class="p">)</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>

<span class="c"># ......</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

<span class="err">执行方式</span>

<span class="c"># Node 1 : ip 192.168.1.201  port : 12345</span>
<span class="n">python</span> <span class="n">tcp_init</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">init_method</span> <span class="n">tcp</span><span class="p">:</span><span class="o">//</span><span class="mf">192.168</span><span class="o">.</span><span class="mf">1.201</span><span class="p">:</span><span class="mi">12345</span> <span class="o">--</span><span class="n">rank</span> <span class="mi">0</span> <span class="o">--</span><span class="n">word_size</span> <span class="mi">3</span>

<span class="c"># Node 2 : </span>
<span class="n">python</span> <span class="n">tcp_init</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">init_method</span> <span class="n">tcp</span><span class="p">:</span><span class="o">//</span><span class="mf">192.168</span><span class="o">.</span><span class="mf">1.201</span><span class="p">:</span><span class="mi">12345</span> <span class="o">--</span><span class="n">rank</span> <span class="mi">1</span> <span class="o">--</span><span class="n">word_size</span> <span class="mi">3</span>

<span class="c"># Node 3 : </span>
<span class="n">python</span> <span class="n">tcp_init</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">init_method</span> <span class="n">tcp</span><span class="p">:</span><span class="o">//</span><span class="mf">192.168</span><span class="o">.</span><span class="mf">1.201</span><span class="p">:</span><span class="mi">12345</span> <span class="o">--</span><span class="n">rank</span> <span class="mi">2</span> <span class="o">--</span><span class="n">word_size</span> <span class="mi">3</span>
</pre></div>


<p>说明</p>
<div class="hlcode"><pre><span class="err">在</span> <span class="n">TCP</span> <span class="err">方式中，在</span> <span class="n">init_process_group</span> <span class="err">中必须手动指定以下参数</span>
<span class="n">rank</span> <span class="err">为当前进程的进程号</span>
<span class="n">word_size</span> <span class="err">为当前</span> <span class="n">job</span> <span class="err">的总进程数</span><span class="n">init_method</span> <span class="err">内指定</span> <span class="n">tcp</span> <span class="err">模式，且所有进程的</span> <span class="n">ip</span><span class="o">:</span><span class="n">port</span> <span class="err">必须一致，设定为主进程的</span> <span class="n">ip</span><span class="o">:</span><span class="n">port</span>
<span class="err">必须在</span> <span class="n">rank</span><span class="o">==</span><span class="mi">0</span> <span class="err">的进程内保存参数。</span>
<span class="err">若程序内未根据</span> <span class="n">rank</span> <span class="err">设定当前进程使用的</span> <span class="n">GPUs</span><span class="err">，则默认使用全部</span> <span class="n">GPU</span><span class="err">，且以数据并行的方式使用。</span>
<span class="err">每条命令表示一个进程。若已开启的进程未达到</span> <span class="n">word_size</span> <span class="err">的数量，则所有进程会一直等待</span>
<span class="err">每台主机上可以开启多个进程。但是，若未为每个进程分配合适的</span> <span class="n">GPU</span><span class="err">，则同机不同进程可能会共用</span> <span class="n">GPU</span><span class="err">，应该坚决避免这种情况。</span>
<span class="err">使用</span> <span class="n">gloo</span> <span class="err">后端进行</span> <span class="n">GPU</span> <span class="err">训练时，会报错。</span>
<span class="err">若每个进程负责多块</span> <span class="n">GPU</span><span class="err">，可以利用多</span> <span class="n">GPU</span> <span class="err">进行模型并行。如下所示：</span>
</pre></div>


<div class="hlcode"><pre><span class="k">class</span> <span class="nc">ToyMpModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dev0</span><span class="p">,</span> <span class="n">dev1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ToyMpModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dev0</span> <span class="o">=</span> <span class="n">dev0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dev1</span> <span class="o">=</span> <span class="n">dev1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dev1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
       <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dev0</span><span class="p">)</span>
       <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
       <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dev1</span><span class="p">)</span>
       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="o">......</span>
<span class="n">dev0</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">dev1</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">mp_model</span> <span class="o">=</span> <span class="n">ToyMpModel</span><span class="p">(</span><span class="n">dev0</span><span class="p">,</span> <span class="n">dev1</span><span class="p">)</span>
<span class="n">ddp_mp_model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">mp_model</span><span class="p">)</span>
<span class="o">......</span>
</pre></div>


<p>Env 初始化方式<br />
代码</p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="kn">as</span> <span class="nn">dist</span>
<span class="kn">import</span> <span class="nn">torch.utils.data.distributed</span>

<span class="c"># ......</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="c"># 注意这个参数，必须要以这种形式指定，即使代码中不使用。因为 launch 工具默认传递该参数</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&quot;--local_rank&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

<span class="c"># ......</span>
<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s">&#39;env://&#39;</span><span class="p">)</span>

<span class="c"># ......</span>
<span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="n">download</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">trainset</span><span class="p">)</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>

<span class="c"># ......</span>
<span class="c"># 根据 local_rank，配置当前进程使用的 GPU</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>
</pre></div>


<p>执行方式</p>
<div class="hlcode"><pre><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="mi">2</span> <span class="o">--</span><span class="n">nnodes</span><span class="o">=</span><span class="mi">3</span> <span class="o">--</span><span class="n">node_rank</span><span class="o">=</span><span class="mi">0</span> <span class="o">--</span><span class="n">master_addr</span><span class="o">=</span><span class="s">&quot;192.168.1.201&quot;</span> <span class="o">--</span><span class="n">master_port</span><span class="o">=</span><span class="mi">23456</span> <span class="n">env_init</span><span class="o">.</span><span class="n">py</span>

<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="mi">2</span> <span class="o">--</span><span class="n">nnodes</span><span class="o">=</span><span class="mi">3</span> <span class="o">--</span><span class="n">node_rank</span><span class="o">=</span><span class="mi">1</span> <span class="o">--</span><span class="n">master_addr</span><span class="o">=</span><span class="s">&quot;192.168.1.201&quot;</span> <span class="o">--</span><span class="n">master_port</span><span class="o">=</span><span class="mi">23456</span> <span class="n">env_init</span><span class="o">.</span><span class="n">py</span>

<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="mi">2</span> <span class="o">--</span><span class="n">nnodes</span><span class="o">=</span><span class="mi">3</span> <span class="o">--</span><span class="n">node_rank</span><span class="o">=</span><span class="mi">2</span> <span class="o">--</span><span class="n">master_addr</span><span class="o">=</span><span class="s">&quot;192.168.1.201&quot;</span> <span class="o">--</span><span class="n">master_port</span><span class="o">=</span><span class="mi">23456</span> <span class="n">env_init</span><span class="o">.</span><span class="n">py</span>
</pre></div>


<p>说明</p>
<div class="hlcode"><pre><span class="err">在</span> <span class="n">Env</span> <span class="err">方式中，在</span> <span class="n">init_process_group</span> <span class="err">中，无需指定任何参数必须在</span> <span class="n">rank</span><span class="o">==</span><span class="mi">0</span> <span class="err">的进程内保存参数。该方式下，使用</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">launch</span> <span class="err">在每台主机上，为其创建多进程，其中</span><span class="o">:</span><span class="n">nproc_per_node</span> <span class="err">参数指定为当前主机创建的进程数。一般设定为当前主机的</span> <span class="n">GPU</span> <span class="err">数量</span><span class="n">nnodes</span> <span class="err">参数指定当前</span> <span class="n">job</span> <span class="err">包含多少个节点</span><span class="n">node_rank</span> <span class="err">指定当前节点的优先级</span><span class="n">master_addr</span> <span class="err">和</span> <span class="n">master_port</span> <span class="err">分别指定</span> <span class="n">master</span> <span class="err">节点的</span> <span class="n">ip</span><span class="o">:</span><span class="n">port</span><span class="err">若没有为每个进程合理分配</span> <span class="n">GPU</span><span class="err">，则默认使用当前主机上所有的</span> <span class="n">GPU</span><span class="err">。即使一台主机上有多个进程，也会共用</span> <span class="n">GPU</span><span class="err">。使用</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">launch</span> <span class="err">工具时，将会为当前主机创建</span> <span class="n">nproc_per_node</span> <span class="err">个进程，每个进程独立执行训练脚本。同时，它还会为每个进程分配一个</span> <span class="n">local_rank</span> <span class="err">参数，表示当前进程在当前主机上的编号。例如：</span><span class="n">rank</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">local_rank</span><span class="o">=</span><span class="mi">0</span> <span class="err">表示第</span> <span class="mi">3</span> <span class="err">个节点上的第</span> <span class="mi">1</span> <span class="err">个进程。需要合理利用</span> <span class="n">local_rank</span> <span class="err">参数，来合理分配本地的</span> <span class="n">GPU</span> <span class="err">资源每条命令表示一个进程。若已开启的进程未达到</span> <span class="n">word_size</span> <span class="err">的数量，则所有进程会一直等待</span>
</pre></div>


<p>进程组<br />
初始化进程组<br />
init_process_group<br />
函数原型</p>
<p>torch.distributed.init_process_group(backend, <br />
                                     init_method=None, <br />
                                     timeout=datetime.timedelta(0, 1800), <br />
                                     world_size=-1, <br />
                                     rank=-1, <br />
                                     store=None)</p>
<p>函数作用</p>
<p>该函数需要在每个进程中进行调用，用于初始化该进程。在使用分布式时，该函数必须在 distributed 内所有相关函数之前使用。<br />
参数详解</p>
<div class="hlcode"><pre><span class="n">backend</span> <span class="err">：指定当前进程要使用的通信后端</span>
</pre></div>


<p>小写字符串，支持的通信后端有 gloo，mpi，nccl 。建议用 nccl。</p>
<div class="hlcode"><pre><span class="n">init_method</span> <span class="err">：</span> <span class="err">指定当前进程组初始化方式</span>
</pre></div>


<p>可选参数，字符串形式。如果未指定 init_method 及 store，则默认为 env://，表示使用读取环境变量的方式进行初始化。该参数与 store 互斥。</p>
<div class="hlcode"><pre><span class="n">rank</span> <span class="err">：</span> <span class="err">指定当前进程的优先级</span>
</pre></div>


<p>int 值。表示当前进程的编号，即优先级。如果指定 store 参数，则必须指定该参数。</p>
<p>rank=0 的为主进程，即 master 节点。</p>
<div class="hlcode"><pre><span class="n">world_size</span> <span class="err">：</span>
</pre></div>


<p>该 job 中的总进程数。如果指定 store 参数，则需要指定该参数。</p>
<div class="hlcode"><pre><span class="n">timeout</span> <span class="err">：</span> <span class="err">指定每个进程的超时时间</span>
</pre></div>


<p>可选参数，datetime.timedelta 对象，默认为 30 分钟。该参数仅用于 Gloo 后端。</p>
<div class="hlcode"><pre><span class="n">store</span>
</pre></div>


<p>所有 worker 可访问的 key / value，用于交换连接 / 地址信息。与 init_method 互斥。<br />
new_group<br />
函数声明</p>
<p>torch.distributed.new_group(ranks=None, <br />
                            timeout=datetime.timedelta(0, 1800), <br />
                            backend=None)</p>
<p>函数作用</p>
<p>new_group() 函数可用于使用所有进程的任意子集来创建新组。其返回一个分组句柄，可作为 collectives 相关函数的 group 参数 。collectives 是分布式函数，用于特定编程模式中的信息交换。<br />
参数详解</p>
<div class="hlcode"><pre><span class="n">ranks</span><span class="err">：指定新分组内的成员的</span> <span class="n">ranks</span> <span class="err">列表</span>
</pre></div>


<p>list ，其中每个元素为 int 型</p>
<div class="hlcode"><pre><span class="n">timeout</span><span class="err">：指定该分组进程组内的操作的超时时间</span>
</pre></div>


<p>可选参数，datetime.timedelta 对象，默认为 30 分钟。该参数仅用于 Gloo 后端。</p>
<div class="hlcode"><pre><span class="n">backend</span><span class="err">：指定要使用的通信后端</span>
</pre></div>


<p>小写字符串，支持的通信后端有 gloo，nccl ，必须与 init_process_group() 中一致。<br />
获取进程组属性<br />
get_backend</p>
<p>原型</p>
<p>torch.distributed.get_backend(group=<object>)</p>
<p>函数说明</p>
<p>返回给定进程组的 backend。</p>
<p>参数</p>
<div class="hlcode"><pre><span class="n">group</span><span class="err">：要获取信息的进程组。</span>
</pre></div>


<p>进程组对象，默认为主进程组。如果指定另一个进程组，则调用该函数的进程必须为所指定的进程组的进程。</p>
<p>返回</p>
<div class="hlcode"><pre><span class="err">给定进程组的后端，以小写字符串的形式给出</span>
</pre></div>


<p>get_rank</p>
<p>函数原型</p>
<p>torch.distributed.get_rank(group=<object>)</p>
<p>函数说明</p>
<p>返回当前进程的 rank。</p>
<p>rank 是赋值给一个分布式进程组组内的每个进程的唯一识别。一般而言，rank 均为从 0 到 world_size 的整数。</p>
<p>参数</p>
<div class="hlcode"><pre><span class="n">group</span>
</pre></div>


<p>要获取信息的进程组对象，默认为主进程组。如果指定另一个进程组，则调用该函数的进程必须为所指定的进程组的进程。<br />
get_world_size</p>
<p>函数原型</p>
<p>torch.distributed.get_world_size(group=<object>)</p>
<p>函数说明</p>
<p>返回当前进程组内的进程数。</p>
<p>参数</p>
<div class="hlcode"><pre><span class="n">group</span>
</pre></div>


<p>要获取信息的进程组对象，默认为主进程组。如果指定另一个进程组，则调用该函数的进程必须为所指定的进程组的进程。<br />
is_initialized</p>
<p>函数原型</p>
<p>torch.distributed.is_initialized()</p>
<p>函数说明</p>
<p>检查默认进程组是否被初始化。<br />
is_mpi_available</p>
<p>函数原型</p>
<p>torch.distributed.is_mpi_available()</p>
<p>函数作用</p>
<p>检查 MPI 后端是否可用。<br />
is_nccl_available</p>
<p>函数原型</p>
<p>torch.distributed.is_nccl_available()</p>
<p>函数原型</p>
<p>检查 NCCL 后端是否可用。<br />
通信后端<br />
概述</p>
<p>使用分布式时，在梯度汇总求平均的过程中，各主机之间需要进行通信。因此，需要指定通信的协议架构等。torch.distributed 对其进行了封装。</p>
<p>torch.distributed 支持 3 种后端，分别为 NCCL，Gloo，MPI。各后端对 CPU / GPU 的支持如下所示：<br />
各种后端<br />
Gloo 后端</p>
<p>gloo 后端支持 CPU 和 GPU，其支持集体通信（collective Communication），并对其进行了优化。</p>
<p>由于 GPU 之间可以直接进行数据交换，而无需经过 CPU 和内存，因此，在 GPU 上使用 gloo 后端速度更快。</p>
<p>torch.distributed 对 gloo 提供原生支持，无需进行额外操作。<br />
NCCL 后端</p>
<p>NCCL 的全称为 Nvidia 聚合通信库（NVIDIA Collective Communications Library），是一个可以实现多个 GPU、多个结点间聚合通信的库，在 PCIe、Nvlink、InfiniBand 上可以实现较高的通信速度。</p>
<p>NCCL 高度优化和兼容了 MPI，并且可以感知 GPU 的拓扑，促进多 GPU 多节点的加速，最大化 GPU 内的带宽利用率，所以深度学习框架的研究员可以利用 NCCL 的这个优势，在多个结点内或者跨界点间可以充分利用所有可利用的 GPU。</p>
<p>NCCL 对 CPU 和 GPU 均有较好支持，且 torch.distributed 对其也提供了原生支持。</p>
<p>对于每台主机均使用多进程的情况，使用 NCCL 可以获得最大化的性能。每个进程内，不许对其使用的 GPUs 具有独占权。若进程之间共享 GPUs 资源，则可能导致 deadlocks。<br />
MPI 后端</p>
<p>MPI 即消息传递接口（Message Passing Interface），是一个来自于高性能计算领域的标准的工具。它支持点对点通信以及集体通信，并且是 torch.distributed 的 API 的灵感来源。使用 MPI 后端的优势在于，在大型计算机集群上，MPI 应用广泛，且高度优化。</p>
<p>但是，torch.distributed 对 MPI 并不提供原生支持。因此，要使用 MPI，必须从源码编译 Pytorch。是否支持 GPU，视安装的 MPI 版本而定。<br />
编译步骤</p>
<div class="hlcode"><pre><span class="err">创建并激活</span> <span class="n">Anaconda</span> <span class="err">环境，安装</span> <span class="n">the</span> <span class="n">guide</span> <span class="err">指定的依赖包，但是此时还不能运行</span> <span class="n">python</span> <span class="n">setup</span><span class="p">.</span><span class="n">py</span> <span class="n">install</span><span class="err">选择并安装偏好的</span> <span class="n">MPI</span> <span class="err">实现。需要注意的是，开启</span> <span class="n">CUDA</span><span class="o">-</span><span class="n">aware</span> <span class="n">MPI</span> <span class="err">可能需要一些额外的步骤。可以使用不提供</span> <span class="n">GPU</span> <span class="err">支持的</span> <span class="n">Open</span><span class="o">-</span><span class="n">MPI</span><span class="err">：</span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forgeopenmpi</span><span class="err">进入</span> <span class="n">Pytorch</span> <span class="err">源码，执行</span> <span class="n">python</span> <span class="n">setup</span><span class="p">.</span><span class="n">py</span> <span class="n">install</span>
</pre></div>


<p>使用实例</p>
<p>源码</p>
<h1 id="filename-ptdistpy">filename 'ptdist.py'</h1>
<p>import torch<br />
import torch.distributed as dist</p>
<p>def main(rank, world):<br />
    if rank == 0:<br />
        x = torch.tensor([1., -1.]) # Tensor of interest<br />
        dist.send(x, dst=1)<br />
        print('Rank-0 has sent the following tensor to Rank-1')<br />
        print(x)<br />
    else:<br />
        z = torch.tensor([0., 0.]) # A holder for recieving the tensor<br />
        dist.recv(z, src=0)<br />
        print('Rank-1 has recieved the following tensor from Rank-0')<br />
        print(z)</p>
<p>if <strong>name</strong> == '<strong>main</strong>':<br />
    dist.init_process_group(backend='mpi')<br />
    main(dist.get_rank(), dist.get_world_size())</p>
<p>执行</p>
<p>$ mpiexec -n 2 -ppn 1 -hosts miriad2a,miriad2b python ptdist.py</p>
<p>结果</p>
<p>Rank-0 has sent the following tensor to Rank-1<br />
 tensor([ 1., -1.])<br />
 Rank-1 has recieved the following tensor from Rank-0<br />
 tensor([ 1., -1.])</p>
<p>如何选择</p>
<div class="hlcode"><pre><span class="err">强烈建议：</span>
<span class="n">NCCL</span> <span class="err">是目前最快的后端，且对多进程分布式（</span><span class="n">Multi</span><span class="o">-</span><span class="n">Process</span> <span class="n">Single</span><span class="o">-</span><span class="n">GPU</span><span class="err">）支持极好，可用于单节点以及多节点的分布式训练。</span>
<span class="err">节点即主机。即使是单节点，由于底层机制不同，</span><span class="n">distributed</span> <span class="err">也比</span> <span class="n">DataParallel</span> <span class="err">方式要高效。</span>
</pre></div>


<p>基本原则：</p>
<div class="hlcode"><pre><span class="err">用</span> <span class="n">NCCL</span> <span class="err">进行分布式</span> <span class="n">GPU</span> <span class="err">训练用</span> <span class="n">Gloo</span> <span class="err">进行分布式</span> <span class="n">CPU</span> <span class="err">训练</span>
</pre></div>


<p>无限带宽互联的 GPU 集群</p>
<div class="hlcode"><pre><span class="err">使用</span> <span class="n">NCCL</span><span class="err">，因为它是目前唯一支持</span> <span class="n">InfiniBand</span> <span class="err">和</span> <span class="n">GPUDirect</span> <span class="err">的后端</span>
</pre></div>


<p>无限带宽和 GPU 直连</p>
<div class="hlcode"><pre><span class="err">使用</span> <span class="n">NCCL</span><span class="err">，因为其目前提供最佳的分布式</span> <span class="n">GPU</span> <span class="err">训练性能。尤其是</span> <span class="n">multiprocess</span> <span class="n">single</span><span class="o">-</span><span class="n">node</span> <span class="err">或</span> <span class="n">multi</span><span class="o">-</span><span class="n">node</span> <span class="n">distributed</span> <span class="err">训练。如果用</span> <span class="n">NCCL</span> <span class="err">训练有问题，再考虑使用</span> <span class="n">Cloo</span><span class="err">。</span><span class="p">(</span><span class="err">当前，</span><span class="n">Gloo</span> <span class="err">在</span> <span class="n">GPU</span> <span class="err">分布式上，相较于</span> <span class="n">NCCL</span> <span class="err">慢</span><span class="p">)</span>
</pre></div>


<p>无限带宽互联的 CPU 集群</p>
<div class="hlcode"><pre><span class="err">如果</span> <span class="n">InfiniBand</span> <span class="err">对</span> <span class="n">IB</span> <span class="err">启用</span> <span class="n">IP</span><span class="err">，请使用</span> <span class="n">Gloo</span><span class="err">，否则使使用</span> <span class="n">MPI</span><span class="err">。在未来将添加</span> <span class="n">infiniBand</span> <span class="err">对</span> <span class="n">Gloo</span> <span class="err">的支持</span>
</pre></div>


<p>以太网互联的 CPU 集群</p>
<div class="hlcode"><pre><span class="err">使用</span> <span class="n">Gloo</span><span class="err">，除非有特别的原因使用</span> <span class="n">MPI</span><span class="err">。</span>
</pre></div>


<p>初始化方式</p>
<p>分布式任务中，各节点之间需要进行协作，比如说控制数据同步等。因此，需要进行初始化，指定协作方式，同步规则等。</p>
<p>torch.distributed 提供了 3 种初始化方式，分别为 tcp、共享文件 和 环境变量初始化 等。<br />
TCP 初始化</p>
<p>代码</p>
<p>TCP 方式初始化，需要指定进程 0 的 ip 和 port。这种方式需要手动为每个进程指定进程号。</p>
<p>import torch.distributed as dist</p>
<h1 id="use-address-of-one-of-the-machines">Use address of one of the machines</h1>
<p>dist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',<br />
                        rank=args.rank, world_size=4)</p>
<p>说明</p>
<p>不同进程内，均使用主进程的 ip 地址和 port，确保每个进程能够通过一个 master 进行协作。该 ip 一般为主进程所在的主机的 ip，端口号应该未被其他应用占用。</p>
<p>实际使用时，在每个进程内运行代码，并需要为每一个进程手动指定一个 rank，进程可以分布与相同或不同主机上。</p>
<p>多个进程之间，同步进行。若其中一个出现问题，其他的也马上停止。</p>
<p>使用</p>
<p>Node 1</p>
<p>python mnsit.py --init-method tcp://192.168.54.179:22225 --rank 0 --world-size 2</p>
<p>Node 2</p>
<p>python mnsit.py --init-method tcp://192.168.54.179:22225 --rank 1 --world-size 2</p>
<p>底层实现</p>
<p>在深入探讨初始化算法之前，先从 C/C++ 层面，大致浏览一下 init_process_group 背后发生了什么。</p>
<div class="hlcode"><pre><span class="err">解析并验证参数后端通过</span> <span class="n">name2channel</span><span class="p">.</span><span class="n">at</span><span class="p">()</span> <span class="err">函数进行解析，返回一个</span> <span class="n">channel</span> <span class="err">类，将用于执行数据传输丢弃</span> <span class="n">GIL</span><span class="err">，并调用</span> <span class="n">THDProcessGroupInit</span><span class="p">()</span> <span class="err">函数，其实例化该</span> <span class="n">channel</span><span class="err">，并添加</span> <span class="n">master</span> <span class="err">节点的地址</span><span class="n">rank</span> <span class="mi">0</span> <span class="err">对应的进程将会执行</span> <span class="n">master</span> <span class="err">过程，而其他的进程则作为</span> <span class="n">workersmaster</span><span class="err">为所有的</span> <span class="n">worker</span> <span class="err">创建</span> <span class="n">sockets</span><span class="err">等待所有的</span> <span class="n">worker</span> <span class="err">连接发送给他们所有其他进程的位置每一个</span> <span class="n">worker</span><span class="err">创建连接</span> <span class="n">master</span> <span class="err">的</span> <span class="n">sockets</span><span class="err">发送自己的位置信息接受其他</span> <span class="n">workers</span> <span class="err">的信息打开一个新的</span> <span class="n">socket</span><span class="err">，并与其他</span> <span class="n">wokers</span> <span class="err">进行握手信号初始化结束，所有的进程之间相互连接</span>
</pre></div>


<p>共享文件系统初始化</p>
<p>该初始化方式，要求共享的文件对于组内所有进程可见！</p>
<p>代码</p>
<p>设置方式如下：</p>
<p>import torch.distributed as dist</p>
<h1 id="rank-should-always-be-specified">rank should always be specified</h1>
<p>dist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',<br />
                        world_size=4, rank=args.rank)</p>
<p>说明</p>
<p>其中，以 file:// 为前缀，表示文件系统各式初始化。/mnt/nfs/sharedfile 表示共享的文件，各个进程在共享文件系统中通过该文件进行同步或异步。因此，所有进程必须对该文件具有读写权限。</p>
<p>每一个进程将会打开这个文件，写入自己的信息，并等待直到其他所有进程完成该操作。在此之后，所有的请求信息将会被所有的进程可访问，为了避免 race conditions，文件系统必须支持通过 fcntl 锁定（大多数的 local 系统和 NFS 均支持该特性）。</p>
<p>说明：若指定为同一文件，则每次训练开始之前，该文件必须手动删除，但是文件所在路径必须存在！</p>
<div class="hlcode"><pre><span class="err">与</span> <span class="n">tcp</span> <span class="err">初始化方式一样，也需要为每一个进程手动指定</span> <span class="n">rank</span><span class="err">。</span>
</pre></div>


<p>使用</p>
<p>在主机 01 上：</p>
<p>python mnsit.py --init-method file://PathToShareFile/MultiNode --rank 0 --world-size 2</p>
<p>在主机 02 上：</p>
<p>python mnsit.py --init-method file://PathToShareFile/MultiNode --rank 1 --world-size 2</p>
<p>这里相比于 TCP 的方式麻烦一点的是运行完一次必须更换共享的文件名，或者删除之前的共享文件，不然第二次运行会报错。<br />
环境变量初始化</p>
<p>默认情况下使用的都是环境变量来进行分布式通信，也就是指定 init_method="env://"。通过在所有机器上设置如下四个环境变量，所有的进程将会适当的连接到 master，获取其他进程的信息，并最终与它们握手(信号)。</p>
<div class="hlcode"><pre><span class="n">MASTER_PORT</span><span class="o">:</span> <span class="err">必须指定，表示</span> <span class="n">rank0</span><span class="err">上机器的一个空闲端口（必须设置）</span><span class="n">MASTER_ADDR</span><span class="o">:</span> <span class="err">必须指定，除了</span> <span class="n">rank0</span> <span class="err">主机，表示主进程</span> <span class="n">rank0</span> <span class="err">机器的地址（必须设置）</span><span class="n">WORLD_SIZE</span><span class="o">:</span> <span class="err">可选，总进程数，可以这里指定，在</span> <span class="n">init</span> <span class="err">函数中也可以指定</span><span class="n">RANK</span><span class="o">:</span> <span class="err">可选，当前进程的</span> <span class="n">rank</span><span class="err">，也可以在</span> <span class="n">init</span> <span class="err">函数中指定</span>
</pre></div>


<p>配合 torch.distribution.launch 使用。</p>
<p>使用实例</p>
<p>Node 1: (IP: 192.168.1.1, and has a free port: 1234)</p>
<blockquote>
<blockquote>
<blockquote>
<p>python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE<br />
           --nnodes=2 --node_rank=0 --master_addr="192.168.1.1"<br />
           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3<br />
           and all other arguments of your training script)</p>
</blockquote>
</blockquote>
</blockquote>
<p>Node 2</p>
<blockquote>
<blockquote>
<blockquote>
<p>python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE<br />
           --nnodes=2 --node_rank=1 --master_addr="192.168.1.1"<br />
           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3<br />
           and all other arguments of your training script)</p>
</blockquote>
</blockquote>
</blockquote>
<p>Distributed Modules<br />
DistributedDataParallel<br />
原型</p>
<p>torch.nn.parallel.DistributedDataParallel(module, <br />
                                          device_ids=None, <br />
                                          output_device=None, <br />
                                          dim=0, <br />
                                          broadcast_buffers=True, <br />
                                          process_group=None, <br />
                                          bucket_cap_mb=25, <br />
                                          find_unused_parameters=False, <br />
                                          check_reduction=False)</p>
<p>功能</p>
<p>将给定的 module 进行分布式封装， 其将输入在 batch 维度上进行划分，并分配到指定的 devices 上。</p>
<p>module 会被复制到每台机器的每个 GPU 上，每一个模型的副本处理输入的一部分。</p>
<p>在反向传播阶段，每个机器的每个 GPU 上的梯度进行汇总并求平均。与 DataParallel 类似，batch size 应该大于 GPU 总数。<br />
参数解析</p>
<div class="hlcode"><pre><span class="n">module</span>
</pre></div>


<p>要进行分布式并行的 module，一般为完整的 model</p>
<div class="hlcode"><pre><span class="n">device_ids</span>
</pre></div>


<p>int 列表或 torch.device 对象，用于指定要并行的设备。参考 DataParallel。</p>
<p>对于数据并行，即完整模型放置于一个 GPU 上（single-device module）时，需要提供该参数，表示将模型副本拷贝到哪些 GPU 上。</p>
<p>对于模型并行的情况，即一个模型，分散于多个 GPU 上的情况（multi-device module），以及 CPU 模型，该参数比必须为 None，或者为空列表。</p>
<p>与单机并行一样，输入数据及中间数据，必须放置于对应的，正确的 GPU 上。</p>
<div class="hlcode"><pre><span class="n">output_device</span>
</pre></div>


<p>int 或者 torch.device，参考 DataParallel。</p>
<p>对于 single-device 的模型，表示结果输出的位置。</p>
<p>对于 multi-device module 和 GPU 模型，该参数必须为 None 或空列表。</p>
<div class="hlcode"><pre><span class="n">broadcast_buffers</span>
</pre></div>


<p>bool 值，默认为 True</p>
<p>表示在 forward() 函数开始时，对模型的 buffer 进行同步 (broadcast)</p>
<div class="hlcode"><pre><span class="n">process_group</span>
</pre></div>


<p>对分布式数据（主要是梯度）进行 all-reduction 的进程组。</p>
<p>默认为 None，表示使用由 torch.distributed.init_process_group 创建的默认进程组 (process group)。</p>
<div class="hlcode"><pre><span class="n">bucket_cap_mb</span>
</pre></div>


<p>DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation.</p>
<p>bucket_cap_mb controls the bucket size in MegaBytes (MB) (default: 25)</p>
<div class="hlcode"><pre><span class="n">find_unused_parameters</span>
</pre></div>


<p>bool 值。</p>
<p>Traverse the autograd graph of all tensors contained in the return value of the wrapped module’s forward function.</p>
<p>Parameters that don’t receive gradients as part of this graph are preemptively marked as being ready to be reduced.</p>
<p>Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation.</p>
<p>If they don’t, this wrapper will hang waiting for autograd to produce gradients for those parameters.</p>
<p>Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach. (default: False)</p>
<div class="hlcode"><pre><span class="n">check_reduction</span>
</pre></div>


<p>when setting to True, it enables DistributedDataParallel to automatically check if the previous iteration’s backward reductions were successfully issued at the beginning of every iteration’s forward function.</p>
<p>You normally don’t need this option enabled unless you are observing weird behaviors such as different ranks are getting different gradients, which should not happen if DistributedDataParallel is correctly used. (default: False)<br />
注意</p>
<div class="hlcode"><pre><span class="err">要使用该</span> <span class="kr">class</span><span class="err">，需要先对</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">distributed</span> <span class="err">进行初进程组始化，可以通过</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">distributed</span><span class="p">.</span><span class="nx">init_process_group</span><span class="p">()</span> <span class="err">实现。</span>
<span class="err">该</span> <span class="nx">module</span> <span class="err">仅在</span> <span class="nx">gloo</span> <span class="err">和</span> <span class="nx">nccl</span> <span class="err">后端上可用。</span>
<span class="err">根据分布式原理，</span><span class="nx">Constructor</span> <span class="err">和</span> <span class="nx">differentiation</span> <span class="nx">of</span> <span class="nx">the</span> <span class="nx">output</span> <span class="p">(</span><span class="err">或</span> <span class="nx">a</span> <span class="kd">function</span> <span class="nx">of</span> <span class="nx">the</span> <span class="nx">output</span> <span class="nx">of</span> <span class="k">this</span> <span class="nx">module</span><span class="p">)</span> <span class="err">是一个分布式同步点。在不同的进程执行不同的代码时，需要考虑这一点。</span>
<span class="err">该</span> <span class="nx">module</span> <span class="err">假设，所有的参数在其创建时，在模型中已经注册，之后没有新的参数加入或者参数移除。对于</span> <span class="nx">buffers</span> <span class="err">也是一样。</span><span class="p">(</span><span class="err">这也是由分布式原理决定</span><span class="p">)</span>
<span class="err">该</span> <span class="nx">module</span> <span class="err">假设，所有的参数在每个分布式进程模型中注册的顺序一致。该</span> <span class="nx">module</span> <span class="err">自身将会按照该模型中参数注册的相反顺序执行梯度的</span> <span class="nx">all</span><span class="o">-</span><span class="nx">reduction</span><span class="err">。换言之，用户应该保证，每个分布式进程模型一样，且参数注册顺序一致。</span><span class="p">(</span><span class="err">这也是由分布式原理决定</span><span class="p">)</span>
<span class="err">如果计划使用该</span> <span class="nx">module</span><span class="err">，并用</span> <span class="nx">NCCL</span> <span class="err">后端或</span> <span class="nx">Gloo</span> <span class="err">后端</span> <span class="p">(</span><span class="err">使用</span> <span class="nx">infiniband</span><span class="p">)</span><span class="err">，需要与多</span> <span class="nx">workers</span> <span class="err">的</span> <span class="nx">Dataloader</span> <span class="err">一同使用，请修改多进程启动算法为</span> <span class="nx">forkserver</span> <span class="p">(</span><span class="nx">python</span> <span class="mi">3</span> <span class="nx">only</span><span class="p">)</span> <span class="err">或</span> <span class="nx">spawn</span> <span class="err">。不幸的是，</span><span class="nx">Gloo</span> <span class="p">(</span><span class="err">使用</span> <span class="nx">infiniband</span><span class="p">)</span> <span class="err">和</span> <span class="nx">NCCL2</span> <span class="nx">fork</span> <span class="err">并不安全，并且如果不改变配置时，很可能会</span> <span class="nx">deadlocks</span><span class="err">。</span>
<span class="err">在</span> <span class="nx">module</span> <span class="err">上定义的前向传播和反向传播</span> <span class="nx">hooks</span> <span class="err">和其子</span> <span class="nx">modules</span> <span class="err">将不会涉及，除非</span> <span class="nx">hooks</span> <span class="err">在</span> <span class="nx">forward</span> <span class="err">中进行了初始化。</span>
<span class="err">在使用</span> <span class="nx">DistributedDataParallel</span> <span class="err">封装</span> <span class="nx">model</span> <span class="err">后，不应该再修改模型的参数。也就是说，当使用</span> <span class="nx">DistributedDataParallel</span> <span class="err">打包</span> <span class="nx">model</span> <span class="err">时，</span><span class="nx">DistributedDataParallel</span> <span class="err">的</span> <span class="nx">constructor</span> <span class="err">将会在模型上注册额外的归约函数，该函数作用于模型的所有参数。</span>
</pre></div>


<p>如果在构建 DistributedDataParallel 之后，改变模型的参数，这是不被允许的，并且可能会导致不可预期的后果，因为部分参数的梯度归约函数可能不会被调用。</p>
<div class="hlcode"><pre><span class="err">在进程之间，参数永远不会进行</span> <span class="n">broadcast</span><span class="err">。该</span> <span class="n">module</span> <span class="err">对梯度执行一个</span> <span class="n">all</span><span class="o">-</span><span class="n">reduce</span> <span class="err">步骤，并假设在所有进程中，可以被</span> <span class="n">optimizer</span> <span class="err">以相同的方式进行更改。</span> <span class="err">在每一次迭代中，</span><span class="n">Buffers</span> <span class="p">(</span><span class="n">BatchNorm</span> <span class="n">stats</span> <span class="err">等</span><span class="p">)</span> <span class="err">是进行</span> <span class="n">broadcast</span> <span class="err">的，从</span> <span class="n">rank</span> <span class="mi">0</span> <span class="err">的进程中的</span> <span class="n">module</span> <span class="err">进行广播，广播到系统的其他副本中。</span>
</pre></div>


<p>使用</p>
<p>DistributedDataParallel 可以通过如下两种方式进行使用：</p>
<div class="hlcode"><pre><span class="n">Single</span><span class="o">-</span><span class="n">Process</span> <span class="n">Multi</span><span class="o">-</span><span class="n">GPU</span>
</pre></div>


<p>在这种情况下，每个主机上将用单进程，每个进程使用所在主机的所有的 GPUs。这种方式下，代码如下所示：</p>
<div class="hlcode"><pre>    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s">&quot;nccl&quot;</span><span class="p">)</span> <span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c"># device_ids will include all GPU devices by default ```</span>


    <span class="n">Multi</span><span class="o">-</span><span class="n">Process</span> <span class="n">Single</span><span class="o">-</span><span class="n">GPU</span>

<span class="err">我们强烈建议用该方式来使用</span> <span class="n">DistributedDataParallel</span><span class="err">，使用多进程，每个进程使用一个</span> <span class="n">GPU</span><span class="err">。这是目前</span> <span class="n">Pytorch</span> <span class="err">中，无论是单节点还是多节点，进行数据并行训练最快的方式。</span>

<span class="err">并且实验证明，在单节点多</span> <span class="n">GPU</span> <span class="err">上进行训练，该方式比</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span> <span class="err">更快。这是因为分布是并行不需要</span> <span class="n">broadcast</span> <span class="err">参数。</span>

<span class="err">假设每个主机有</span> <span class="n">N</span> <span class="err">个</span> <span class="n">GPUs</span><span class="err">，那么需要使用</span> <span class="n">N</span> <span class="err">个进程，并保证每个进程单独处理一个</span> <span class="n">GPU</span><span class="err">。因此，需要保证训练代码在单个</span> <span class="n">GPU</span> <span class="err">上进行操作，可以用如下代码进行实现：</span>

<span class="n">python</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

<span class="err">其中，</span><span class="n">i</span> <span class="err">应该为</span> <span class="mi">0</span> <span class="err">到</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="err">之间。在每一个进程中，应该通过如下方式构建模型：</span>

<span class="sb">``</span><span class="err">`</span><span class="n">python</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s">&#39;...&#39;</span><span class="p">)</span> <span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">i</span><span class="p">])</span> <span class="sb">``</span><span class="err">`</span>


<span class="err">为了在每个主机（</span><span class="n">node</span><span class="err">）上使用多进程，可以使用</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span> <span class="err">或</span> <span class="n">torch</span><span class="o">.</span><span class="n">multiprocessing</span><span class="o">.</span><span class="n">spawn</span> <span class="err">来实现。</span>
<span class="n">DistributedDataParallelCPU</span>
<span class="err">原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallelCPU</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>

<span class="err">参数</span>

    <span class="n">module</span> <span class="err">–</span> <span class="n">module</span> <span class="n">to</span> <span class="n">be</span> <span class="n">parallelized</span>

<span class="err">说明</span>

    <span class="err">在</span> <span class="n">module</span> <span class="err">级别上利用</span> <span class="n">CPU</span> <span class="err">实现分布式数据并行。</span>
    <span class="err">该</span> <span class="n">module</span> <span class="err">支持</span> <span class="n">mpi</span> <span class="err">和</span> <span class="n">gloo</span> <span class="err">后端。</span>
    <span class="err">该</span> <span class="n">container</span> <span class="err">通过在</span> <span class="n">batch</span> <span class="err">维度上，对输入进行分割，并分配到特定的设备上，实现模型的并行。将该</span> <span class="n">module</span> <span class="err">复制到每一台机器上，每一个副本处理输入的一部分。在反向传播阶段，每各节点的梯度求平均。</span>
    <span class="err">该</span> <span class="n">module</span> <span class="err">应该与</span> <span class="n">DistributedSampler</span> <span class="err">一起使用。</span><span class="n">DistributedSampler</span> <span class="err">将会为每个节点加载一个原始数据集的子集，每个子集的</span> <span class="n">batchsize</span> <span class="err">相同。因此，总</span> <span class="n">bs</span> <span class="err">的缩放如下所示：</span>


    <span class="n">n</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">8</span><span class="n">n</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">batch</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">16</span><span class="n">n</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">batch</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">32</span><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">batch</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="err">该</span> <span class="k">class</span> <span class="err">的创建，需要该 </span><span class="nc">distributed</span> <span class="n">package</span> <span class="err">以</span> <span class="n">process</span> <span class="n">group</span> <span class="err">模式进行初始化。</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">())</span><span class="err">。</span>
<span class="err">警告</span>

    <span class="n">constructor</span><span class="err">，</span> <span class="n">forward</span> <span class="n">method</span> <span class="err">和</span> <span class="n">differentiation</span> <span class="n">of</span> <span class="n">the</span> <span class="n">output</span> <span class="p">(</span><span class="err">或</span> <span class="n">a</span> <span class="n">function</span> <span class="n">of</span> <span class="n">the</span> <span class="n">output</span> <span class="n">of</span> <span class="n">this</span> <span class="n">module</span><span class="p">)</span> <span class="err">是一个分布式同步点。在不同节点可能执行不同代码的情况下，需要考虑这一点。该</span> <span class="n">module</span> <span class="err">假设，所有的参数在其创建时，在模型中已经注册，之后没有新的参数加入或者参数移除。对于</span> <span class="n">buffers</span> <span class="err">也是一样。该</span> <span class="n">module</span> <span class="err">假设所有的</span> <span class="n">buffers</span> <span class="err">和梯度都是密集型的。在</span> <span class="n">module</span> <span class="err">上定义的前向传播和反向传播</span> <span class="n">hooks</span> <span class="err">和其子</span> <span class="n">modules</span> <span class="err">将不会涉及，除非</span> <span class="n">hooks</span> <span class="err">在</span> <span class="n">forward</span> <span class="err">中进行了初始化。参数在</span> <span class="n">__init__</span><span class="p">()</span> <span class="err">函数中，在不同节点之间进行</span> <span class="n">broadcast</span><span class="err">。该</span> <span class="n">module</span> <span class="err">在梯度上执行一个</span> <span class="nb">all</span><span class="o">-</span><span class="nb">reduce</span> <span class="err">步骤，并假设它们将会被</span> <span class="n">optimizer</span> <span class="err">在所有节点上以相同的方式进行更改。</span>

<span class="n">DistributedSampler</span>
<span class="err">原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="err">参数</span>

    <span class="n">dataset</span>

<span class="err">进行采样的数据集</span>

    <span class="n">num_replicas</span>

<span class="err">分布式训练中，参与训练的进程数</span>

    <span class="n">rank</span>

<span class="err">当前进程的</span> <span class="n">rank</span> <span class="err">序号（必须位于分布式训练中）</span>
<span class="err">说明</span>

<span class="err">对数据集进行采样，使之划分为几个子集，不同</span> <span class="n">GPU</span> <span class="err">读取的数据应该是不一样的。</span>

<span class="err">一般与</span> <span class="n">DistributedDataParallel</span> <span class="err">配合使用。此时，每个进程可以传递一个</span> <span class="n">DistributedSampler</span> <span class="err">实例作为一个</span> <span class="n">Dataloader</span> <span class="n">sampler</span><span class="err">，并加载原始数据集的一个子集作为该进程的输入。</span>

<span class="err">在</span> <span class="n">Dataparallel</span> <span class="err">中，数据被直接划分到多个</span> <span class="n">GPU</span> <span class="err">上，数据传输会极大的影响效率。相比之下，在</span> <span class="n">DistributedDataParallel</span> <span class="err">使用</span> <span class="n">sampler</span> <span class="err">可以为每个进程划分一部分数据集，并避免不同进程之间数据重复。</span>

<span class="err">注意：在</span> <span class="n">DataParallel</span> <span class="err">中，</span><span class="n">batch</span> <span class="n">size</span> <span class="err">设置必须为单卡的</span> <span class="n">n</span> <span class="err">倍，但是在</span> <span class="n">DistributedDataParallel</span> <span class="err">内，</span><span class="n">batch</span> <span class="n">size</span> <span class="err">设置于单卡一样即可。</span>
<span class="err">使用实例</span>

<span class="c"># 分布式训练示例</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torch.utils.data.distributed</span> <span class="kn">import</span> <span class="n">DistributedSampler</span>
<span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">your_dataset</span><span class="p">()</span>
<span class="n">datasampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size_per_gpu</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">datasampler</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">your_model</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataPrallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">local_rank</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">local_rank</span><span class="p">)</span>

<span class="err">通信方式</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span> <span class="err">支持</span> <span class="n">Collective</span> <span class="n">Communication</span> <span class="err">和</span> <span class="n">Point</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">Point</span> <span class="n">Communication</span><span class="err">，前者为默认通信方式。</span>
<span class="n">Point</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">Point</span> <span class="n">Communication</span>
<span class="err">基本概念</span>

<span class="err">点对点通信，使用同一个</span> <span class="n">IP</span> <span class="err">和端口。</span>

<span class="err">点对点通信指的是，数据从一个进程转移到另一个进程。这通过</span> <span class="n">send</span> <span class="err">和</span> <span class="n">recv</span> <span class="err">函数实现，也可以用对应的即时版本，</span><span class="n">isend</span> <span class="err">和</span> <span class="n">irevc</span> <span class="err">进行实现。</span>

<span class="err">当想要对进程之间的通信有一个</span> <span class="n">fine</span><span class="o">-</span><span class="n">grained</span> <span class="err">控制的时候，点对点通信是很有用的。</span>
<span class="err">阻塞式</span> <span class="n">blocking</span>
<span class="n">send</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="err">函数作用</span>

<span class="err">同步发送一个</span> <span class="n">tensor</span><span class="err">。</span>

<span class="err">参数</span>

    <span class="n">tensor</span> <span class="err">：要发送的</span> <span class="n">tensordst</span><span class="err">：目标</span> <span class="n">rank</span><span class="err">，整数</span><span class="n">group</span><span class="err">：工作的进程组</span><span class="n">tag</span> <span class="err">：用于匹配当前</span> <span class="n">send</span> <span class="err">与远程</span> <span class="n">recv</span> <span class="err">的标记</span>

<span class="n">recv</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="err">函数作用</span>

<span class="err">同步接收一个</span> <span class="n">tensor</span><span class="err">。</span>

<span class="err">参数：</span>

    <span class="n">tensor</span><span class="err">：要接收的</span> <span class="n">tensorsrc</span><span class="err">：源</span> <span class="n">rank</span><span class="err">，如果未指定，则可以从任意进程接收数据</span><span class="n">group</span><span class="err">：工作的进程组</span><span class="n">tag</span><span class="err">：用于匹配当前</span> <span class="n">recv</span> <span class="err">与远程</span> <span class="n">send</span> <span class="err">的标记</span>

<span class="err">返回值</span>

<span class="n">Sender</span> <span class="n">rank</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">part</span> <span class="n">of</span> <span class="n">the</span> <span class="n">group</span>
<span class="err">非阻塞式</span> <span class="n">Non</span><span class="o">-</span><span class="n">blocking</span>
<span class="n">isend</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="err">函数作用</span>

<span class="err">异步发送一个</span> <span class="n">tensor</span><span class="err">。</span>

<span class="err">参数</span>

    <span class="n">tensor</span> <span class="err">：要发送的</span> <span class="n">tensordst</span><span class="err">：目标</span> <span class="n">rank</span><span class="err">，整数</span><span class="n">group</span><span class="err">：工作的进程组</span><span class="n">tag</span> <span class="err">：用于匹配当前</span> <span class="n">send</span> <span class="err">与远程</span> <span class="n">recv</span> <span class="err">的标记</span>

<span class="err">返回</span>

<span class="n">A</span> <span class="n">distributed</span> <span class="n">request</span> <span class="nb">object</span><span class="o">.</span> <span class="bp">None</span><span class="p">,</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">part</span> <span class="n">of</span> <span class="n">the</span> <span class="n">group</span>
<span class="n">irecv</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">irecv</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="err">函数作用</span>

<span class="err">异步接收一个</span> <span class="n">tensor</span><span class="err">。</span>

<span class="err">参数：</span>

    <span class="n">tensor</span><span class="err">：要接收的</span> <span class="n">tensorsrc</span><span class="err">：源</span> <span class="n">rank</span><span class="err">，如果未指定，则可以从任意进程接收数据</span><span class="n">group</span><span class="err">：工作的进程组</span><span class="n">tag</span><span class="err">：用于匹配当前</span> <span class="n">recv</span> <span class="err">与远程</span> <span class="n">send</span> <span class="err">的标记</span>

<span class="err">返回值</span>

<span class="n">A</span> <span class="n">distributed</span> <span class="n">request</span> <span class="nb">object</span><span class="o">.</span> <span class="bp">None</span><span class="p">,</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">part</span> <span class="n">of</span> <span class="n">the</span> <span class="n">group</span>
<span class="err">实例</span>
<span class="err">阻塞式</span>

<span class="err">在阻塞式通信中，进程间通信必须等待数据传输完成才会进行下一步。</span>

<span class="sd">&quot;&quot;&quot;Blocking point-to-point communication.&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c"># Send the tensor to process 1</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c"># Receive tensor from process 0</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="err">在上面的例子中，每一个进程都是以</span> <span class="mi">0</span> <span class="err">开始的，随后进程</span> <span class="mi">0</span> <span class="err">增加了该</span> <span class="n">tensor</span><span class="err">，然后将其发送给进程</span> <span class="mi">1</span><span class="err">，因此两个进程中均更新为</span> <span class="mi">1</span><span class="err">。</span>
<span class="err">非阻塞式</span>

<span class="err">在非阻塞通信中，进程间通信无需等待。该方法返回一个</span> <span class="n">DistributedRequest</span> <span class="err">对象，该对象支持如下两个方法：</span>

    <span class="n">is_completed</span><span class="p">()</span>

<span class="err">如果通信完成，则返回</span> <span class="bp">True</span><span class="err">。</span>

    <span class="n">wait</span><span class="p">()</span>

<span class="err">对进程上锁，等待通信结束。在</span> <span class="n">req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span> <span class="err">执行之后，我们可以保证通信已经结束。</span>

<span class="sd">&quot;&quot;&quot;Non-blocking point-to-point communication.&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">req</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">tensor</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c"># Send the tensor to process 1</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;Rank 0 started sending&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c"># Receive tensor from process 0</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">irecv</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;Rank 1 started receiving&#39;</span><span class="p">)</span>
    <span class="n">req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Rank &#39;</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="s">&#39; has data &#39;</span><span class="p">,</span> <span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="err">我们需要小心的处理发送和接受的</span> <span class="n">tensor</span><span class="err">。由于我们不知道什么时候数据将会与其他进程进行通信，因此在</span> <span class="n">req</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span> <span class="err">结束之前，我们不应该更改发送的</span> <span class="n">tensor</span> <span class="err">或者访问接受的</span> <span class="n">tensor</span><span class="err">。</span>
<span class="n">Collective</span> <span class="n">Communication</span>
<span class="err">集体通信的概念</span>

<span class="err">每个</span> <span class="n">collective</span> <span class="n">operations</span> <span class="err">，也就是群体操作，支持同步和异步的方式。</span>

    <span class="err">同步操作（默认）</span>

<span class="err">当</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">False</span> <span class="err">时，为同步操作。</span>

<span class="err">当函数返回时，可以保证</span> <span class="n">collective</span> <span class="err">操作执行完毕</span><span class="p">(</span><span class="err">由于所有的</span> <span class="n">CUDA</span> <span class="err">操作都是异步的，因此当为</span> <span class="n">CUDA</span> <span class="err">操作时，操作不一定已完成</span><span class="p">)</span><span class="err">，同时任何进一步的函数调用取决于</span> <span class="n">collective</span> <span class="err">操作能够调用的数据。在同步模式下，</span><span class="n">collective</span> <span class="err">函数不返回任何值。</span>

    <span class="err">异步操作</span>

<span class="err">当</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span> <span class="err">时，为此模式。</span>

<span class="err">此时，</span><span class="n">collective</span> <span class="err">操作函数返回一个分布式请求对象。通常来说，无需手动创建该对象，且该对象支持两个操作：</span>

    <span class="n">is_completed</span><span class="p">()</span> <span class="err">：判断是否执行完毕，若是则返回</span> <span class="n">Truewait</span><span class="p">()</span><span class="err">：使用这个方法来阻塞这个进程，直到调用的</span> <span class="n">collective</span> <span class="n">function</span> <span class="err">执行完毕</span>

<span class="err">与点对点通信相反，集体通信支持同组内的所有进程之间的通信。</span>

<span class="err">要创建一个组，可以传递一个</span> <span class="n">rank</span> <span class="err">的列表给</span> <span class="n">dist</span><span class="o">.</span><span class="n">new_group</span><span class="p">(</span><span class="n">group</span><span class="p">)</span><span class="err">。</span>

<span class="err">默认情况下，集体操作是执行在所有的进程上的，也被称之为</span> <span class="n">world</span><span class="err">（所有的进程）。</span>
<span class="err">单</span> <span class="n">GPU</span> <span class="err">集体操作</span>
<span class="n">broadcast</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="err">函数功能</span>

<span class="err">广播该</span> <span class="n">tensor</span> <span class="err">到整个</span> <span class="n">group</span><span class="err">。</span>

<span class="err">该</span> <span class="n">tensor</span> <span class="err">在该组内所有对应的</span> <span class="n">tensor</span> <span class="err">必须尺寸一致。</span>

<span class="err">参数</span>

    <span class="n">tensor</span>

<span class="err">若</span> <span class="n">src</span> <span class="err">为当前进程，则</span> <span class="n">tensor</span> <span class="err">为要发送的数据；若不为当前进程，则</span> <span class="n">tensor</span> <span class="err">为要接收的数据。</span>

    <span class="n">src</span>

<span class="err">源进程。</span>

    <span class="n">group</span>

<span class="err">可选参数，指定该操作所在的组。</span>

    <span class="n">async_op</span>

<span class="err">可选参数，决定是同步还是异步</span>

<span class="err">返回值</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span><span class="err">，则返回</span> <span class="n">work</span> <span class="err">句柄；否则返回</span> <span class="bp">None</span><span class="err">。</span>
<span class="n">scatter</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">scatter_list</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="err">函数功能</span>

<span class="err">分发</span> <span class="n">tensor</span> <span class="err">到组内所有进程，注意与</span> <span class="n">broadcast</span> <span class="err">的区别。</span>

<span class="err">参数</span>

    <span class="n">tensor</span>

<span class="err">输出</span> <span class="n">Tensor</span><span class="err">，对应于上图的</span> <span class="n">Rank</span> <span class="mi">0</span> <span class="err">内的值</span>

    <span class="n">scatter_list</span>

<span class="err">要分发的目标</span> <span class="n">tensor</span> <span class="err">列表。仅在发送数据的进程中需要设定。</span>

    <span class="n">src</span>

<span class="err">源</span> <span class="n">rank</span><span class="err">。除了发送数据的进程，其与所有进程均需要设定该参数。</span>

    <span class="n">group</span>

<span class="err">指定该操作所在的组。</span>

    <span class="n">async_op</span>

<span class="err">可选参数，决定是同步还是异步</span>

<span class="err">返回值</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span><span class="err">，则返回</span> <span class="n">work</span> <span class="err">句柄；否则返回</span> <span class="bp">None</span><span class="err">。</span>
<span class="n">barrier</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">(</span><span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="err">函数功能</span>

<span class="err">同步所有进程。</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">为</span> <span class="bp">False</span><span class="err">，或</span> <span class="n">async</span> <span class="err">进程是在</span> <span class="n">wait</span><span class="p">()</span> <span class="err">中调用的，则该操作将封锁进程，直到整个组进入该函数。</span>

<span class="err">参数</span>

    <span class="n">group</span>

<span class="err">指定该操作所在的组。</span>

    <span class="n">async_op</span>

<span class="err">可选参数，决定是同步还是异步</span>

<span class="err">返回值</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span><span class="err">，则返回</span> <span class="n">work</span> <span class="err">句柄；否则返回</span> <span class="bp">None</span><span class="err">。</span>
<span class="n">gather</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">gather_list</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="err">函数功能</span>

<span class="err">将一组</span> <span class="n">tensor</span> <span class="err">聚集于一个进程。</span>

<span class="err">参数</span>

    <span class="n">tensor</span>

<span class="err">输入</span> <span class="n">tensor</span>

    <span class="n">gather_list</span>

<span class="err">仅在接收数据的进程中需要设定，为一个尺寸合适的</span> <span class="n">tensor</span><span class="err">。</span>

    <span class="n">dst</span>

<span class="err">目标</span> <span class="n">rank</span><span class="err">。在所有发送数据的进程中，均需要设定该参数</span>

    <span class="n">group</span>

<span class="err">指定该操作所在的组。</span>

    <span class="n">async_op</span>

<span class="err">可选参数，决定是同步还是异步</span>

<span class="err">返回值</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span><span class="err">，则返回</span> <span class="n">work</span> <span class="err">句柄；否则返回</span> <span class="bp">None</span><span class="err">。</span>
<span class="n">all_gather</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="err">函数功能</span>

<span class="err">将</span> <span class="n">group</span> <span class="err">中的</span> <span class="n">tensor</span> <span class="err">集中到</span> <span class="n">tensor_list</span> <span class="err">中。</span>

<span class="err">参数</span>

    <span class="n">tensor_list</span>

<span class="err">合适尺寸的输出</span> <span class="n">tensor</span> <span class="err">列表。</span>

    <span class="n">tensor</span>

<span class="err">当前进程需要</span> <span class="n">broadcast</span> <span class="err">的</span> <span class="n">tensor</span><span class="err">。</span>

    <span class="n">group</span>

<span class="err">指定该操作所在的组。</span>

    <span class="n">async_op</span>

<span class="err">可选参数，决定是同步还是异步</span>

<span class="err">返回值</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span><span class="err">，则返回</span> <span class="n">work</span> <span class="err">句柄；否则返回</span> <span class="bp">None</span><span class="err">。</span>
<span class="nb">reduce</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="err">函数功能</span>

<span class="err">对所有进程内的数据进行归约。但是结果只存储于</span> <span class="n">dst</span> <span class="err">进程。</span>

<span class="err">参数</span>

    <span class="n">tensor</span>

<span class="n">collective</span> <span class="err">输入输出，该操作是</span> <span class="n">inplace</span> <span class="err">的</span>

    <span class="n">dst</span>

<span class="err">目标</span> <span class="n">rank</span>

    <span class="n">op</span>

<span class="err">指定归约操作的类型，为</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span> <span class="err">枚举类型。支持：</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUMtorch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">PRODUCTtorch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MINtorch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span>
    <span class="n">group</span>

<span class="err">指定该操作所在的组</span>

    <span class="n">async_op</span>

<span class="err">可选参数，决定是同步还是异步</span>

<span class="err">返回值</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span><span class="err">，则返回</span> <span class="n">work</span> <span class="err">句柄；否则返回</span> <span class="bp">None</span><span class="err">。</span>
<span class="n">all_reduce</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="err">函数功能</span>

<span class="err">与</span> <span class="nb">reduce</span> <span class="err">一致，区别在于，所有进程都获取最终结果，</span><span class="n">inplace</span> <span class="err">操作。</span>

<span class="err">参数</span>

    <span class="n">tensor</span>

<span class="n">collective</span> <span class="err">输入输出，该操作是</span> <span class="n">inplace</span> <span class="err">的</span>

    <span class="n">op</span>

<span class="err">指定归约操作的类型，为</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span> <span class="err">枚举类型。支持：</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUMtorch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">PRODUCTtorch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MINtorch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span>
    <span class="n">group</span>

<span class="err">指定该操作所在的组</span>

    <span class="n">async_op</span>

<span class="err">可选参数，决定是同步还是异步</span>

<span class="err">返回值</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span><span class="err">，则返回</span> <span class="n">work</span> <span class="err">句柄；否则返回</span> <span class="bp">None</span><span class="err">。</span>
<span class="err">多</span> <span class="n">GPU</span> <span class="err">集体操作</span>
<span class="err">说明</span>

<span class="err">当使用</span> <span class="n">NCCL</span> <span class="err">和</span> <span class="n">Gloo</span> <span class="err">后端时，如果每个节点上拥有多个</span> <span class="n">GPU</span><span class="err">，支持每个节点内的多</span> <span class="n">GPUs</span> <span class="err">之间的分布式</span> <span class="n">collective</span> <span class="err">操作。要注意到每个进程上的</span> <span class="n">tensor</span> <span class="nb">list</span> <span class="err">长度都必须相同。</span>

<span class="err">该操作可能潜在的改善整个分布式训练性能，并易于通过传递一个</span> <span class="n">tensor</span> <span class="err">列表来使用。函数调用时，在传递的列表中的每个</span> <span class="n">tensor</span><span class="err">，需要在主机的一个单独的</span> <span class="n">GPU</span> <span class="err">上。</span>
<span class="err">实例</span>

<span class="err">例如，假设用于训练的系统包含</span> <span class="mi">2</span> <span class="err">个节点</span><span class="p">(</span><span class="n">node</span><span class="p">)</span><span class="err">，也就是主机，每个节点有</span> <span class="mi">8</span> <span class="err">个</span> <span class="n">GPU</span><span class="err">。在这</span> <span class="mi">16</span> <span class="err">个</span> <span class="n">GPU</span> <span class="err">上的每一个中，有一个需要进行</span> <span class="n">all_reduce</span> <span class="err">的</span> <span class="n">tensor</span><span class="err">。那么可以参考如下代码：</span>

<span class="n">Code</span> <span class="n">running</span> <span class="n">on</span> <span class="n">Node</span> <span class="mi">0</span><span class="err">：</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="kn">as</span> <span class="nn">dist</span>

<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s">&quot;nccl&quot;</span><span class="p">,</span>
                        <span class="n">init_method</span><span class="o">=</span><span class="s">&quot;file:///distributed_test&quot;</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">dev_idx</span><span class="p">))</span>

<span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>

<span class="n">Code</span> <span class="n">running</span> <span class="n">on</span> <span class="n">Node</span> <span class="mi">1</span><span class="err">：</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="kn">as</span> <span class="nn">dist</span>

<span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s">&quot;nccl&quot;</span><span class="p">,</span>
                        <span class="n">init_method</span><span class="o">=</span><span class="s">&quot;file:///distributed_test&quot;</span><span class="p">,</span>
                        <span class="n">world_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">dev_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()):</span>
    <span class="n">tensor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">dev_idx</span><span class="p">))</span>

<span class="n">dist</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">)</span>

<span class="err">在所有的调用之后，两个节点上，所有的</span> <span class="n">GPU</span> <span class="err">上的对应</span> <span class="n">tensor</span> <span class="err">均为</span> <span class="mi">16</span><span class="err">。</span>
<span class="n">broadcast_multigpu</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">src_tensor</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="err">函数功能</span>

<span class="err">将</span> <span class="n">tensors</span> <span class="err">在组内进行</span> <span class="n">broadcast</span> <span class="err">组内每个节点均有多个</span> <span class="n">GPU</span> <span class="n">tensor</span><span class="err">。</span>

<span class="err">参与到</span> <span class="n">collective</span> <span class="err">的所有的进程对应的所有</span> <span class="n">GPU</span> <span class="err">内的指定</span> <span class="n">tensor</span><span class="err">，必须具有相同的元素数量，且</span> <span class="n">tensor_list</span> <span class="err">内的每一个</span> <span class="n">tensor</span> <span class="err">必须处于不同的</span> <span class="n">GPU</span> <span class="err">上。</span>

<span class="err">当前仅有</span> <span class="n">nccl</span> <span class="err">和</span> <span class="n">gloo</span> <span class="err">后端支持，且必须为</span> <span class="n">GPU</span> <span class="n">tensors</span><span class="err">。</span>

<span class="err">参数</span>

    <span class="n">tensor_list</span>

<span class="err">参与到该</span> <span class="n">collective</span> <span class="err">内的</span> <span class="n">tensors</span> <span class="err">列表。</span>

<span class="err">如果</span> <span class="n">src</span> <span class="err">为当前进程，则</span> <span class="n">src_tensor</span> <span class="err">指定的元素（</span><span class="n">tensor_list</span><span class="p">[</span><span class="n">src_tensor</span><span class="p">]</span><span class="err">）将被</span> <span class="n">broadcast</span> <span class="err">到</span> <span class="n">src</span> <span class="err">进程内所有的其他</span> <span class="n">tensors</span><span class="err">（分别不同</span> <span class="n">GPU</span> <span class="err">），以及其他非</span> <span class="n">src</span> <span class="err">进程的</span> <span class="n">tensor_list</span> <span class="err">内的所有元素。</span>

<span class="err">需要保证，对于所有调用该函数的分布式进程中，</span><span class="n">tensor_list</span> <span class="err">的长度是一样的。</span>

    <span class="n">src</span>

<span class="err">源进程</span>

    <span class="n">group</span>

<span class="err">指定该操作所在的组</span>

    <span class="n">async_op</span>

<span class="err">可选参数，决定是同步还是异步</span>

    <span class="n">src_tensor</span>

<span class="err">可选参数，是定</span> <span class="n">tensor_list</span> <span class="err">内的源</span> <span class="n">tensor</span> <span class="err">的索引。</span>

<span class="err">返回值</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span><span class="err">，则返回</span> <span class="n">work</span> <span class="err">句柄；否则返回</span> <span class="bp">None</span><span class="err">。</span>
<span class="n">all_gather_multigpu</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_gather_multigpu</span><span class="p">(</span><span class="n">output_tensor_lists</span><span class="p">,</span> <span class="n">input_tensor_list</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="err">函数功能</span>

<span class="err">从列表中的整个组收集张量。</span><span class="n">tensor_list</span> <span class="err">中的每个张量应该位于一个单独的</span> <span class="n">GPU</span> <span class="err">上目前只支持</span> <span class="n">nccl</span> <span class="err">后端张量，应该只支持</span> <span class="n">GPU</span> <span class="err">张量。</span>

<span class="err">参数</span>

    <span class="n">output_tensor_lists</span>

<span class="err">输出列表。在每张</span> <span class="n">GPU</span> <span class="err">上，其应该包含合适的尺寸来接收</span> <span class="n">collective</span> <span class="err">的输出。例如，</span><span class="n">output_tensor_lists</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="err">包含</span> <span class="n">all_gather</span> <span class="err">的结果，其存在于</span> <span class="n">input_tensor_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="err">所属的</span> <span class="n">GPU</span><span class="err">。</span>

<span class="err">需要注意，</span><span class="n">output_tensor_lists</span> <span class="err">内的每一个元素，尺寸均为</span> <span class="n">world_size</span> <span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">input_tensor_list</span><span class="p">)</span><span class="err">。</span>

<span class="n">input_tensor_list</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="err">中索引为</span> <span class="n">k</span> <span class="err">的值，对应于</span> <span class="n">output_tensor_lists</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span> <span class="o">*</span><span class="n">world_size</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span><span class="err">。</span>

    <span class="n">input_tensor_list</span> 

<span class="err">当前进程中，需要进行</span> <span class="n">broadcast</span> <span class="err">的</span> <span class="n">tensors</span> <span class="err">的列表，每个</span> <span class="n">tensors</span> <span class="err">应该位于不同的</span> <span class="n">GPU</span> <span class="err">上。</span>

<span class="err">要注意，所有调用该函数的分布式进程中，</span><span class="n">input_tensor_list</span> <span class="err">的长度应该一致。</span>

    <span class="n">group</span>

<span class="err">指定该操作所在的组。</span>

    <span class="n">async_op</span>

<span class="err">可选参数，决定是同步还是异步</span>

<span class="err">返回值</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span><span class="err">，则返回</span> <span class="n">work</span> <span class="err">句柄；否则返回</span> <span class="bp">None</span><span class="err">。</span>
<span class="n">reduce_multigpu</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">dst</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dst_tensor</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="err">函数功能</span>

<span class="err">对所有机器上的多个</span> <span class="n">GPUs</span> <span class="err">中的</span> <span class="n">tensors</span> <span class="err">进行归约。</span><span class="n">tensor_list</span> <span class="err">内的每个</span> <span class="n">tensor</span> <span class="err">应该位于独立的</span> <span class="n">GPU</span> <span class="err">上。</span>

<span class="err">只有</span> <span class="n">dst</span> <span class="err">进程上的</span> <span class="n">tensor_list</span><span class="p">[</span><span class="n">dst_tensor</span><span class="p">]</span> <span class="err">对应的</span> <span class="n">GPU</span> <span class="err">会接收最后的结果。</span>

<span class="err">当前只有</span> <span class="n">nccl</span> <span class="err">支持该操作，且必须全部为</span> <span class="n">GPU</span> <span class="n">tensors</span><span class="err">。</span>

<span class="err">参数</span>

    <span class="n">tensor_list</span>

<span class="n">collective</span> <span class="err">对应的输入输出</span> <span class="n">GPU</span> <span class="n">tensor</span><span class="err">，该操作为</span> <span class="n">inplace</span><span class="err">。</span>

<span class="err">需要确保所有调用该函数的分布式进程中的</span> <span class="n">tensor_list</span> <span class="err">长度一致。</span>

    <span class="n">dst</span>

<span class="err">目标进程</span>

    <span class="n">op</span>

<span class="err">可选参数，支持：</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUMtorch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">PRODUCTtorch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MINtorch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span>
    <span class="n">group</span>

<span class="err">指定该操作所在的组</span>

    <span class="n">async_op</span>

<span class="err">可选参数，决定是同步还是异步</span>

    <span class="n">dst_tensor</span>

<span class="err">可选参数，指定</span> <span class="n">tensor_list</span> <span class="err">内的</span> <span class="n">tensor</span> <span class="err">的索引。</span>

<span class="err">返回值</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span><span class="err">，则返回</span> <span class="n">work</span> <span class="err">句柄；否则返回</span> <span class="bp">None</span><span class="err">。</span>
<span class="n">all_reduce_multigpu</span>

<span class="err">函数原型</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=&lt;</span><span class="nb">object</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="err">函数说明</span>

<span class="err">对所有机器上的多个</span> <span class="n">GPUs</span> <span class="err">中的</span> <span class="n">tensors</span> <span class="err">进行归约。</span><span class="n">tensor_list</span> <span class="err">内的每个</span> <span class="n">tensor</span> <span class="err">应该位于独立的</span> <span class="n">GPU</span> <span class="err">上。</span>

<span class="err">所有的进程都将获得最终结果。</span>

<span class="err">当前只有</span> <span class="n">nccl</span> <span class="err">支持该操作，且必须全部为</span> <span class="n">GPU</span> <span class="n">tensors</span><span class="err">。</span>

<span class="err">参数</span>

    <span class="n">tensor_list</span>

<span class="n">collective</span> <span class="err">对应的输入输出</span> <span class="n">GPU</span> <span class="n">tensor</span><span class="err">，该操作为</span> <span class="n">inplace</span><span class="err">。</span>

<span class="err">需要确保所有调用该函数的分布式进程中的</span> <span class="n">tensor_list</span> <span class="err">长度一致。</span>

    <span class="n">op</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">PRODUCTtorch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MINtorch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">MAX</span>
    <span class="n">group</span>

<span class="err">指定该操作所在的组</span>

    <span class="n">async_op</span>

<span class="err">可选参数，决定是同步还是异步</span>

<span class="err">返回值</span>

<span class="err">若</span> <span class="n">async_op</span> <span class="err">设置为</span> <span class="bp">True</span><span class="err">，则返回</span> <span class="n">work</span> <span class="err">句柄；否则返回</span> <span class="bp">None</span><span class="err">。</span>
<span class="err">启动工具</span> <span class="n">Launch</span> <span class="n">utility</span>
<span class="err">概述</span>

<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span> <span class="err">提供了一个启动工具，即</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">launch</span><span class="err">，用于在每个单节点上启动多个分布式进程。其同时支持</span> <span class="n">Python2</span> <span class="err">和</span> <span class="n">Python</span> <span class="mi">3</span><span class="err">。</span>

<span class="n">launch</span> <span class="err">可用于单节点的分布式训练，支持</span> <span class="n">CPU</span> <span class="err">和</span> <span class="n">GPU</span><span class="err">。对于</span> <span class="n">GPU</span> <span class="err">而言，若每个进程对应一个</span> <span class="n">GPU</span><span class="err">，则训练将取得最大性能。可通过指定参数（</span><span class="n">nproc_per_node</span><span class="err">），让</span> <span class="n">launch</span> <span class="err">在单节点上创建指定数目的进程（不可大于该节点对应的</span> <span class="n">GPU</span> <span class="err">数目）。</span>

<span class="err">该工具以及多进程分布式训练，目前只有在</span> <span class="n">NCCL</span> <span class="err">上才能发挥最好的性能，</span><span class="n">NCCL</span> <span class="err">也是被推荐用于分布式</span> <span class="n">GPU</span> <span class="err">训练的。</span>
<span class="err">参数</span>

    <span class="n">training_script</span>

<span class="err">位置参数，单</span> <span class="n">GPU</span> <span class="err">训练脚本的完整路径，该工具将并行启动该脚本。</span>

    <span class="o">--</span><span class="n">nnodes</span>

<span class="err">指定用来分布式训练脚本的节点数</span>

    <span class="o">--</span><span class="n">node_rank</span>

<span class="err">多节点分布式训练时，指定当前节点的</span> <span class="n">rank</span><span class="err">。</span>

    <span class="o">--</span><span class="n">nproc_per_node</span>

<span class="err">指定当前节点上，使用</span> <span class="n">GPU</span> <span class="err">训练的进程数。建议将该参数设置为当前节点的</span> <span class="n">GPU</span> <span class="err">数量，这样每个进程都能单独控制一个</span> <span class="n">GPU</span><span class="err">，效率最高。</span>

    <span class="o">--</span><span class="n">master_addr</span>

<span class="n">master</span> <span class="err">节点（</span><span class="n">rank</span> <span class="err">为</span> <span class="mi">0</span><span class="err">）的地址，应该为</span> <span class="n">ip</span> <span class="err">地址或者</span> <span class="n">node</span> <span class="mi">0</span> <span class="err">的</span> <span class="n">hostname</span><span class="err">。对于单节点多进程训练的情况，该参数可以设置为</span> <span class="mf">127.0</span><span class="o">.</span><span class="mf">0.1</span><span class="err">。</span>

    <span class="o">--</span><span class="n">master_port</span>

<span class="err">指定分布式训练中，</span><span class="n">master</span> <span class="err">节点使用的端口号，必须与其他应用的端口号不冲突。</span>
<span class="err">使用</span>
<span class="err">单节点多进程分布式训练</span>
</pre></div>


<div class="hlcode"><pre><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span> <span class="n">YOUR_TRAINING_SCRIPT</span><span class="p">.</span><span class="n">py</span> <span class="p">(</span><span class="o">--</span><span class="n">arg1</span> <span class="o">--</span><span class="n">arg2</span> <span class="o">--</span><span class="n">arg3</span> <span class="n">and</span> <span class="n">all</span> <span class="n">other</span> <span class="n">arguments</span> <span class="n">of</span> <span class="n">your</span> <span class="n">training</span> <span class="n">script</span><span class="p">)</span>
</pre></div>


<p>多节点多进程分布式</p>
<p>Node 1 (192.168.1.1:1234)</p>
<div class="hlcode"><pre><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span> <span class="o">--</span><span class="n">nnodes</span><span class="o">=</span><span class="mi">2</span> <span class="o">--</span><span class="n">node_rank</span><span class="o">=</span><span class="mi">0</span> <span class="o">--</span><span class="n">master_addr</span><span class="o">=</span><span class="s">&quot;192.168.1.1&quot;</span> <span class="o">--</span><span class="n">master_port</span><span class="o">=</span><span class="mi">1234</span> <span class="n">YOUR_TRAINING_SCRIPT</span><span class="p">.</span><span class="n">py</span> <span class="p">(</span><span class="o">--</span><span class="n">arg1</span> <span class="o">--</span><span class="n">arg2</span> <span class="o">--</span><span class="n">arg3</span> <span class="n">and</span> <span class="n">all</span> <span class="n">other</span> <span class="n">arguments</span> <span class="n">of</span> <span class="n">your</span> <span class="n">training</span> <span class="n">script</span><span class="p">)</span>

<span class="cp"># Node 2</span>


<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">nproc_per_node</span><span class="o">=</span><span class="n">NUM_GPUS_YOU_HAVE</span> <span class="o">--</span><span class="n">nnodes</span><span class="o">=</span><span class="mi">2</span> <span class="o">--</span><span class="n">node_rank</span><span class="o">=</span><span class="mi">1</span> <span class="o">--</span><span class="n">master_addr</span><span class="o">=</span><span class="s">&quot;192.168.1.1&quot;</span> <span class="o">--</span><span class="n">master_port</span><span class="o">=</span><span class="mi">1234</span> <span class="n">YOUR_TRAINING_SCRIPT</span><span class="p">.</span><span class="n">py</span> <span class="p">(</span><span class="o">--</span><span class="n">arg1</span> <span class="o">--</span><span class="n">arg2</span> <span class="o">--</span><span class="n">arg3</span> <span class="n">and</span> <span class="n">all</span> <span class="n">other</span> <span class="n">arguments</span> <span class="n">of</span> <span class="n">your</span> <span class="n">training</span> <span class="n">script</span><span class="p">)</span>
</pre></div>


<p>查看帮助</p>
<div class="hlcode"><pre><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">launch</span> <span class="o">--</span><span class="n">help</span>
</pre></div>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>