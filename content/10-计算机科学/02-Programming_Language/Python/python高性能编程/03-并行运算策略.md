---
title: "03-并行运算策略"
layout: page
date: 2099-06-02 00:00
---
[TOC]

# 多任务编程  

东西不够，时间有限，互相依赖，需要协调

[TOC]

**问题：** 有多任务怎么办？
## 1. 任务在计算机上的完成（进程、线程）

**进程Process** 是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。操作系统将物理CPU 虚拟称为多个CPU，每个虚拟CPU运行一个进程

**线程thread** 是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位

进程=[线程1+线程2+...+线程n]

|         项目 |             进程 |                                                                                                     线程 |
| -----------: | ---------------: | -------------------------------------------------------------------------------------------------------: |
|     地址空间 | 进程之间相互独立 |                                                                       同一进程下个线程共享，无法分配资源 |
|         通信 |    进程间通信IPC | 线程间可以直接读写进程数据段（如全局变量）来进行通信——需要进程同步和互斥手段的辅助，以保证数据的一致性。 |
|   调度和切换 |               慢 |                                                                                                       快 |
| 多线程系统中 |     是可执行实体 |                                                                                 线程不是一个可执行的实体 |


### 1.1 进程

#### 1.1.1 多进程(multiprocessing)

参考 https://www.liaoxuefeng.com/wiki/1016959663602400/1017628290184064

父进程：当前进程（python解释器）
子进程：将父进程复制一份

如果用多进程实现Master-Worker，主进程就是Master，其他进程就是Worker。

```mermaid
graph TB
   A[主进程Master]
   B[其他进程1Worker1]
   C[其他进程2Worker2]
   A-->B
   A-->C
```

进程创建需要占用系统内存来存放PCB的数据结构，所以，一个系统能够创建的进程总数是有限的，进程的最大数目取决于系统内存的大小，由系统安装时已经确定(若后期内存增加了，系统能够创建的进程总数也应增加，但是一般需要重新启动)。而用户数目、外设数量和文件等均与此无关。


```python
from multiprocessing import Process
import os

# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')


>>>
Parent process 928.
Process will start.
Run child process test (929)...
Process end.
```



#### 1.1.2 进程管理
进程管理指的是操作系统调整多个进程的功能

##### 1.1.2.1  进程锁
**进程锁**:也是为了控制同一操作系统中多个进程访问一个共享资源，只是因为程序的独立性，各个进程是无法控制其他进程对资源的访问的，但是可以使用本地系统的信号量控制（操作系统基本知识）。

##### 1.1.2.2 进程间通信（IPC）
IPC方法包括管道（PIPE）、消息排队、旗语、共用内存以及套接字（Socket）

#### 1.1.3 守护进程(daemon)
守护进程(daemon)是一类在后台运行的特殊进程，用于执行特定的系统任务。

### 1.2 线程
**线程thread** 是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。


#### 1.2.1 多线程（multi_Thread）


一个进程可以开启多个线程，**开启线程的数量受可用内存限制**。
如果是32位的机器，那么默认一个进程有2G的可用内存，而每个线程默认分析1M的栈空间，所以这种情况下理论最线程数在2000多个。一个解决办法是创建线程时减少线程栈的大小或是使用64位的系统。64位系统应该可以忽略这个问题了。
当然受cpu及磁盘速度及物理内存的限制。不用到达上限值，你的机器应该已经是慢如牛车了。

32位系统一个进程用户空间为2G，一个线程默认需要1M堆栈(最少128K)，可以算一算极限。

如果用多线程实现Master-Worker，主线程就是Master，其他线程就是Worker。

```python
import time, threading

# 新线程执行的代码:

def loop():
    print('thread %s is running...' % threading.current_thread().name)
    n = 0
    while n < 5:
        n = n + 1
        print('thread %s >>> %s' % (threading.current_thread().name, n))
        time.sleep(1)
    print('thread %s ended.' % threading.current_thread().name)

print('thread %s is running...' % threading.current_thread().name)
t = threading.Thread(target=loop, name='LoopThread')
t.start()
t.join()
print('thread %s ended.' % threading.current_thread().name)

执行结果如下：

thread MainThread is running...
thread LoopThread is running...
thread LoopThread >>> 1
thread LoopThread >>> 2
thread LoopThread >>> 3
thread LoopThread >>> 4
thread LoopThread >>> 5
thread LoopThread ended.
thread MainThread ended.
```
#### 1.2.2  线程管理
##### 1.2.2  线程锁 ThreadLocal




二者的区别还是很明显的：请求发出后，是否需要等待请求结果，才能继续执行其他操作。


进程（Process）、线程（Thread）、协程（）

总结一下就是，多任务的实现有3种方式：
1. 多进程模式；
2. 多线程模式；
3. 多进程+多线程模式。
4. 单进程单线程的异步编程模型

|                    模式 |                                                                                             优点 |                                       缺点 |
| ----------------------: | -----------------------------------------------------------------------------------------------: | -----------------------------------------: |
|                  多进程 | **稳定性高**，因为一个子进程崩溃了，不会影响主进程和其他子进程。（当然主进程挂了所有进程就全挂了 |                       **创建进程的代价大** |
|                  多线程 |                                                                         快一点，但是也快不到哪去 | 任何一个线程挂掉都可能直接造成整个进程崩溃 |
| 多进程+多线程的混合模式 |                                                                      理论上解决 稳定性和效率问题 |                                            |  |
|                    协程 |                                               单线程的异步编程模型，解决系统总的进程数量十分有限 |                                            |




**对应到Python语言，单线程的异步编程模型称为`协程`，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。我们会在后面讨论如何编写协程。**





## 2. Python概念：可迭代、迭代器、生成器-->协程


|              名称 |                                                                        例子 |                                                特征内置函数 |       适用 |
| ----------------: | --------------------------------------------------------------------------: | ----------------------------------------------------------: | ---------: |
|        可迭代对象 |                          字符串，list，dict，tuple，deque等都是**可迭代的** |                                                  `__iter__` |        for |
|            迭代器 |                                                              特定的数据结构 |                                       `__iter__` `__next__` | for next() |
| 生成器(generator) | 生成器是种特殊的迭代器，这种一边循环一边计算的机制，称为生成器：generator。 | `__iter__` `__next__` `close`,`gi_yieldfrom`,`send`,`throw` |    `yield` |
|                   |


### 1.1 可迭代 

**关键：** `__iter__`

字符串、列表、字典等 是 **可迭代对象** 但不是迭代器，也不是生成器。
>**扩展知识**:
可迭代对象，是其内部实现了，`__iter__` 这个方法，这个方法可使用for来循环。
### 1.2 迭代器

**关键：** `__iter__`+`__next__`

所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。我们不能提前知道序列的长度

`__init__` 函数内包括：

    `__iter__`:可以直接使用for来实现循环
    `__next__`:可以直接使用next()方法来实现
**示例**


    class Fib:
        def __init__(self):
            self.prev = 0
            self.curr = 1

        def __iter__(self):
            return self

        def __next__(self):
            self.curr, self.prev = self.prev + self.curr, self.curr
            return self.curr

    fib = Fib()
    for i in range(10):
        print(next(fib))
输出：

    1
    2
    3
    5
    8
    13
    21
    34
    55

### 1.3 生成器

**关键：** yield


生成器，是在迭代器的基础上（可以用for循环，可以使用next()），再实现了yield。

yield 是什么东西呢，它相当于我们函数里的return。在每次next()，或者for遍历的时候，都会yield这里将新的值返回回去，并在这里阻塞，等待下一次的调用。正是由于这个机制，才使用生成器在Python编程中大放异彩。实现节省内存，实现异步编程。
这种一边循环一边计算的机制，称为生成器：generator。

但是不需要像迭代器一样实现__iter__和__next__方法，只需要使用关键字yield就可以。

```python
def fib():
    prev, curr = 0, 1
    while True:
        yield curr
        curr, prev = prev + curr, curr

f = fib()
for i in range(10):
    print(next(f))

1
2
3
5
8
13
21
34
55
```





#### 1.3.1 如何创建一个生成器
主要有如下两种方法
##### 1. 使用列表生成式
   
        L = (x * x for x in range(10))
        print(isinstance(L, Generator))  # True

> 使用列表生成式，注意不是[]，而是()


##### 2. 实现了yield的函数(生成器函数)


        def mygen(n):
            now = 0
            while now < n:
                yield now
                now += 1
        if __name__ == '__main__':
            gen = mygen(10)
            print(isinstance(gen, Generator))  # True

#### 1.3.2 如何运行/激活生成器
由于生成器并不是一次生成所有元素，而是一次一次的执行返回，那么如何刺激生成器执行(或者说激活)呢？
激活主要有两个方法:
1. 使用next()
2. 使用generator.send(None)

示例：

        def mygen(n):
            now = 0
            while now < n:
                yield now
                now += 1

        if __name__ == '__main__':
            gen = mygen(4)

            # 通过交替执行，来说明这两种方法是等价的。
            print(gen.send(None))
            print(next(gen))
            print(gen.send(None))
            print(next(gen))
输出

        0
        1
        2
        3

#### 1.3.3 生成器的执行状态

生成器在其生命周期中，会有如下四个状态

1. GEN_CREATED   # 等待开始执行
2. GEN_RUNNING    # 解释器正在执行（只有在多线程应用中才能看到这个状态）
3. GEN_SUSPENDED  # 在yield表达式处暂停
4. GEN_CLOSED     # 执行结束


#### 1.3.4  生成器的异常处理
在生成器工作过程中，若生成器不满足生成元素的条件，就会/应该 抛出异常（StopIteration）。

通过列表生成式构建的生成器，其内部已经自动帮我们实现了抛出异常这一步。
所以我们在自己定义一个生成器的时候，我们也应该在不满足生成元素条件的时候，抛出异常。

    def mygen(n):
        now = 0
        while now < n:
            yield now
            now += 1
        raise StopIteration

    if __name__ == '__main__':
        gen = mygen(2)
        next(gen)
        next(gen)
        next(gen)

#### 1.3.5 从生成器过渡到协程：yield

1. 生成器暂停
2. 生成器发送

**1+2 实现协程**

通过上面的介绍，我们知道生成器为我们引入了**暂停函数**执行（yield）的功能。当有了暂停的功能之后，人们就想能不能在生成器暂停的时候向其**发送一点东西**（其实上面也有提及：send(None)）。这种向暂停的生成器发送信息的功能通过 PEP 342 进入 Python 2.5 中，并催生了 Python 中协程的诞生。根据 wikipedia 中的定义

  协程是为非抢占式多任务产生子程序的计算机程序组件，协程允许不同入口点在不同位置暂停或开始执行程序。

注意从本质上而言，**协程并不属于语言中的概念，而是编程模型上的概念**。
协程和线程，有相似点，多个协程之间和线程一样，只会交叉串行执行；也有不同点，**线程之间要频繁进行切换，加锁，解锁**，从复杂度和效率来看，和协程相比，这确是一个痛点。协程通过使用 yield 暂停生成器，可以将程序的执行流程交给其他的子程序，从而实现不同子程序的之间的交替执行。
下面通过一个简明的演示来看看，如何向生成器中发送消息。

    def jumping_range(N):
        index = 0
        while index < N:
            # 通过send()发送的信息将赋值给jump
            jump = yield index
            if jump is None:
                jump = 1
            index += jump

    if __name__ == '__main__':
        itr = jumping_range(5)
        print(next(itr))
        print(itr.send(2))
        print(next(itr))
        print(itr.send(-1))

输出：

        0
        2
        3
        2

这里解释下为什么这么输出。重点是`jump = yield index`这个语句。
分成两部分：

    yield index 是将index return给外部调用程序。
    jump = yield 可以接收外部程序通过send()发送的信息，并赋值给jump

以上这些，都是讲协程并发的基础必备知识，请一定要亲自去实践并理解它，不然后面的内容，将会变得枯燥无味，晦涩难懂。

下一章，我将讲一个Python3.5新引入的语法：yield from。篇幅也比较多，所以就单独拿出来讲。

协程是在单线程里实现任务的切换的
利用同步的方式去实现异步
不再需要锁，提高了并发性能
## 3. 异步IO框架--协程

单线程的异步编程模型称为协程（Coroutine），协程是一种程序组件，也称为微线程。


1 线程（Thread）=协程（Coroutine）+协程（Coroutine）+协程（Coroutine）

### yield
Python通过yield提供了对协程的基本支持
### gevent --第三方库

Python通过yield提供了对协程的基本支持，但是不完全。而第三方的gevent为Python提供了比较完善的协程支持。

gevent是第三方库，通过greenlet实现协程，其基本思想是：

当一个greenlet遇到IO操作时，比如访问网络，就自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换回来继续执行。由于IO操作非常耗时，经常使程序处于等待状态，有了gevent为我们自动切换协程，就保证总有greenlet在运行，而不是等待IO。

由于切换是在IO操作时自动完成，所以gevent需要修改Python自带的一些标准库，这一过程在启动时通过monkey patch完成：
```python
# bin/bash/python2.6
from gevent import monkey; monkey.patch_socket()
import gevent

def f(n):
    for i in range(n):
        print gevent.getcurrent(), i

g1 = gevent.spawn(f, 5)
g2 = gevent.spawn(f, 5)
g3 = gevent.spawn(f, 5)
g1.join()
g2.join()
g3.join()

运行结果：
<Greenlet at 0x10e49f550: f(5)> 0
<Greenlet at 0x10e49f550: f(5)> 1
<Greenlet at 0x10e49f550: f(5)> 2
<Greenlet at 0x10e49f550: f(5)> 3
<Greenlet at 0x10e49f550: f(5)> 4
<Greenlet at 0x10e49f910: f(5)> 0
<Greenlet at 0x10e49f910: f(5)> 1
<Greenlet at 0x10e49f910: f(5)> 2
<Greenlet at 0x10e49f910: f(5)> 3
<Greenlet at 0x10e49f910: f(5)> 4
<Greenlet at 0x10e49f4b0: f(5)> 0
<Greenlet at 0x10e49f4b0: f(5)> 1
<Greenlet at 0x10e49f4b0: f(5)> 2
<Greenlet at 0x10e49f4b0: f(5)> 3
<Greenlet at 0x10e49f4b0: f(5)> 4
可以看到，3个greenlet是依次运行而不是交替运行。
```

要让greenlet交替运行，可以通过gevent.sleep()交出控制权：
```python
def f(n):
    for i in range(n):
        print gevent.getcurrent(), i
        gevent.sleep(0)
执行结果：

<Greenlet at 0x10cd58550: f(5)> 0
<Greenlet at 0x10cd58910: f(5)> 0
<Greenlet at 0x10cd584b0: f(5)> 0
<Greenlet at 0x10cd58550: f(5)> 1
<Greenlet at 0x10cd584b0: f(5)> 1
<Greenlet at 0x10cd58910: f(5)> 1
<Greenlet at 0x10cd58550: f(5)> 2
<Greenlet at 0x10cd58910: f(5)> 2
<Greenlet at 0x10cd584b0: f(5)> 2
<Greenlet at 0x10cd58550: f(5)> 3
<Greenlet at 0x10cd584b0: f(5)> 3
<Greenlet at 0x10cd58910: f(5)> 3
<Greenlet at 0x10cd58550: f(5)> 4
<Greenlet at 0x10cd58910: f(5)> 4
<Greenlet at 0x10cd584b0: f(5)> 4
3个greenlet交替运行，
```
把循环次数改为500000，让它们的运行时间长一点，然后在操作系统的进程管理器中看，线程数只有1个。

当然，实际代码里，我们不会用gevent.sleep()去切换协程，而是在执行到IO操作时，gevent自动切换，代码如下：
```python
from gevent import monkey; monkey.patch_all()
import gevent
import urllib2

def f(url):
    print('GET: %s' % url)
    resp = urllib2.urlopen(url)
    data = resp.read()
    print('%d bytes received from %s.' % (len(data), url))

gevent.joinall([
        gevent.spawn(f, 'https://www.python.org/'),
        gevent.spawn(f, 'https://www.yahoo.com/'),
        gevent.spawn(f, 'https://github.com/'),
])


>>>运行结果：
GET: https://www.python.org/
GET: https://www.yahoo.com/
GET: https://github.com/
45661 bytes received from https://www.python.org/.
14823 bytes received from https://github.com/.
304034 bytes received from https://www.yahoo.com/.
从结果看，3个网络操作是并发执行的，而且结束顺序不同，但只有一个线程。

```

#  示例

## 将for 循环使用多线程处理

```python

results = list()
for item in item_list:
    result = process(item)
    results.append(result)
return results



from multiprocessing.dummy import Pool as ThreadPool
pool = ThreadPool()
results = pool.map(process, item_list)
pool.close()
pool.join()
```

#  进程池 

ProcessPoolExecutor


# 线程池的基本使用

从Python3.2开始，标准库为我们提供了 concurrent.futures 模块，它提供了 `ThreadPoolExecutor` (线程池)。


## submit
相比 threading 等模块，该模块通过 submit 返回的是一个 future 对象，它是一个未来可期的对象，通过它可以获悉线程的状态主线程(或进程)中可以获取某一个线程(进程)执行的状态或者某一个任务执行的状态及返回值：

主线程可以获取某一个线程（或者任务的）的状态，以及返回值。当一个线程完成的时候，主线程能够立即知道。
让多线程和多进程的编码接口一致。

```python
# coding: utf-8
from concurrent.futures import ThreadPoolExecutor
import time


def spider(page):
    time.sleep(page)
    print(f"crawl task{page} finished")
    return page

with ThreadPoolExecutor(max_workers=5) as t:  # 创建一个最大容纳数量为5的线程池
    task1 = t.submit(spider, 1)
    task2 = t.submit(spider, 2)  # 通过submit提交执行的函数到线程池中
    task3 = t.submit(spider, 3)

    print(f"task1: {task1.done()}")  # 通过done来判断线程是否完成
    print(f"task2: {task2.done()}")
    print(f"task3: {task3.done()}")

    time.sleep(2.5)
    print(f"task1: {task1.done()}")
    print(f"task2: {task2.done()}")
    print(f"task3: {task3.done()}")
    print(task1.result())  # 通过result来获取返回值
>>>

task1: False
task2: False
task3: False
crawl task1 finished
crawl task2 finished
task1: True
task2: True
task3: False
crawl task3 finished
```


使用 with 语句 ，通过 ThreadPoolExecutor 构造实例，同时传入 max_workers 参数来设置线程池中最多能同时运行的线程数目。

使用 submit 函数来提交线程需要执行的任务到线程池中，并返回该任务的句柄（类似于文件、画图），注意 submit() 不是阻塞的，而是立即返回。

通过使用 done() 方法判断该任务是否结束。上面的例子可以看出，提交任务后立即判断任务状态，显示四个任务都未完成。在延时2.5后，task1 和 task2 执行完毕，task3 仍在执行中。

使用 result() 方法可以获取任务的返回值。


## wait
```python
wait(fs, timeout=None,return_when=ALL_COMPLETED)
```
wait 接受三个参数：
fs: 表示需要执行的序列
timeout: 等待的最大时间，如果超过这个时间即使线程未执行完成也将返回
return_when：表示wait返回结果的条件，默认为 ALL_COMPLETED 全部执行完成再返回

还是用上面那个例子来熟悉用法
示例：
```python
from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED, ALL_COMPLETED
import time

def spider(page):
    time.sleep(page)
    print(f"crawl task{page} finished")
    return page

with ThreadPoolExecutor(max_workers=5) as t: 
    all_task = [t.submit(spider, page) for page in range(1, 5)]
    wait(all_task, return_when=FIRST_COMPLETED)
    print('finished')
    print(wait(all_task, timeout=2.5))

# 运行结果
crawl task1 finished
finished
crawl task2 finished
crawl task3 finished
DoneAndNotDoneFutures(done={<Future at 0x28c8710 state=finished returned int>, <Future at 0x2c2bfd0 state=finished returned int>, <Future at 0x2c1b7f0 state=finished returned int>}, not_done={<Future at 0x2c3a240 state=running>})
crawl task4 finished
```
代码中返回的条件是：当完成第一个任务的时候，就停止等待，继续主线程任务
由于设置了延时， 可以看到最后只有 task4 还在运行中
## as_completed
上面虽然提供了判断任务是否结束的方法，但是不能在主线程中一直判断啊。最好的方法是当某个任务结束了，就给主线程返回结果，而不是一直判断每个任务是否结束。

ThreadPoolExecutorThreadPoolExecutor 中 的 as_completed() 就是这样一个方法，当子线程中的任务执行完后，直接用 result() 获取返回结果

用法如下：
```python

# coding: utf-8
from concurrent.futures import ThreadPoolExecutor, as_completed
import time


def spider(page):
    time.sleep(page)
    print(f"crawl task{page} finished")
    return page

def main():
    with ThreadPoolExecutor(max_workers=5) as t:
        obj_list = []
        for page in range(1, 5):
            obj = t.submit(spider, page)
            obj_list.append(obj)

        for future in as_completed(obj_list):
            data = future.result()
            print(f"main: {data}")

# 执行结果
crawl task1 finished
main: 1
crawl task2 finished
main: 2
crawl task3 finished
main: 3
crawl task4 finished
main: 4
as_completed() 
```
方法是一个生成器，在没有任务完成的时候，会一直阻塞，除非设置了 timeout。

当有某个任务完成的时候，会 yield 这个任务，就能执行 for 循环下面的语句，然后继续阻塞住，循环到所有的任务结束。同时，先完成的任务会先返回给主线程。

## map
map(fn, *iterables, timeout=None)
fn： 第一个参数 fn 是需要线程执行的函数；
iterables：第二个参数接受一个可迭代对象；
timeout： 第三个参数 timeout 跟 wait() 的 timeout 一样，但由于 map 是返回线程执行的结果，如果 timeout小于线程执行时间会抛异常 TimeoutError。

用法如下:
```python
import time
from concurrent.futures import ThreadPoolExecutor

def spider(page):
    time.sleep(page)
    return page

start = time.time()
executor = ThreadPoolExecutor(max_workers=4)

i = 1
for result in executor.map(spider, [2, 3, 1, 4]):
    print("task{}:{}".format(i, result))
    i += 1

#  运行结果
task1:2
task2:3
task3:1
task4:4
```
使用 map 方法，无需提前使用 submit 方法，map 方法与 python 高阶函数 map 的含义相同，都是将序列中的每个元素都执行同一个函数。

上面的代码对列表中的每个元素都执行 spider() 函数，并分配各线程池。

可以看到执行结果与上面的 as_completed() 方法的结果不同，输出顺序和列表的顺序相同，就算 1s 的任务先执行完成，也会先打印前面提交的任务返回的结果。

