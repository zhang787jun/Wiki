---
title: "注意力机制Attention"
layout: page
date: 2099-06-02 00:00
---



# Self-Attention


# Multi-Head Attention

# Transformer
![](../../../../attach/images/2019-09-23-16-37-23.png)


# BERT 

![](../../../../attach/images/2019-09-23-16-42-53.png)

# 参考资料

[^1]: 李宏毅 

. 从Encoder-Decoder(Seq2Seq)理解Attention的本质