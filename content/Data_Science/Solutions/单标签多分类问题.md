---
title: "单标签多分类问题求解思路"
layout: page
date: 2099-06-02 00:00
---
[TOC]

# 1. 定义
单标签多分类问题，考虑的是类别之间相互独立。

# 2. 解决思路

单标签多分类主要解决思路是

1. 一对一 （One vs. One, 简称OvO）
2. 一对其余 （One vs. Rest，简称 OvR）也可是OvA
3. 一对所有（One vs. All）但是不严格
4. 多对多（Many vs. Many，简称 MvM）

## 2.1. One-VS-One

### 2.1.1. 思路
在one-vs-one策略中，同样假设有n个类别，则会针对**两两类别建立二项分类器**，得到k=n*(n-1)/2个分类器


### 2.1.2. 特点
**优点**
1. 稳定
   - 相比于 One-Vs-Rest 由于样本数量可能的偏向性带来的不稳定性，One-Vs-One 是一种相对稳健的扩展方法。
2. 高效
   - OvO的**训练时间**开销通常比OvR更小。OvR的每个分类器均使用全部的训练样例，而OvO的每个分类器仅用到两个类的样例每次训练时训练集的数量都降低很多，其训练效率会提高。
   - 至于预测性能，则取决于具体的数据分布，在多数情况下两者差不多。

**缺点**

1. 模型数据量多
   - 需要训练的模型数虽然增多（OvR需训练N个分类器，而OvO则需要训练$\frac{N(N-1)}{2}$个分类器）


## 2.2. One-VS-Rest

### 2.2.1. 思路
One-Vs-Rest 最为一种常用的二分类拓展方法
One-Vs-Rest 的思想是把**一个多分类的问题变成多个二分类的问题**。

转变的思路就如同方法名称描述的那样，选择其中一个类别为正类（Positive），使其他所有类别为负类（Negative）。
。
### 2.2.2. 特点 

**优点**：
1. **普适性**还比较广，可以应用于能输出值或者概率的分类器，
2. 同时**效率**相对较好，有多少个类别就训练多少个分类器。

**缺点**：
1. 很容易造成训练集样本数量的**不平衡**（Unbalance），尤其在类别较多的情况下，经常容易出现正类样本的数量远远不及负类样本的数量，这样就会造成分类器的偏向性。

## 2.3. Many-VS-Many
### 2.3.1. 思路

多对多是每次将若干类作为正例，若干其他类作为负例。
### 2.3.2. 说明
MvM的正反例构造有特殊的设计，不能随意选取。我们这里介绍一种常用的MvM技术：纠错输出码（EOOC）。

编码：对N个类做M次划分，每次划分将一部分类别划分为正例，一部分划分为反例，从而形成一个二分类的训练集：这样共有M个训练集，则可训练出M个分类器。


解码：M个分类器分别对测试样本进行预测，这些预测样本组成一个编码。将这个编码与每个类各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。

类别划分通过"编码矩阵" (coding matrix) 指定.编码矩阵有多种形式，常见的主要有二元码 [Dietterich and iri 1995] 和三元码 [Allwein et al.,2000]. 前者将每个类别分别指定为正类和反类，**后者在正、反类之外，还可指定"停用类"**因 3.5 给出了一个示意图，在图 3.5(a) 中，分类器 Cl 类和C3 类的样例作为正例 C2 类和 C4 类的样例作为反例;在图 3.5(b) 中，分类器14 类和 C4 类的样例作为正例 C3 类的样例作为反例.在解码阶段，各分类器的预测结果联合起来形成了测试示例的编码，该编码与各类所对应的编码进行比较?将距离最小的编码所对应的类别作为预测结果.例如在图 3.5(a) 中，若基于欧民距离，预测结果将是 C3.


为什么要用纠错输出码呢？因为在测试阶段，ECOC编码对分类器的错误有一定的容忍和修正能力。例如上图中对测试示例正确的预测编码是（-1，1，1，-1，1），但在预测时f2出错从而导致了错误的编码（-1， -1， 1， -1，1）。但是基于这个编码仍然能产生正确的最终分类结果C3。


# 3. 实践


```python
from sklearn.linear_model import LogisticRegression
LogisticRegression(...multi_class='auto')

# multi_class{`auto`, `ovr`, `multinomial`}, default=`auto`
```

选项说明
1. `ovr`
If the option chosen is `ovr`, then a binary problem is fit for each label. ovr即前面提到的one-vs-rest(OvR)
2. multinomial
For `multinomial` the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary. `multinomial` is unavailable when solver=`liblinear`. 
multinomial即前面提到的many-vs-many(MvM)
1. `auto`
`auto` selects `ovr` if the data is binary, or if solver=`liblinear`, and otherwise selects `multinomial`.

如果你的数据集类型是大于两类的，那么Sklearn 会自动把multi_class赋值为multinomial

如果不是就是ovr 所以一般我们不需要指定multi_class和solver参数


```python
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
import numpy as np

cluster = 3
#cluster = 4
# 3. 生成三类数据
def get_data():
    data, target = make_blobs(n_samples=18, n_features=2, centers=[[1, 5], [0.5, 0.5], [2.5, 3]], cluster_std=0.3)
    #data, target = make_blobs(n_samples=24, n_features=2, centers=[[1, 5], [0.5, 0.5], [2.5, 3], [2, 2.5]],
    #                          cluster_std=0.3)
    print(data)
    print(target)
    return data, target

#用逻辑回归的MvR拟合数据
def fit_ovr(X, y):
    clf = LogisticRegression(random_state=0, multi_class= 'ovr', solver='liblinear').fit(X, y)
    predict_Result = clf.predict(X[-2:, :])
    print("predict_Result", predict_Result, y[-2:])
    predict_proba = clf.predict_proba(X[-2:, :])
    print("predict_proba", predict_proba, y[-2:])
    score = clf.score(X, y)
    print(score)
    return clf

# 4. 用逻辑回归的MvM拟合数据
def fit_mvm(X, y):
    clf = LogisticRegression(random_state=0, multi_class= 'multinomial', solver='newton-cg').fit(X, y)
    predict_Result = clf.predict(X[-2:, :])
    print("predict_Result", predict_Result, y[-2:])
    predict_proba = clf.predict_proba(X[-2:, :])
    print("predict_proba", predict_proba, y[-2:])
    score = clf.score(X, y)
    print(score)
    return clf


# 5. 展示散点和拟合的三条直线
def show_pic(X, y, clf=None):
    # 在2D图中绘制样本，每个样本颜色不同,形状不同
    markers = ['s', '^', 'x','o']
    for i in range(cluster):
        temp = X[y == i]
        plt.scatter(temp[:, 0], temp[:, 1], marker=markers[i]);

    if clf is not None:
        print(clf.coef_)
        print(clf.intercept_)
        tx = np.arange(0, 4, 1)
        for i in range(len(clf.intercept_)):
            ty1 = -((tx * clf.coef_[i, 0] + clf.intercept_[i]) / clf.coef_[i, 1])
            print('ty'+str(i), ty1)
            plt.plot(tx, ty1)

    plt.xlim(0, 3)
    plt.ylim(0, 6)
    plt.show()


if __name__ == '__main__':
    X, y = get_data()
    clf1 = fit_ovr(X, y)
    show_pic(X, y, clf1)

    clf2 = fit_mvm(X, y)
    show_pic(X, y, clf2)
```


# 4. 总结

MvM比OvR的准确率高，






