---
title: "优化问题"
layout: page
date: 2099-06-02 00:00
---
[TOC]
# 优化问题

参考

| 编号 | 标题                                                          | 地址                                                                                                                                                    |
| :--- | :------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 1    |                                                               | http://www.dscademy.com/optimization/newton/index.php?from=header                                                                                       |
| 2    | 最优化原理笔记（推荐）                                        | https://wulc.me/2017/02/01/%E6%9C%80%E4%BC%98%E5%8C%96%E8%AE%A1%E7%AE%97%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/                                           |
| 3    | An overview of gradient descent optimization algorithms(经典) | https://arxiv.org/pdf/1609.04747.pdf                                                                                                                    |
| 4    | Adam那么棒，为什么还对SGD念念不忘                             | http://wulc.me/2019/03/18/Adam%E9%82%A3%E4%B9%88%E6%A3%92%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E5%AF%B9SGD%E5%BF%B5%E5%BF%B5%E4%B8%8D%E5%BF%98/ |



## 1. 凸函数

目标函数是否光滑（Smooth）或者是否强凸（Strongly Convex）对算法是否适用有着很大的影响。 一般来说， 部分算法在强凸的条件下， 收敛率会有进一步的提升。 强凸是一个锦上添花的性质。 但是，光滑不光滑就会决定着算法是否适用， 因为不光滑的话，一般情况下，导函数是不存在的， 那么就需要修正成近似算法（SubGradient， Proximal）来解决。 还有部分算法（Newton）除了要求光滑可导，还要求导函数光滑可导，也就是二阶可导（quadratic）。

## 2. 标准优化问题
标准的优化问题格式如下：
```python
minimize   f0(x)
subject to:
fi(x) ≤ 0, i = 1, . . . , m
hi(x) = 0, i = 1, . . . , p
``` 
机器学习中，结构风险SRM最小的框架下， 给定了优化目标

## 3. 优化方法

### 1. 一次优化

#### 1.1 降梯度
##### 1.1.1 批量梯度下降法
##### 1.1.2 随机梯度下降法
##### 1.1.3 小批量梯度下降法

### 2. 二次优化

1. 牛顿法起始点不能离局部极小点太远，否则很可能不会收敛。(考虑到二阶拟合应该很容易想象)，所以实际操作中会先使用别的方法，比如梯度下降法，使更新的点离最优点比较近，再开始用牛顿法。2. 牛顿法每次需要更新一个二阶矩阵，当维数增加的时候是非常耗内存的，所以实际使用是会用拟牛顿法。3. 梯度下降法在非常靠近最优点时会有震荡，就是说明明离的很近了，却很难到达，因为线性的逼近非常容易一个方向过去就过了最优点(因为只能是负梯度方向)。但牛顿法因为是二次收敛就很容易到达了。牛顿法最明显快的特点是对于二阶函数(考虑多元函数的话要在凸函数的情况下)，牛顿法能够一步到达，非常有效。



#### 2.1 牛顿法 

牛顿法的优点是收敛速度快，而缺点是牛顿法的计算中需要计算海森矩阵和该矩阵的逆。海森矩阵往往需要较大的计算量，而且一般矩阵的求逆运算时间复杂度是O(p3)。

##### 3.1.1 拟牛顿法
为了保持收敛速度快的同时，减小每一步迭代的计算量，我们可以将海森矩阵的逆替换为一个近似的矩阵。而该矩阵的计算复杂度远小于原始的海森矩阵求逆。

比较著名的拟牛顿法有DFP和BFGS和L-BFGS。

### 3.2 梯度下降法 

最优化的常见算法。梯度下降类算法，包括下降法的共同缺点是需要确定学习速率α。

对于一个不好的学习速率，收敛速度会很慢，甚至会出现不收敛(overshoot)的情况。

牛顿法利用了函数的二阶梯度信息／海森矩阵，从而加快了每一步的收敛速度。

牛顿法的缺点是计算时需要用到二阶梯度信息以及对海森矩阵求逆，因此每一步需要更多的计算量。

人们更多地使用拟牛顿法，如著名的L-BFGS来加快计算。
一元函数的牛顿法

牛顿法可以用来解决一元方程的求解问题f(x)=0
或者解决一元方程的最小化问题minf(x)

。

最小化问题等价于求解g(x)=f′(x)=0

，因此我们通过一元方程的求解问题来介绍牛顿迭代法。

如图所示，牛顿迭代法可以逐步地接近方程的解并收敛。

牛顿法的几何意义是每次从x(b)
出发，计算g在(x(b),g(x(b)))处的切线与横轴(x-axis)的交点作为x(b+1)

更新方程为

x(b+1)←x(b)−g(x(b))g′(x(b))

对于解方程问题，牛顿迭代法相当于每次将g
看作其在x(b)处的一阶泰勒逼近(Taylor's approximation)g˜

并求解。

对于最小化f

的问题，牛顿迭代如下：

    选取x(1)

对b=1,2,…,B
，计算f′(x(b)),f′′(x(b))
更新
x(b+1)←x(b)−f′(x(b))f′′(x(b))

对于最小化问题，牛顿迭代法相当于每次将f
看作其在x(b)处的二阶泰勒逼近f˜

并对其最小化。
实例

Q:给定y>0
，近似地计算y√

。

A:该问题是一个非常常见的算法问题，一个标准的解法是使用二分搜索。

而更有效率的算法是使用牛顿迭代求解g(x)=x2−y

。

牛顿迭代方程为
x(b+1)=x(b)−g(x(b))g′(x(b))=x(b)−x(b)2−y2x(b)=x(b)+yx(b)2

Q:最小化f(x)=x2

A:牛顿迭代方程为

x(b+1)=x(b)−f′(x(b))f′′(x(b))=x(b)−2x(b)2=0

牛顿迭代一步就求得了最优解x^=0

。
多元函数的牛顿法

对于多元函数的最小化问题f:Rp→R

可以将一元的牛顿法推广如下：

    选取x(1)

对于b=1,2,…,B
，计算梯度∇f(x(b))，海森(Hessian)矩阵Hf(x(b))并求逆[Hf(x(b))]−1
更新
x(b+1)←x(b)−[Hf(x(b))]−1∇f(x(b))

类似于一元的牛顿迭代，x(b+1)
是f在x(b)

处的二阶泰勒逼近的最小值点。

对于线性回归而言，f
本身是二次函数，因此f在x(1)出的二阶泰勒逼近就是f本身。因此使用牛顿迭代法可以在一个迭代内直接求得最优解β^=(XTX)−1XTy

。
拟牛顿法(quasi-Newton's method)

牛顿法的优点是收敛速度快，而缺点是牛顿法的计算中需要计算海森矩阵和该矩阵的逆。海森矩阵往往需要较大的计算量，而且一般矩阵的求逆运算时间复杂度是O(p3)

。

为了保持收敛速度快的同时，减小每一步迭代的计算量，我们可以将海森矩阵的逆替换为一个近似的矩阵。而该矩阵的计算复杂度远小于原始的海森矩阵求逆。

比较著名的拟牛顿法有DFP和BFGS和L-BFGS。
更多最优化相关问题见本网站论坛最优化版面。
