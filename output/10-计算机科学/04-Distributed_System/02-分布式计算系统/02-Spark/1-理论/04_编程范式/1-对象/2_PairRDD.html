<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>2. Pair RDDs - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#10-计算机科学">10-计算机科学</a>&nbsp;»&nbsp;<a href="/Wiki/#-04-Distributed_System">04-Distributed_System</a>&nbsp;»&nbsp;<a href="/Wiki/#-02-分布式计算系统">02-分布式计算系统</a>&nbsp;»&nbsp;<a href="/Wiki/#-02-Spark">02-Spark</a>&nbsp;»&nbsp;<a href="/Wiki/#-1-理论">1-理论</a>&nbsp;»&nbsp;<a href="/Wiki/#-04_编程范式">04_编程范式</a>&nbsp;»&nbsp;<a href="/Wiki/#-1-对象">1-对象</a>&nbsp;»&nbsp;2. Pair RDDs</div>
</div>
<div class="clearfix"></div>
<div id="title">2. Pair RDDs</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1-pair-rdds">1. 什么是Pair RDDs</a><ul>
<li><a href="#11-pair-rdds">1.1. 动机--为什么有 Pair RDDs</a></li>
</ul>
</li>
<li><a href="#2">2. 操作</a><ul>
<li><a href="#21-pair-rdds">2.1. 创建Pair RDDs</a><ul>
<li><a href="#211-rdd">2.1.1. 通过RDD创建</a></li>
</ul>
</li>
<li><a href="#22-transform">2.2. Transform</a><ul>
<li><a href="#221-keys">2.2.1. keys</a></li>
<li><a href="#222-values">2.2.2. values</a></li>
<li><a href="#223-combinebykey">2.2.3. combineByKey</a></li>
<li><a href="#225-groupbykey">2.2.5. groupByKey</a></li>
<li><a href="#226-sortbykey">2.2.6. sortByKey</a></li>
<li><a href="#227-mapvalues">2.2.7. mapValues</a></li>
<li><a href="#228-join">2.2.8. join</a></li>
</ul>
</li>
<li><a href="#23-action">2.3. Action</a><ul>
<li><a href="#231-collectasmap">2.3.1. collectAsMap</a></li>
<li><a href="#232-countbykey">2.3.2. countByKey</a></li>
<li><a href="#233-countbykeyapprox">2.3.3. countByKeyApprox</a></li>
<li><a href="#234-lookup">2.3.4. lookup</a></li>
<li><a href="#235-reducebykeylocally">2.3.5. reduceByKeyLocally</a></li>
<li><a href="#236-saveashadoopdataset">2.3.6. saveAsHadoopDataset</a></li>
<li><a href="#237-saveashadoopfile">2.3.7. saveAsHadoopFile</a></li>
<li><a href="#238-saveasnewapihadoopdataset">2.3.8. saveAsNewAPIHadoopDataset</a></li>
<li><a href="#239-saveasnewapihadoopfile">2.3.9. saveAsNewAPIHadoopFile</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3">3. 参考资料</a></li>
</ul>
</div>
<h1 id="1-pair-rdds">1. 什么是Pair RDDs</h1>
<h2 id="11-pair-rdds">1.1. 动机--为什么有 Pair RDDs</h2>
<p>普通RDD里面存储的数据类型是Int、String等，而 <code>Pair RDDs</code>里面存储的数据类型是"Key-Value"。</p>
<p>Spark provides <strong>special operations</strong>on RDDs containing key/value pairs. These RDDs are called pair RDDs. Pair RDDs are a useful building block in many programs, as they expose operations that allow you to act on each key in parallel or regroup data across the network. For example, pair RDDs have a reduceByKey() method that can aggregate data separately for each key, and a join() method that can merge two RDDs together by grouping elements with the same key. It is common to extract fields from an RDD (representing, for instance, an event time, customer ID, or other identifier) and use those fields as keys in pair RDD operations.</p>
<p>键值对RDD是Spark操作中最常用的RDD，它是很多程序的构成要素，因为他们提供了并行操作各个键或跨界点重新进行数据分组的操作接口。</p>
<h1 id="2">2. 操作</h1>
<h2 id="21-pair-rdds">2.1. 创建Pair RDDs</h2>
<p>Spark中有许多中创建键值对RDD的方式，其中包括<br />
1. 文件读取时直接返回键值对RDD<br />
2. 通过List创建键值对RDD<br />
3. 在Scala中，可通过Map函数生成二元组</p>
<h3 id="211-rdd">2.1.1. 通过RDD创建</h3>
<div class="hlcode"><pre><span class="k">val</span> <span class="n">listRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="nc">List</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">))</span>
<span class="k">val</span> <span class="n">result</span> <span class="k">=</span> <span class="n">listRDD</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">,</span><span class="mi">1</span><span class="o">))</span>
<span class="n">result</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
<span class="o">(</span><span class="mi">2</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
<span class="o">(</span><span class="mi">3</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
<span class="o">(</span><span class="mi">4</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
<span class="o">(</span><span class="mi">5</span><span class="o">,</span><span class="mi">1</span><span class="o">)</span>
</pre></div>


<h2 id="22-transform">2.2. Transform</h2>
<ol>
<li>reduceByKey()</li>
<li>groupByKey()</li>
<li>sortByKey()</li>
<li>join()</li>
<li>cogroup()</li>
</ol>
<h3 id="221-keys">2.2.1. keys</h3>
<p>keys只会把键值对RDD中的key返回形成一个新的RDD。</p>
<div class="hlcode"><pre><span class="n">res</span><span class="o">=</span><span class="n">pair_rdd</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="nb">type</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="n">pyspark</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">PipelinedRDD</span>

<span class="n">pair_rdd</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="nb">list</span>
</pre></div>


<h3 id="222-values">2.2.2. values</h3>
<p>values 只会把键值对RDD中的value返回形成一个新的RDD</p>
<div class="hlcode"><pre><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([(</span><span class="s">&quot;a1&quot;</span><span class="p">,</span><span class="s">&quot;b1&quot;</span><span class="p">,</span><span class="s">&quot;c1&quot;</span><span class="p">,</span><span class="s">&quot;d1&quot;</span><span class="p">,</span><span class="s">&quot;e1&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s">&quot;a2&quot;</span><span class="p">,</span><span class="s">&quot;b2&quot;</span><span class="p">,</span><span class="s">&quot;c2&quot;</span><span class="p">,</span><span class="s">&quot;d2&quot;</span><span class="p">,</span><span class="s">&quot;e2&quot;</span><span class="p">)])</span>
<span class="n">pair_rdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:])))</span>

<span class="n">res</span><span class="o">=</span><span class="n">pair_rdd</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
<span class="nb">type</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="n">pyspark</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">PipelinedRDD</span>

<span class="n">pair_rdd</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="p">[[</span><span class="s">&#39;b1&#39;</span><span class="p">,</span> <span class="s">&#39;c1&#39;</span><span class="p">,</span> <span class="s">&#39;d1&#39;</span><span class="p">,</span> <span class="s">&#39;e1&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;b2&#39;</span><span class="p">,</span> <span class="s">&#39;c2&#39;</span><span class="p">,</span> <span class="s">&#39;d2&#39;</span><span class="p">,</span> <span class="s">&#39;e2&#39;</span><span class="p">]]</span>
<span class="c"># list</span>
</pre></div>


<h3 id="223-combinebykey">2.2.3. combineByKey</h3>
<div class="hlcode"><pre><span class="o">&gt;</span> <span class="n">combineByKey</span><span class="p">(</span> <span class="n">createCombiner</span><span class="p">,</span> <span class="n">mergeValue</span><span class="p">,</span> <span class="n">mergeCombiners</span><span class="p">,</span> <span class="n">partitioner</span><span class="p">,</span><span class="n">mapSideCombine</span><span class="p">)</span>
<span class="o">&gt;</span>
<span class="o">&gt;</span> <span class="n">combineByKey</span><span class="p">(</span> <span class="n">createCombiner</span><span class="p">,</span> <span class="n">mergeValue</span><span class="p">,</span> <span class="n">mergeCombiners</span><span class="p">,</span> <span class="n">partitioner</span><span class="p">)</span>
<span class="o">&gt;</span>
<span class="o">&gt;</span> <span class="n">combineByKey</span><span class="p">(</span> <span class="n">createCombiner</span><span class="p">,</span> <span class="n">mergeValue</span><span class="p">,</span> <span class="n">mergeCombiners</span><span class="p">)</span>
</pre></div>


<p><strong>函数功能：</strong></p>
<p>聚合各分区的元素，而每个元素都是二元组。功能与基础RDD函数aggregate()差不多，可让用户返回与输入数据类型不同的返回值。</p>
<p>combineByKey函数的每个参数分别对应聚合操作的各个阶段。所以，理解此函数对Spark如何操作RDD会有很大帮助。</p>
<p><strong>参数解析：</strong></p>
<p>createCombiner：<strong>分区内 </strong>创建组合函数</p>
<p>mergeValue：<strong>分区内 </strong>合并值函数</p>
<p>mergeCombiners：<strong>多分区 </strong>合并组合器函数</p>
<p>partitioner：自定义分区数，默认为HashPartitioner</p>
<p>mapSideCombine：是否在map端进行Combine操作，默认为true</p>
<p><strong>工作流程：</strong></p>
<ol>
<li>combineByKey会遍历分区中的<strong>所有元素</strong>，因此每个元素的key要么没遇到过，要么和之前某个元素的key相同。</li>
<li>如果这是一个新的元素，函数会调用createCombiner创建那个key对应的累加器<strong>初始值</strong>。</li>
<li>如果这是一个在处理当前分区之前已经遇到的key，会调用mergeCombiners把该key累加器对应的当前value与这个新的value<strong>合并</strong>。</li>
</ol>
<p><strong>代码例子：</strong></p>
<p>//统计男女个数</p>
<div class="hlcode"><pre><span class="k">val</span> <span class="n">conf</span> <span class="k">=new</span> <span class="nc">SparkConf</span> <span class="o">().</span><span class="n">setMaster</span> <span class="o">(</span><span class="s">&quot;local&quot;</span><span class="o">).</span><span class="n">setAppName</span> <span class="o">(</span><span class="s">&quot;app_1&quot;</span><span class="o">)</span>
<span class="k">val</span> <span class="n">sc</span> <span class="k">=new</span> <span class="nc">SparkContext</span> <span class="o">(</span><span class="n">conf</span><span class="o">)</span>
<span class="k">val</span> <span class="n">people</span> <span class="k">=</span> <span class="nc">List</span><span class="o">((</span><span class="s">&quot;男&quot;</span><span class="o">,</span><span class="s">&quot;李四&quot;</span><span class="o">),</span> <span class="o">(</span><span class="s">&quot;男&quot;</span><span class="o">,</span><span class="s">&quot;张三&quot;</span><span class="o">),</span> <span class="o">(</span><span class="s">&quot;女&quot;</span><span class="o">,</span><span class="s">&quot;韩梅梅&quot;</span><span class="o">),</span> <span class="o">(</span><span class="s">&quot;女&quot;</span><span class="o">,</span><span class="s">&quot;李思思&quot;</span><span class="o">),</span> <span class="o">(</span><span class="s">&quot;男&quot;</span><span class="o">,</span><span class="s">&quot;马云&quot;</span><span class="o">))</span>
<span class="k">val</span> <span class="n">rdd</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">people</span><span class="o">,</span><span class="err">`</span><span class="n">`2``)</span>
<span class="n">val result = rdd.combineByKey(``(x: String) =&gt; (List(x),1``),//createCombiner``(peo: (List[String], Int), x : String) =&gt; (x :: peo._1, peo._2 +1``),//mergeValue``(sex1: (List[String], Int), sex2: (List[String], Int)) =&gt; (sex1._1 ::: sex2._1, sex1._2 + sex2._2))//mergeCombiners`</span><span class="err">`</span><span class="n">result</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
<span class="o">&gt;&gt;&gt;&gt;</span>

<span class="o">(</span><span class="n">男</span><span class="o">,</span> <span class="o">(</span> <span class="nc">List</span><span class="o">(</span> <span class="n">张三</span><span class="o">,</span>  <span class="n">李四</span><span class="o">,</span>  <span class="n">马云</span><span class="o">),</span><span class="mi">3</span> <span class="o">)</span> <span class="o">)</span>
<span class="o">(</span><span class="n">女</span><span class="o">,</span> <span class="o">(</span> <span class="nc">List</span><span class="o">(</span> <span class="n">李思思</span><span class="o">,</span>  <span class="n">韩梅梅</span><span class="o">),</span><span class="mi">2</span> <span class="o">)</span> <span class="o">)</span>
</pre></div>


<p><strong>流程分解：</strong></p>
<p><a href="http://images2015.cnblogs.com/blog/368951/201702/368951-20170223163812663-2101150083.png"><img alt="Spark算子-combineByKey" src="https://images2015.cnblogs.com/blog/368951/201702/368951-20170223163813804-1005141379.png&quot;Spark算子-combineByKey&quot;" /></a></p>
<p>解析：两个分区，分区一按顺序V1、V2、V3遍历</p>
<ul>
<li>
<p>V1，发现第一个key=男时，调用createCombiner，即<br />
<code>(x: String) =&gt; (List(x), 1)</code></p>
</li>
<li>
<p>V2，第二次碰到key=男的元素，调用mergeValue，即<br />
<code>(peo: (List[String], Int), x : String) =&gt; (x :: peo._1, peo._2 + 1)</code></p>
</li>
<li>
<p>V3，发现第一个key=女，继续调用createCombiner，即<br />
<code>(x: String) =&gt; (List(x), 1)</code></p>
</li>
<li>
<p>... ...</p>
</li>
<li>待各V1、V2分区都计算完后，数据进行混洗，调用mergeCombiners，即<br />
<code>(sex1: (List[String], Int), sex2: (List[String], Int)) =&gt; (sex1._1 ::: sex2._1, sex1._2 + sex2._2))</code></li>
</ul>
<hr />
<p>add by jan 2017-02-27 18:34:39</p>
<p>以下例子都基于此RDD</p>
<table>
<thead>
<tr>
<th>1234</th>
<th><code>(Hadoop,``1``)``(Spark,``1``)``(Hive,``1``)``(Spark,``1``)</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>### 2.2.4. reduceByKey</td>
<td></td>
</tr>
</tbody>
</table>
<p>reduceByKey(func)的功能是，使用func函数合并具有相同键的值。</p>
<p>比如，reduceByKey((a,b) =&gt; a+b)，有四个键值对("spark",1)、("spark",2)、("hadoop",3)和("hadoop",5)，对具有相同key的键值对进行合并后的结果就是：("spark",3)、("hadoop",8)。可以看出，(a,b) =&gt; a+b这个Lamda表达式中，a和b都是指value，比如，对于两个具有相同key的键值对("spark",1)、("spark",2)，a就是1，b就是2。</p>
<div class="hlcode"><pre><span class="n">pairRDD</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span><span class="n">b</span><span class="o">)</span><span class="k">=&gt;</span><span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>

<span class="o">(</span><span class="nc">Spark</span><span class="o">,</span><span class="err">`</span><span class="n">`2``)``(Hive,``1``)``(Hadoop,``1``)`</span> 
</pre></div>


<h3 id="225-groupbykey">2.2.5. groupByKey</h3>
<p>groupByKey()的功能是，对具有相同键的值进行分组，返回RDD。比如，对四个键值对("spark",1)、("spark",2)、("hadoop",3)和("hadoop",5)，采用groupByKey()后得到的结果是：("spark",(1,2))和("hadoop",(3,5))。</p>
<div class="hlcode"><pre><span class="n">pair_rdd</span><span class="o">.</span><span class="n">groupByKey</span><span class="p">()</span>
</pre></div>


<h3 id="226-sortbykey">2.2.6. sortByKey</h3>
<p>sortByKey()的功能是返回一个根据键排序的RDD。</p>
<h3 id="227-mapvalues">2.2.7. mapValues</h3>
<p>我们经常会遇到一种情形，我们只想对键值对RDD的value部分进行处理，而不是同时对key和value进行处理。对于这种情形，Spark提供了mapValues(func)，它的功能是，对键值对RDD中的每个value都应用一个函数，但是，key不会发生变化。比如，对四个键值对("spark",1)、("spark",2)、("hadoop",3)和("hadoop",5)构成的pairRDD，如果执行pairRDD.mapValues(x =&gt; x+1)，就会得到一个新的键值对RDD，它包含下面四个键值对("spark",2)、("spark",3)、("hadoop",4)和("hadoop",6)。 </p>
<h3 id="228-join">2.2.8. join</h3>
<p>join(连接)操作是键值对常用的操作。"连接"(join)这个概念来自于关系数据库领域，因此，join的类型也和关系数据库中的join一样，包括内连接(join)、左外连接(leftOuterJoin)、右外连接(rightOuterJoin)等。最常用的情形是内连接，所以，join就表示内连接。<br />
对于内连接，对于给定的两个输入数据集(K,V1)和(K,V2)，只有在两个数据集中都存在的key才会被输出，最终得到一个(K,(V1,V2))类型的数据集。</p>
<p>比如，pairRDD1是一个键值对集合{("spark",1)、("spark",2)、("hadoop",3)和("hadoop",5)}，pairRDD2是一个键值对集合{("spark","fast")}，那么，pairRDD1.join(pairRDD2)的结果就是一个新的RDD，这个新的RDD是键值对集合{("spark",1,"fast"),("spark",2,"fast")}。对于这个实例，我们下面在spark-shell中运行一下：</p>
<h2 id="23-action">2.3. Action</h2>
<h3 id="231-collectasmap">2.3.1. collectAsMap</h3>
<p>collectAsMap    Returns the pair RDD as a Map to the Spark Master.</p>
<h3 id="232-countbykey">2.3.2. countByKey</h3>
<p>countByKey  Returns the count of each key elements. This returns the final result to local Map which is your driver.</p>
<h3 id="233-countbykeyapprox">2.3.3. countByKeyApprox</h3>
<p>countByKeyApprox    Same as countByKey but returns the partial result. This takes a timeout as parameter to specify how long this function to run before returning.</p>
<h3 id="234-lookup">2.3.4. lookup</h3>
<p>lookup  Returns a list of values from RDD for a given input key.</p>
<h3 id="235-reducebykeylocally">2.3.5. reduceByKeyLocally</h3>
<p>reduceByKeyLocally  Returns a merged RDD by merging the values of each key and final result will be sent to the master.</p>
<h3 id="236-saveashadoopdataset">2.3.6. saveAsHadoopDataset</h3>
<p>saveAsHadoopDataset Saves RDD to any hadoop supported file system (HDFS, S3, ElasticSearch, e.t.c), It uses Hadoop JobConf object to save.</p>
<h3 id="237-saveashadoopfile">2.3.7. saveAsHadoopFile</h3>
<p>saveAsHadoopFile    Saves RDD to any hadoop supported file system (HDFS, S3, ElasticSearch, e.t.c), It uses Hadoop OutputFormat class to save.</p>
<h3 id="238-saveasnewapihadoopdataset">2.3.8. saveAsNewAPIHadoopDataset</h3>
<p>saveAsNewAPIHadoopDataset   Saves RDD to any hadoop supported file system (HDFS, S3, ElasticSearch, e.t.c) with new Hadoop API, It uses Hadoop Configuration object to save.</p>
<h3 id="239-saveasnewapihadoopfile">2.3.9. saveAsNewAPIHadoopFile</h3>
<p>saveAsNewAPIHadoopFile  Saves RDD to any hadoop supported fule system (HDFS, S3, ElasticSearch, e.t.c), It uses new Hadoop API OutputFormat class to save.</p>
<h1 id="3">3. 参考资料</h1>
<p>https://intellipaat.com/blog/tutorial/spark-tutorial/working-with-keyvalue-pairs/</p>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>