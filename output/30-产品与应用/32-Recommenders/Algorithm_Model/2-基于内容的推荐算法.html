<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>1. Collaborative Filtering - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#30-产品与应用">30-产品与应用</a>&nbsp;»&nbsp;<a href="/Wiki/#-32-Recommenders">32-Recommenders</a>&nbsp;»&nbsp;<a href="/Wiki/#-Algorithm_Model">Algorithm_Model</a>&nbsp;»&nbsp;1. Collaborative Filtering</div>
</div>
<div class="clearfix"></div>
<div id="title">1. Collaborative Filtering</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">概况</a></li>
<li><a href="#profile-learning">Profile Learning</a><ul>
<li><a href="#k-nearest-neighborknn">最近邻方法（k-Nearest Neighbor，简称kNN）</a></li>
<li><a href="#rocchio">Rocchio算法</a></li>
</ul>
</li>
<li><a href="#lr">逻辑回归 (LR)</a></li>
</ul>
</div>
<h1 id="_1">概况</h1>
<p>The content-based model recommends based on similarity of the items and/or users using their description/metadata/profile. </p>
<p>通过商品/用户的特征的相似度进行预测</p>
<p>In this method, the algorithm is trained to understand the context of the content and find similarities in other content to recommend the same class of content to a particular user.</p>
<p>CB的过程一般包括以下三步：</p>
<ol>
<li>
<p>Item Representation：为每个item抽取出一些特征（也就是item的content了）来表示此item；</p>
</li>
<li>
<p>Profile Learning：利用一个用户过去喜欢（及不喜欢）的item的特征数据，来学习出此用户的喜好特征（profile）；</p>
</li>
<li>
<p>Recommendation Generation：通过比较上一步得到的用户profile与候选item的特征，为此用户推荐一组相关性最大的item。</p>
</li>
</ol>
<p>[3]中对于上面的三个步骤给出一张很细致的流程图（第一步对应着Content Analyzer，第二步对应着Profile Learner，第三步对应着Filtering Component）</p>
<h1 id="profile-learning">Profile Learning</h1>
<div class="hlcode"><pre>   <span class="err">假设用户</span><span class="n">u</span><span class="err">已经对一些</span><span class="n">item</span><span class="err">给出了他的喜好判断，喜欢其中的一部分</span><span class="n">item</span><span class="err">，不喜欢其中的另一部分。那么，这一步要做的就是通过用户</span><span class="n">u</span><span class="err">过去的这些喜好判断，为他产生一个模型。有了这个模型，我们就可以根据此模型来判断用户</span><span class="n">u</span><span class="err">是否会喜欢一个新的</span><span class="n">item</span><span class="err">。所以，我们要解决的是一个典型的有监督分类问题，理论上机器学习里的分类算法都可以照搬进这里。</span>

  <span class="err">下面我们简单介绍下</span><span class="n">CB</span><span class="err">里常用的一些学习算法：</span>
</pre></div>


<h2 id="k-nearest-neighborknn">最近邻方法（k-Nearest Neighbor，简称kNN）</h2>
<div class="hlcode"><pre>  <span class="err">对于一个新的</span><span class="n">item</span><span class="err">，最近邻方法首先找用户</span><span class="n">u</span><span class="err">已经评判过并与此新</span><span class="n">item</span><span class="err">最相似的</span><span class="n">k</span><span class="err">个</span><span class="n">item</span><span class="err">，然后依据用户</span><span class="n">u</span><span class="err">对这</span><span class="n">k</span><span class="err">个</span><span class="n">item</span><span class="err">的喜好程度来判断其对此新</span><span class="n">item</span><span class="err">的喜好程度。这种做法和</span><span class="n">CF</span><span class="err">中的</span><span class="n">item</span><span class="o">-</span><span class="n">based</span> <span class="n">kNN</span><span class="err">很相似，差别在于这里的</span><span class="n">item</span><span class="err">相似度是根据</span><span class="n">item</span><span class="err">的属性向量计算得到，而</span><span class="n">CF</span><span class="err">中是根据所有用户对</span><span class="n">item</span><span class="err">的评分计算得到。</span>

  <span class="err">对于这个方法，比较关键的可能就是如何通过</span><span class="n">item</span><span class="err">的属性向量计算</span><span class="n">item</span><span class="err">之间的两两相似度。</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="err">中建议对于结构化数据，相似度计算使用欧几里得距离；而如果使用向量空间模型（</span><span class="n">VSM</span><span class="err">）来表示</span><span class="n">item</span><span class="err">的话，则相似度计算可以使用</span><span class="n">cosine</span><span class="err">。</span>
</pre></div>


<h2 id="rocchio">Rocchio算法</h2>
<div class="hlcode"><pre>  <span class="n">Rocchio</span><span class="err">算法是信息检索中处理相关反馈（</span><span class="n">Relevance</span> <span class="n">Feedback</span><span class="err">）的一个著名算法。比如你在搜索引擎里搜“苹果”，当你最开始搜这个词时，搜索引擎不知道你到底是要能吃的水果，还是要不能吃的苹果，所以它往往会尽量呈现给你各种结果。当你看到这些结果后，你会点一些你觉得相关的结果（这就是所谓的相关反馈了）。然后如果你翻页查看第二页的结果时，搜索引擎可以通过你刚才给的相关反馈，修改你的查询向量取值，重新计算网页得分，把跟你刚才点击的结果相似的结果排前面。比如你最开始搜索“苹果”时，对应的查询向量是</span><span class="p">{</span><span class="err">“苹果”</span> <span class="o">:</span> <span class="mi">1</span><span class="p">}</span><span class="err">。而当你点击了一些与</span><span class="n">Mac</span><span class="err">、</span><span class="n">iPhone</span><span class="err">相关的结果后，搜索引擎会把你的查询向量修改为</span><span class="p">{</span><span class="err">“苹果”</span> <span class="o">:</span> <span class="mi">1</span><span class="p">,</span> <span class="err">“</span><span class="n">Mac</span><span class="err">”</span> <span class="o">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="err">“</span><span class="n">iPhone</span><span class="err">”</span> <span class="o">:</span> <span class="mf">0.7</span><span class="p">}</span><span class="err">，通过这个新的查询向量，搜索引擎就能比较明确地知道你要找的是不能吃的苹果了。</span><span class="n">Rocchio</span><span class="err">算法的作用就是用来修改你的查询向量的：</span><span class="p">{</span><span class="err">“苹果”</span> <span class="o">:</span> <span class="mi">1</span><span class="p">}</span>  <span class="o">--&gt;</span> <span class="p">{</span><span class="err">“苹果”</span> <span class="o">:</span> <span class="mi">1</span><span class="p">,</span> <span class="err">“</span><span class="n">Mac</span><span class="err">”</span> <span class="o">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="err">“</span><span class="n">iPhone</span><span class="err">”</span> <span class="o">:</span> <span class="mf">0.7</span><span class="p">}</span><span class="err">。</span>

  <span class="err">在</span><span class="n">CB</span><span class="err">里，我们可以类似地使用</span><span class="n">Rocchio</span><span class="err">算法来获得用户</span><span class="n">u</span><span class="err">的</span><span class="n">profileImage</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span><span class="err">：</span>
</pre></div>


<p>Image(12)</p>
<p>其中Image(13)表示item j的属性，Image(14)与Image(15)分别表示已知的用户u喜欢与不喜欢的item集合；而Image(16)与Image(17)为正负反馈的权重，它们的值由系统给定。</p>
<div class="hlcode"><pre>  <span class="err">在获得</span><span class="n">Image11</span><span class="err">后，对于某个给定的</span><span class="n">item</span> <span class="n">j</span><span class="err">，我们可以使用</span><span class="n">Image</span><span class="p">(</span><span class="mi">11</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="err">与</span><span class="n">Image</span><span class="p">(</span><span class="mi">13</span><span class="p">)</span><span class="err">的相似度来代表用户</span><span class="n">u</span><span class="err">对</span><span class="n">j</span><span class="err">的喜好度。</span>

  <span class="n">Rocchio</span><span class="err">算法的一个好处是</span><span class="n">Image</span><span class="p">(</span><span class="mi">11</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="err">可以根据用户的反馈实时更新，其更新代价很小。</span>

  <span class="err">正如在本节开头所说，本节要解决的是一个典型的有监督分类问题。所以各种有效的分类机器学习算法都可以用到这里，下面列举几个常用的分类算法：</span>
</pre></div>


<ol>
<li>
<p>决策树算法（Decision Tree，简称DT）</p>
<p>当item的属性较少而且是结构化属性时，决策树一般会是个好的选择。这种情况下决策树可以产生简单直观、容易让人理解的结果。而且我们可以把决策树的决策过程展示给用户u，告诉他为什么这些item会被推荐。但是如果item的属性较多，且都来源于非结构化数据（如item是文章），那么决策树的效果可能并不会很好。</p>
</li>
<li>
<p>线性分类算法（Linear Classifer，简称LC）</p>
<p>对于我们这里的二类问题，线性分类器（LC）尝试在高维空间找一个平面，使得这个平面尽量分开两类点。也就是说，一类点尽可能在平面的某一边，而另一类点尽可能在平面的另一边。</p>
<p>仍以学习用户u的分类模型为例。Image(13)[5]表示item j的属性向量，那么LC尝试在Image(13)[5]空间中找平面Image(18)，使得此平面尽量分开用户u喜欢与不喜欢的item。其中的Image(19)就是我们要学习的参数了。最常用的学习Image19的方法就是梯度下降法了，其更新过程如下：</p>
</li>
</ol>
<p>Image(20)</p>
<p>其中的上角标t表示第t次迭代，Image(21)表示用户u对item j的打分（例如喜欢则值为1，不喜欢则值为-1）。Image(22)为学习率，它控制每步迭代变化多大，由系统给定。</p>
<div class="hlcode"><pre> <span class="err">和</span><span class="n">Rocchio</span><span class="err">算法一样，上面更新公式的好处就是它可以以很小的代价进行实时更新，实时调整用户</span><span class="n">u</span><span class="err">对应的</span><span class="n">Image19</span><span class="err">。</span>

 <span class="err">说到这里，很多童鞋可能会想起一些著名的线性分类器：</span><span class="n">Logistic</span> <span class="n">Regression</span><span class="err">和</span><span class="n">Linear</span> <span class="n">SVM</span><span class="err">等等，它们当然能胜任我们这里的分类任务。</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="err">中提到</span><span class="n">Linear</span> <span class="n">SVM</span><span class="err">用在文本分类上能获得相当不错的效果</span><span class="o">:</span><span class="p">)</span><span class="err">。</span>

 <span class="err">如果</span><span class="n">item</span><span class="err">属性</span><span class="n">Image</span><span class="p">(</span><span class="mi">13</span><span class="p">)[</span><span class="mi">5</span><span class="p">]</span><span class="err">的每个分量都是</span><span class="mi">0</span><span class="o">/</span><span class="mi">1</span><span class="err">取值的话（如</span><span class="n">item</span><span class="err">为文章，</span><span class="n">Image</span><span class="p">(</span><span class="mi">13</span><span class="p">)[</span><span class="mi">5</span><span class="p">]</span><span class="err">的第</span><span class="n">k</span><span class="err">个分量为</span><span class="mi">1</span><span class="err">表示词典中第</span><span class="n">k</span><span class="err">个词在</span><span class="n">item</span> <span class="n">j</span><span class="err">中，为</span><span class="mi">0</span><span class="err">表示第</span><span class="n">k</span><span class="err">个词不在</span><span class="n">item</span> <span class="n">j</span><span class="err">中），那么还有一种很有意思的启发式更新</span><span class="n">Image19</span><span class="err">的算法：</span><span class="n">Winnow</span><span class="err">算法。</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="err">中就是使用</span><span class="n">Winnow</span><span class="err">算法来获得</span><span class="n">user</span> <span class="n">profile</span><span class="err">的。</span>
</pre></div>


<ol>
<li>
<p>朴素贝叶斯算法（Naive Bayes，简称NB）</p>
<p>NB算法就像它的简称一样，牛逼！NB经常被用来做文本分类，它假设在给定一篇文章的类别后，其中各个词出现的概率相互独立。它的假设虽然很不靠谱，但是它的结果往往惊人地好。再加上NB的代码实现比较简单，所以它往往是很多分类问题里最先被尝试的算法。我们现在的profile learning问题中包括两个类别：用户u喜欢的item，以及他不喜欢的item。在给定一个item的类别后，其各个属性的取值概率互相独立。我们可以利用用户u的历史喜好数据训练NB，之后再用训练好的NB对给定的item做分类。NB的介绍很多，这里就不再啰嗦了，有不清楚的童鞋可以参考NB Wiki，或者[1-3]。</p>
</li>
</ol>
<h1 id="lr">逻辑回归 (LR)</h1>
<p>将推荐问题转换为一个CTR预估问题</p>
<p>流行原因：1）数学含义上的支撑 2）可解释性强 3）工程化的需要</p>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>