<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>Tensorflow2.x--分布式Tensorflow - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Science">Data_Science</a>&nbsp;»&nbsp;<a href="/Wiki/#-Library_Platform">Library_Platform</a>&nbsp;»&nbsp;<a href="/Wiki/#-05-Tensorflow 2.x">05-Tensorflow 2.x</a>&nbsp;»&nbsp;Tensorflow2.x--分布式Tensorflow</div>
</div>
<div class="clearfix"></div>
<div id="title">Tensorflow2.x--分布式Tensorflow</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1-tfdistributestrategy">1. 使用 tf.distribute.Strategy</a><ul>
<li><a href="#11">1.1. 概要</a></li>
<li><a href="#12">1.2. 分布式策略</a></li>
</ul>
</li>
<li><a href="#2">2. 实践</a><ul>
<li><a href="#21">2.1. 基本模式</a></li>
<li><a href="#22">2.2. 单机策略</a><ul>
<li><a href="#221-onedevicestrategy-">2.2.1. OneDeviceStrategy--单机单卡</a></li>
<li><a href="#222-mirroredstrategy-">2.2.2. MirroredStrategy--单机多卡</a><ul>
<li><a href="#2221-nccl">2.2.2.1. NCCL</a></li>
<li><a href="#2222-hierarchicalcopyallreduce">2.2.2.2. HierarchicalCopyAllReduce</a></li>
<li><a href="#2223-reductiontoonedevice">2.2.2.3. ReductionToOneDevice</a></li>
</ul>
</li>
<li><a href="#223-centralstoragestrategy-">2.2.3. CentralStorageStrategy--单机多卡</a><ul>
<li><a href="#2231-mirroredstrategy">2.2.3.1. 注意--与MirroredStrategy的区别</a></li>
<li><a href="#2232-mirroredstrategy-centralstoragestrategy">2.2.3.2. 选择 MirroredStrategy/ CentralStorageStrategy?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#23">2.3. 多机策略</a><ul>
<li><a href="#231-multiworkermirroredstrategy-all-reduce">2.3.1. MultiWorkerMirroredStrategy--多机多卡 all-reduce</a></li>
<li><a href="#232-parameterserverstrategy">2.3.2. ParameterServerStrategy</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3">3. 存储与数据问题</a><ul>
<li><a href="#31">3.1. 数据源的存储策略</a><ul>
<li><a href="#_1">在分布式文件系统上使用分布式系统接口</a></li>
<li><a href="#_2">在单机文件系统上的创建副本</a></li>
</ul>
</li>
<li><a href="#32-input">3.2. 分布式 input</a><ul>
<li><a href="#321-batching">3.2.1. Batching 分批</a></li>
<li><a href="#322-shard">3.2.2. shard 分片</a><ul>
<li><a href="#auto">AUTO</a></li>
<li><a href="#file">FILE</a></li>
<li><a href="#data">DATA</a></li>
<li><a href="#off">OFF</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#_3">性能</a></li>
<li><a href="#4">4. 参考资料</a></li>
</ul>
</div>
<h1 id="1-tfdistributestrategy">1. 使用 tf.distribute.Strategy</h1>
<h2 id="11">1.1. 概要</h2>
<p><code>tf.distribute.Strategy</code> 是TensorFlow的一个用于进行 多GPU/多设备/多TPU 分布式训练的API。</p>
<p>使用这个API，可以以最小的改动实现分布式训练。</p>
<p><code>tf.distribute.Strategy</code> 设计的3个原则:<br />
1. 上手简单，可供包括研究员和机器学习工程师在内的多部门使用。<br />
2. 提供开箱即用的高性能计算。<br />
3. 在不同策略之间切换简单。</p>
<p><code>tf.distribute.Strategy</code> 配合Keras使用效果更好。</p>
<p><code>tf.distribute.Strategy</code> 支持<a href="../tutorials/eager/tf_function.ipynb"><code>tf.function</code></a>.模式进行编程开放，同时适用于模型训练和推理。</p>
<p>可以以很少的改动使用 <code>tf.distribute.Strategy</code></p>
<h2 id="12">1.2. 分布式策略</h2>
<p><code>tf.distribute.Strategy</code> 试图覆盖大部分的分布式训练的案例，这些案例的维度包括：</p>
<ol>
<li>覆盖异步/同步训练（<em>Synchronous vs asynchronous training</em>）</li>
<li>覆盖大部分硬件平台</li>
</ol>
<p>异步/同步训练 是基于数据并行策略训练的方式</p>
<ul>
<li>In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. </li>
<li>
<p>In async training, all workers are independently training over the input data and updating variables asynchronously. Typically sync training is supported via all-reduce and async through parameter server architecture.</p>
</li>
<li>
<p>MirroredStrategy</p>
</li>
<li>TPUStrategy</li>
<li>MultiWorkerMirroredStrategy</li>
<li>CentralStorageStrategy   </li>
<li>ParameterServerStrategy</li>
<li>OneDeviceStrategy</li>
</ul>
<table>
<thead>
<tr>
<th align="left">Training API</th>
<th align="left">MirroredStrategy</th>
<th align="left">TPUStrategy</th>
<th align="left">MultiWorkerMirroredStrategy</th>
<th align="left">CentralStorageStrategy</th>
<th align="left">ParameterServerStrategy</th>
<th align="left">OneDeviceStrategy</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><strong>Keras API</strong></td>
<td align="left">Supported</td>
<td align="left">Experimental support</td>
<td align="left">Experimental support</td>
<td align="left">Experimental support</td>
<td align="left">Supported planned post 2.0</td>
<td align="left">Supported</td>
</tr>
<tr>
<td align="left"><strong>Custom training loop</strong></td>
<td align="left">Experimental support</td>
<td align="left">Experimental support</td>
<td align="left">Support planned post 2.0</td>
<td align="left">Support planned post 2.0</td>
<td align="left">No support yet</td>
<td align="left">Supported</td>
</tr>
<tr>
<td align="left"><strong>Estimator API</strong></td>
<td align="left">Limited Support</td>
<td align="left">Not supported</td>
<td align="left">Limited Support</td>
<td align="left">Limited Support</td>
<td align="left">Limited Support</td>
<td align="left">Limited Support</td>
</tr>
</tbody>
</table>
<h1 id="2">2. 实践</h1>
<h2 id="21">2.1. 基本模式</h2>
<div class="hlcode"><pre><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">()</span>

<span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
   <span class="c"># 1. build model</span>
   <span class="n">model</span><span class="o">=</span><span class="n">get_model</span><span class="p">()</span>

   <span class="c"># 2. compile modle</span>

   <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

   <span class="c"># 3. fit model </span>
   <span class="n">model</span><span class="o">.</span><span class="n">fit</span> <span class="p">()</span>
   <span class="o">...</span>
</pre></div>


<h2 id="22">2.2. 单机策略</h2>
<h3 id="221-onedevicestrategy-">2.2.1. OneDeviceStrategy--单机单卡</h3>
<div class="hlcode"><pre><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">OneDeviceStrategy</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s">&quot;/gpu:0&quot;</span><span class="p">)</span>
</pre></div>


<h3 id="222-mirroredstrategy-">2.2.2. MirroredStrategy--单机多卡</h3>
<p><img alt="" src="https://pic4.zhimg.com/80/v2-7b05382a7a62e3664ebc83f1272ea9e3_720w.jpg" /></p>
<p><img alt="" src="https://theaisummer.com/static/72f7634fe4cc7d260ba081bdb345e7bb/0012b/multi-gpu-system.png" /></p>
<p>镜像策略用了高效的All-reduce算法来实现设备之间变量的传递更新。默认情况下，它使用NVIDIA NCCL作为all-reduce实现。用户还可以在官方提供的其他几个选项之间进行选择。</p>
<p><strong>特点</strong><br />
in-graph replication with synchronous</p>
<p>MirroredStrategy是一种支持<strong>多张GPU</strong>在<strong>同一个机器</strong>上的同步训练方法。在训练开始时，Mirrored会在每张卡上复制一份模型，</p>
<p>每个显卡会收到<code>tf.data.Dataset</code>传来的数据，独立计算梯度，然后采用all-reduce的方法进行同步更新。多个显卡在通信时默认使用Nvidia NCCL进行。</p>
<p>我们可以深入MirroredStrategy的实现了解一下。基本上所有的distributed strategy都是通过某些collective ops和cross device ops进行数据通讯。MirroredStrategy也是如此，它是这样选择cross device ops的：</p>
<div class="hlcode"><pre><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">cross_device_ops</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="c"># strategy = tf.distribute.MirroredStrategy(devices=[&quot;/gpu:0&quot;, &quot;/gpu:1&quot;])</span>

<span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
   <span class="c"># 1. build model</span>
   <span class="n">model</span><span class="o">=</span><span class="n">get_model</span><span class="p">()</span>

   <span class="c"># 2. compile modle</span>
   <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

   <span class="c"># 3. fit model </span>
   <span class="n">model</span><span class="o">.</span><span class="n">fit</span> <span class="p">()</span>
</pre></div>


<p>官方也提供了一些all-reduce实现(tf.distribute.CrossDeviceOps)：</p>
<div class="hlcode"><pre><span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">HierarchicalCopyAllReduce</span>
<span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">ReductionToOneDevice</span>
<span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">NcclAllReduce</span> <span class="p">(</span><span class="n">default</span><span class="p">)</span>
</pre></div>


<h4 id="2221-nccl">2.2.2.1. NCCL</h4>
<p><code>MirroredStrategy</code> 类默认使用 NVIDIA Collective Communications 库（NCCL,用于GPU之间通讯的）做 AllReduce 平均值运算, 但依赖 GPU 的数量和类型</p>
<p>参考资料:https://github.com/veaba/tensorflow-docs/blob/39c8233ef6f46521d44edeaa08002da0be9b6df8/docs/tf.distribute/NcclAllReduce.md</p>
<h4 id="2222-hierarchicalcopyallreduce">2.2.2.2. HierarchicalCopyAllReduce</h4>
<p>This is a reduction created for Nvidia DGX-1 which assumes GPUs connects likethat on DGX-1 machine. </p>
<p>If you have different GPU inter-connections, it islikely that it would be slower than <code>tf.distribute.ReductionToOneDevice .</code></p>
<p>如果不是 Nvidia DGX-1 ,可能比<code>tf.distribute.ReductionToOneDevice .</code>慢</p>
<p>参考资料:<br />
1. http://nvidia.zhidx.com/content-9-1608-1.html</p>
<p><img alt="" src="https://pic2.zhimg.com/80/v2-249e0641218cab2091df8d74d0502972_720w.jpg?source=1940ef5c" /></p>
<h4 id="2223-reductiontoonedevice">2.2.2.3. ReductionToOneDevice</h4>
<p>总是先还原到一个设备，然后再进行广播。</p>
<div class="hlcode"><pre><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">MirroredStrategy</span><span class="p">(</span><span class="n">devices</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">cross_device_ops</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">ReductionToOneDevice</span><span class="p">)</span>
</pre></div>


<p>参考资料:https://github.com/veaba/tensorflow-docs/blob/39c8233ef6f46521d44edeaa08002da0be9b6df8/docs/tf.distribute/ReductionToOneDevice.md</p>
<h3 id="223-centralstoragestrategy-">2.2.3. CentralStorageStrategy--单机多卡</h3>
<p><code>tf.distribute.experimental.CentralStorageStrategy</code>  <br />
也执行同步训练，但是变量不会被镜像，而是放在CPU上。各操作(operation)在本地GPU之间复制进行。如果只有一个GPU，变量和操作都会放在GPU上。</p>
<p><img alt="" src="https://tse2-mm.cn.bing.net/th/id/OIP.ymuwRWOecXOsUP1g25ewgAHaFF?pid=ImgDet&amp;rs=1" /></p>
<p><img alt="" src="https://pic4.zhimg.com/80/v2-ed1e40774fe67adb535321df99dc991f_1440w.jpg" /><br />
单机多卡是指单台服务器有多块GPU设备。假设一台机器上有4块GPU，单机多GPU的训练过程如下：</p>
<ol>
<li>在单机单GPU的训练中，数据是一个batch一个batch的训练。 在单机多GPU中，数据一次处理4个batch(假设是4个GPU训练）， 每个GPU处理一个batch的数据计算。</li>
<li>变量，或者说参数，保存在CPU上。数据由CPU分发给4个GPU，在GPU上完成计算，得到每个批次要更新的梯度</li>
<li>在CPU上收集完4个GPU上要更新的梯度，计算一下平均梯度，然后更新。</li>
<li>循环进行上面步骤</li>
</ol>
<blockquote>
<p><strong>注意</strong>: 该策略是 <strong>实验性的</strong> ，因为我们正在对它进行改进，使他能在更多场景下工作. 敬请期待此API的变化</p>
</blockquote>
<p>synchronous 同步训练.<br />
变量不进行复制和分发，</p>
<div class="hlcode"><pre><span class="c"># Create a CentralStorageStrategy by:</span>

<span class="n">central_storage_strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">CentralStorageStrategy</span><span class="p">()</span>
</pre></div>


<p>这会创建一个 CentralStorageStrategy 实例使用所有可见的CPU和GPU。在更新应用到变量之前，不同副本上变量的更新将会汇总。</p>
<h4 id="2231-mirroredstrategy">2.2.3.1. 注意--与MirroredStrategy的区别</h4>
<p>If you use 4 GPUs, MirroredStrategy <code>will create 4 variables instead of "my_var" variable, one on each GPU</code>. However each variable will have the same value, because they are always updated in the same way. So the variable updates happen in sync on all the GPUs.</p>
<p>In case of the CentralStorageStrategy, only one variable is created for "my_var", in the host (CPU) memory. The updates only happen in one place.</p>
<h4 id="2232-mirroredstrategy-centralstoragestrategy">2.2.3.2. 选择 MirroredStrategy/ CentralStorageStrategy?</h4>
<p>依据：<br />
1. 节点的物理拓扑结构<br />
2. CPU-GPU 通信速度与GPU-GPU 通信速度比较<br />
   1. 若 GPU-GPU 比 CPU-GPU 快 MirroredStrategy好<br />
   2. 一般情况下CPU和GPU通信代价大，不建议使用CentralStorageStrategy</p>
<p>Which one is better probably depends on the computer's topology and how fast CPU-GPU communication is compared with GPU-GPU. </p>
<p>If the GPUs can communicate fast with each other, MirroredStrategy may be more efficient. But I'd benchmark it to be sure.</p>
<h2 id="23">2.3. 多机策略</h2>
<p>多机训练，需要配置环境变量 <code>TF_CONFIG</code>， 需要指定集群中ps和worker的配置，可以在 TF_CONFIG 中获取更多相关信息。</p>
<div class="hlcode"><pre><span class="p">{</span>
   <span class="nt">&quot;cluster&quot;</span><span class="p">:</span> <span class="p">{</span>
      <span class="err">#</span><span class="nt">&quot;chief&quot;</span><span class="p">:</span><span class="err">chief_hosts</span><span class="p">,</span> <span class="err">#</span> <span class="err">可不设置</span>
      <span class="nt">&quot;worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;host1:port&quot;</span><span class="p">,</span> <span class="s2">&quot;host2:port&quot;</span><span class="p">,</span> <span class="s2">&quot;host3:port&quot;</span><span class="p">],</span>
      <span class="nt">&quot;ps&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;host4:port&quot;</span><span class="p">,</span> <span class="s2">&quot;host5:port&quot;</span><span class="p">],</span>
      <span class="nt">&quot;evaluator&quot;</span><span class="p">:</span><span class="err">evaluator_hosts</span><span class="p">,</span> <span class="err">#</span> <span class="err">不做评估的话，可不设置</span>
   <span class="p">},</span>
   <span class="nt">&quot;task&quot;</span><span class="p">:</span> <span class="p">{</span><span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;worker&quot;</span><span class="p">,</span> <span class="nt">&quot;index&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p><code>TF_CONFIG</code> 主要由 <code>cluster</code> 集群信息 与 本机任务 <code>task</code> 组成 </p>
<p>此 <code>TF_CONFIG</code> 指定了集群中包含三个工作进程和两个 ps 任务，以及它们的主机和端口。"task" 部分指定当前任务在集群中的角色，即 worker 1（第二个工作进程）。集群中的有效角色是 "chief"、"worker"、"ps" 和 "evaluator"。除使用 <code>tf.distribute.experimental.ParameterServerStrategy</code> 时外，不应有 "ps" 作业</p>
<div class="hlcode"><pre><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s">&quot;TF_CONFIG&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s">&quot;cluster&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s">&quot;worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;host1:port&quot;</span><span class="p">,</span> <span class="s">&quot;host2:port&quot;</span><span class="p">,</span> <span class="s">&quot;host3:port&quot;</span><span class="p">],</span>         <span class="s">&quot;ps&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;host4:port&quot;</span><span class="p">,</span> <span class="s">&quot;host5:port&quot;</span><span class="p">]</span>     <span class="p">},</span>    <span class="s">&quot;task&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s">&quot;type&quot;</span><span class="p">:</span> <span class="s">&quot;worker&quot;</span><span class="p">,</span> <span class="s">&quot;index&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span> <span class="p">})</span>
</pre></div>


<h3 id="231-multiworkermirroredstrategy-all-reduce">2.3.1. MultiWorkerMirroredStrategy--多机多卡 all-reduce</h3>
<p><img alt="" src="https://pic4.zhimg.com/80/v2-7b05382a7a62e3664ebc83f1272ea9e3_720w.jpg" /></p>
<p><img alt="" src="https://picture.iczhiku.com/weixin/weixin16080120396692.png" /></p>
<p><code>tf.distribute.experimental.MultiWorkerMirroredStrategy</code>与MirroredStrategy非常类似，都在每一个device上存储一份模型的备份，进行同步的分布式训练。</p>
<p>该策略采用CollectiveOps作为多个worker之间通讯的操作。所谓的collective op是Tensorflow自己实现的根据当前硬件环境，网络结构，和Tensor大小自动采用最佳算法进行all-reduce的计算操作。一个collective op的实现逻辑十分简单</p>
<div class="hlcode"><pre><span class="k">if</span> <span class="p">(</span><span class="n">CanProceedWithCompute</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">col_exec</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span> <span class="p">{</span>
<span class="n">col_exec</span><span class="o">-&gt;</span><span class="n">ExecuteAsync</span><span class="p">(</span>
   <span class="n">c</span><span class="p">,</span> <span class="n">col_params_</span><span class="p">,</span> <span class="n">GetCollectiveKey</span><span class="p">(</span><span class="n">c</span><span class="p">),</span> <span class="n">actual_done</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>


<p>c是当前op的计算状态，col_exec是Tensorflow根据系统情况选择的collective executor，所有的all reduce，boardcast和receive操作都有collective executor去执行。</p>
<p>该策略目前也实现了很多优化，比如将很多个小tensor的all reduce操作变成几个大tensor的all reduce操作，以及在开发当中的采用最新NCCL 2.0进行通讯的操作，具体可以参见Issue 24505。可以看出Tensorflow分布式训练在被吐槽很多次后，感受到了来自Pytorch，Horovod的压力，在努力的提升自己。</p>
<p>最后，关于MultiWorkerMirroredStrategy的配置，有两点需要注意。</p>
<p>一点是collective ops的策略选择，目前支持CollectiveCommunication.RING，采用与Horovod类似的ring-based通讯策略。另一个是CollectiveCommunication.NCCL，采用Nvidia NCCL进行通讯，在启动策略时可以传入参数指定：</p>
<div class="hlcode"><pre><span class="n">multiworker_strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">CollectiveCommunication</span><span class="o">.</span><span class="n">NCCL</span><span class="p">)</span>
</pre></div>


<p>CollectiveCommunication.AUTO defers the choice to the runtime.</p>
<p>另一个需要注意的是关于TF_CONFIG的设置，该策略并不需要指定Parameter server，只需要一系列worker即可，其配置如下：</p>
<div class="hlcode"><pre><span class="n">TF_CONFIG</span> <span class="o">=</span> <span class="p">{</span>
<span class="s">&#39;cluster&#39;</span><span class="p">:</span> <span class="p">{</span>
   <span class="s">&#39;worker&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s">&#39;worker1:port1&#39;</span><span class="p">,</span> <span class="s">&#39;worker2:port2&#39;</span><span class="p">,</span> <span class="s">&#39;worker3:port3&#39;</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
<span class="p">},</span>
<span class="s">&#39;task&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s">&#39;type&#39;</span><span class="p">:</span> <span class="s">&#39;worker&#39;</span><span class="p">,</span> <span class="s">&#39;index&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="p">})</span>
</pre></div>


<p>目前该API尚处于实验阶段。如果在代码中通过MultiWorkerMirroredStrategy指定使用All-Reduce架构，则分布式提交时，TF_CONFIG环境变量中的cluster就不需要ps类型的节点了，例如：</p>
<div class="hlcode"><pre><span class="n">TF_CONFIG</span><span class="o">=</span><span class="s">&#39;{</span>
   <span class="s">&quot;cluster&quot;</span><span class="p">:</span> <span class="p">{</span>
      <span class="s">&quot;worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;host1:2222&quot;</span><span class="p">,</span> <span class="s">&quot;host2:2222&quot;</span><span class="p">,</span> <span class="s">&quot;host3:2222&quot;</span><span class="p">]</span>
   <span class="p">},</span>
   <span class="s">&quot;task&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s">&quot;type&quot;</span><span class="p">:</span> <span class="s">&quot;work&quot;</span><span class="p">,</span> <span class="s">&quot;index&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="p">}</span><span class="s">&#39;</span>



<span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
   <span class="n">train_distribute</span><span class="o">=</span><span class="n">strategy</span><span class="p">,</span> <span class="n">eval_distribute</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span>
<span class="n">regressor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">LinearRegressor</span><span class="p">(</span>
   <span class="n">feature_columns</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">feature_column</span><span class="o">.</span><span class="n">numeric_column</span><span class="p">(</span><span class="s">&#39;feats&#39;</span><span class="p">)],</span>
   <span class="n">optimizer</span><span class="o">=</span><span class="s">&#39;SGD&#39;</span><span class="p">,</span>
   <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>


<p>tf.keras例子</p>
<div class="hlcode"><pre><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="kn">as</span> <span class="nn">tfds</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size_per_replica</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">num_workers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s">&#39;TF_CONFIG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span>
   <span class="s">&#39;cluster&#39;</span><span class="p">:</span> <span class="p">{</span>
      <span class="s">&#39;worker&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;localhost:20000&quot;</span><span class="p">,</span> <span class="s">&quot;localhost:20001&quot;</span><span class="p">]</span>
   <span class="p">},</span>
   <span class="s">&#39;task&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s">&#39;type&#39;</span><span class="p">:</span> <span class="s">&#39;worker&#39;</span><span class="p">,</span> <span class="s">&#39;index&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="p">})</span>
<span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">MultiWorkerMirroredStrategy</span><span class="p">()</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size_per_replica</span> <span class="o">*</span> <span class="n">num_workers</span>

<span class="k">def</span> <span class="nf">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
   <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">])</span> <span class="o">/</span> <span class="mf">255.0</span>
   <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;cats_vs_dogs&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="n">tfds</span><span class="o">.</span><span class="n">Split</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span> <span class="n">as_supervised</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">resize</span><span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">1024</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

<span class="k">with</span> <span class="n">strategy</span><span class="o">.</span><span class="n">scope</span><span class="p">():</span>
   <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">MobileNetV2</span><span class="p">()</span>
   <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
      <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">),</span>
      <span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span><span class="p">,</span>
      <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">sparse_categorical_accuracy</span><span class="p">]</span>
   <span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">)</span>
</pre></div>


<h3 id="232-parameterserverstrategy">2.3.2. ParameterServerStrategy</h3>
<p><strong>Note</strong>：ParameterServerStrategy是Tensorflow最初的分布式训练方法。</p>
<p><img alt="" src="https://pic4.zhimg.com/80/v2-1416837956874bb92c719aca09634a17_720w.jpg" /></p>
<p>ParameterServerStrategy 由若干个parameter servers和若干个worker servers构成，parameter servers用于存储参数，worker servers用于计算。</p>
<div class="hlcode"><pre><span class="n">ps_strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">ParameterServerStrategy</span><span class="p">()</span>
</pre></div>


<p>ParameterServerStrategy 在训练过程中worker servers会和不同的parameter servers沟通获得参数，然后计算，向parameter servers传递参数的梯度。配置一个这样的训练环境非常简单，只需要在程序运行时设置好环境变量TF_CONFIG，需要注意的是需要给分布式集群里每一个机子不同的task。</p>
<div class="hlcode"><pre><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s">&quot;TF_CONFIG&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span>
  <span class="s">&quot;cluster&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="s">&quot;worker&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;host1:port&quot;</span><span class="p">,</span> <span class="s">&quot;host2:port&quot;</span><span class="p">,</span> <span class="s">&quot;host3:port&quot;</span><span class="p">],</span>
    <span class="s">&quot;ps&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;host4:port&quot;</span><span class="p">,</span> <span class="s">&quot;host5:port&quot;</span><span class="p">]</span>
  <span class="p">},</span>
  <span class="s">&quot;task&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s">&quot;type&quot;</span><span class="p">:</span> <span class="s">&quot;worker&quot;</span><span class="p">,</span> <span class="s">&quot;index&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="p">})</span>
</pre></div>


<p>同时，ParameterServerStrategy还有比较神奇的功能，它可以通过传入num_gpus_per_worker在一个worker上进行多GPU的同步计算，然后不同worker之间进行异步计算。但是由于单一worker上多GPU并没有利用NCCL进行通讯，而是直接将结果发送到CPU，所以效率非常低下。</p>
<p>但是 ParameterServerStrategy 不支持一机多卡</p>
<div class="hlcode"><pre><span class="n">strategy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distribute</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">ParameterServerStrategy</span><span class="p">()</span>
<span class="n">run_config</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">RunConfig</span><span class="p">(</span>
    <span class="n">experimental_distribute</span><span class="o">.</span><span class="n">train_distribute</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span>
<span class="n">estimator</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">Estimator</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">run_config</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">train_and_evaluate</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span><span class="o">...</span><span class="p">)</span>
 
</pre></div>


<h1 id="3">3. 存储与数据问题</h1>
<h2 id="31">3.1. 数据源的存储策略</h2>
<p>原则</p>
<blockquote>
<p>be able to able to access your data sourc</p>
</blockquote>
<p>不同的训练策略，都涉及到一个问题，训练数据的存储与读取。</p>
<h3 id="_1">在分布式文件系统上使用分布式系统接口</h3>
<p>数据可以远程存储 （the input data may be stored remotely,Google Cloud Storage or HDFS）</p>
<h3 id="_2">在单机文件系统上的创建副本</h3>
<p>数据也可以单一文件系统上,存储副本,相同名称</p>
<h2 id="32-input">3.2. 分布式 input</h2>
<p>关键问题</p>
<ol>
<li>batch_size </li>
<li>分片</li>
<li>缓存</li>
</ol>
<h3 id="321-batching">3.2.1. Batching 分批</h3>
<p>在分布式策略下, tf.distribute 会将数据集<code>rebatches</code> 再次分批, 所有新的 </p>
<p><code>batch size</code>=<code>global batch size</code>/<code>replicas</code></p>
<p><code>tf.distribute</code> <strong>rebatches</strong> the input <code>tf.data.Dataset</code> instance with a new batch size that is equal to the global batch size divided by the number of replicas in sync.</p>
<p>The number of replicas in sync is equal to the number of devices that are taking part in the gradient allreduce during training. When a user calls <code>next</code> on the distributed iterator, a per replica batch size of data is returned on each replica. The rebatched dataset cardinality will always be a multiple of the number of replicas. Here are a couple of<br />
examples:<br />
* <code>tf.data.Dataset.range(6).batch(4, drop_remainder=False)</code><br />
  * Without distribution:<br />
    * Batch 1: [0, 1, 2, 3]<br />
    * Batch 2: [4, 5]<br />
  * With distribution over 2 replicas.<br />
    The last batch ([4, 5]) is split between 2 replicas.</p>
<div class="hlcode"><pre><span class="o">*</span> <span class="n">Batch</span> <span class="mi">1</span><span class="o">:</span>
  <span class="o">*</span> <span class="n">Replica</span> <span class="mi">1</span><span class="o">:</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
  <span class="o">*</span> <span class="n">Replica</span> <span class="mi">2</span><span class="o">:</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="o">*</span> <span class="n">Batch</span> <span class="mi">2</span><span class="o">:</span>
  <span class="o">*</span> <span class="n">Replica</span> <span class="mi">2</span><span class="o">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span>
  <span class="o">*</span> <span class="n">Replica</span> <span class="mi">2</span><span class="o">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">]</span>
</pre></div>


<ul>
<li><code>tf.data.Dataset.range(4).batch(4)</code></li>
<li>Without distribution:<ul>
<li>Batch 1: [[0], [1], [2], [3]]</li>
</ul>
</li>
<li>
<p>With distribution over 5 replicas:</p>
<ul>
<li>Batch 1:</li>
<li>Replica 1: [0]</li>
<li>Replica 2: [1]</li>
<li>Replica 3: [2]</li>
<li>Replica 4: [3]</li>
<li>Replica 5: []</li>
</ul>
</li>
<li>
<p><code>tf.data.Dataset.range(8).batch(4)</code></p>
</li>
<li>Without distribution:<ul>
<li>Batch 1: [0, 1, 2, 3]</li>
<li>Batch 2: [4, 5, 6, 7]</li>
</ul>
</li>
<li>With distribution over 3 replicas:<ul>
<li>Batch 1:</li>
<li>Replica 1: [0, 1]</li>
<li>Replica 2: [2, 3]</li>
<li>Replica 3: []</li>
<li>Batch 2:</li>
<li>Replica 1: [4, 5]</li>
<li>Replica 2: [6, 7]</li>
<li>Replica 3: []</li>
</ul>
</li>
</ul>
<p>Note: The above examples only illustrate how a global batch is split on different replicas. It is not advisable to depend on the actual values that might end up on each replica as it can change depending on the implementation.</p>
<p>Rebatching the dataset has a space complexity that increases linearly with the number of replicas. This means that for the multi worker training use case the input pipeline can run into OOM errors. </p>
<div class="hlcode"><pre><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensors</span><span class="p">(([</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">]))</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
<span class="n">options</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Options</span><span class="p">()</span>
<span class="n">options</span><span class="o">.</span><span class="n">experimental_distribute</span><span class="o">.</span><span class="n">auto_shard_policy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AutoShardPolicy</span><span class="o">.</span><span class="n">DATA</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">with_options</span><span class="p">(</span><span class="n">options</span><span class="p">)</span>
</pre></div>


<h3 id="322-shard">3.2.2. shard 分片</h3>
<p>There are three different options that you can set for the <code>tf.data.experimental.AutoShardPolicy</code>:</p>
<h4 id="auto">AUTO</h4>
<ul>
<li>This is the default option which means an attempt will be made to shard by FILE. The attempt to shard by FILE fails if a file-based dataset is not detected. <code>tf.distribute</code> will then fall back to sharding by DATA. Note that if the input dataset is file-based but the number of files is less than the number of workers, an <code>InvalidArgumentError</code> will be raised. If this happens, explicitly set the policy to <code>AutoShardPolicy.DATA</code>, or split your input source into smaller files such that number of files is greater than number of workers.</li>
</ul>
<h4 id="file">FILE</h4>
<p>如果输入文件的数量远远大于辅助进程的数量，并且文件中的<strong>数据分布均匀</strong>，则应该使用此选项</p>
<p>This is the option if you want to shard the input files over all the workers. You should use this option if the number of input files is much larger than the number of workers and the data in the files is evenly distributed. The downside of this option is having idle workers if the data in the files is not evenly distributed. If the number of files is less than the number of workers, an <code>InvalidArgumentError</code> will be raised. If this happens, explicitly set the policy to <code>AutoShardPolicy.DATA</code>.</p>
<blockquote>
<blockquote>
<p>For example</p>
</blockquote>
</blockquote>
<p>let us distribute 2 files over 2 workers with 1 replica each. </p>
<div class="hlcode"><pre><span class="n">File</span> <span class="mi">1</span> <span class="n">contains</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span> 
<span class="n">File</span> <span class="mi">2</span> <span class="n">contains</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">]</span>
</pre></div>


<p>Let the total number of replicas in sync be 2 and global batch size be 4.</p>
<div class="hlcode"><pre>  <span class="o">*</span> <span class="n">Worker</span> <span class="mi">0</span><span class="o">:</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">1</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">1</span><span class="o">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">2</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">1</span><span class="o">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">3</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">1</span><span class="o">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">4</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">1</span><span class="o">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">]</span>
  <span class="o">*</span> <span class="n">Worker</span> <span class="mi">1</span><span class="o">:</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">1</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">2</span><span class="o">:</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">2</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">2</span><span class="o">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">3</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">2</span><span class="o">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">]</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">4</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">2</span><span class="o">:</span> <span class="p">[</span><span class="mi">11</span><span class="p">]</span>
</pre></div>


<h4 id="data">DATA</h4>
<p>通常 <br />
This will autoshard the elements across all the workers. Each of the workers will read the entire dataset and only process the shard assigned to it. </p>
<p>All other shards will be discarded. </p>
<p>This is generally used if the number of input files is less than the number of workers and you want better sharding of data across all workers. The downside is that the entire dataset will be read on each worker.</p>
<blockquote>
<blockquote>
<p>For example</p>
</blockquote>
</blockquote>
<p>let us distribute 1 files over 2 workers. </p>
<div class="hlcode"><pre><span class="cp"># 每个workers 都保存一个 File 1  副本</span>
<span class="n">File</span> <span class="mi">1</span> <span class="n">contains</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">].</span> 
</pre></div>


<p>Let the total number of replicas in sync be 2.</p>
<div class="hlcode"><pre>  <span class="o">*</span> <span class="n">Worker</span> <span class="mi">0</span><span class="o">:</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">1</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">1</span><span class="o">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">2</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">1</span><span class="o">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">3</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">1</span><span class="o">:</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
  <span class="o">*</span> <span class="n">Worker</span> <span class="mi">1</span><span class="o">:</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">1</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">2</span><span class="o">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">2</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">2</span><span class="o">:</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
    <span class="o">*</span> <span class="n">Batch</span> <span class="mi">3</span> <span class="o">=</span>  <span class="n">Replica</span> <span class="mi">2</span><span class="o">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">]</span>
</pre></div>


<h4 id="off">OFF</h4>
<p>If you turn off autosharding, each worker will process all the data.<br />
For example, let us distribute 1 files over 2 workers. File 1 contains [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. Let the total number of replicas in sync be 2. Then each worker will see the following distribution:</p>
<ul>
<li>
<p>Worker 0:</p>
<ul>
<li>Batch 1 =  Replica 1: [0, 1]</li>
<li>Batch 2 =  Replica 1: [2, 3]</li>
<li>Batch 3 =  Replica 1: [4, 5]</li>
<li>Batch 4 =  Replica 1: [6, 7]</li>
<li>Batch 5 =  Replica 1: [8, 9]</li>
<li>Batch 6 =  Replica 1: [10, 11]</li>
</ul>
</li>
<li>
<p>Worker 1:</p>
<ul>
<li>Batch 1 =  Replica 2: [0, 1]</li>
<li>Batch 2 =  Replica 2: [2, 3]</li>
<li>Batch 3 =  Replica 2: [4, 5]</li>
<li>Batch 4 =  Replica 2: [6, 7]</li>
<li>Batch 5 =  Replica 2: [8, 9]</li>
<li>Batch 6 =  Replica 2: [10, 11]</li>
</ul>
</li>
</ul>
<h1 id="_3">性能</h1>
<p>前面提到的文献在建模的时候都没有考虑分布式机器学习对位置的敏感性，<br />
事实上，现在业界的分布式机器学习集群大多采用虚拟化技术，采用了虚拟化方<br />
案的集群的使用者通常不能得到相关的位置信息。比如某使用者申请在该集群中<br />
分配两台虚拟机/容器，每台虚拟机/容器包含一块 GPU，该使用者通常并不能知<br />
道这两台虚拟机是否在同一台物理服务器上或者是否在同一个 PCIe switch 下。 <br />
VGG16 ResNet-50</p>
<p>标准化的训练速度<br />
SamePCIeSw  SameSocket  DiffSocket</p>
<p>图 3-4  位置对机器学习训练速度的影响 <br />
ResNet-50 VGG16 MobileNet</p>
<p>标准化的训练速度<br />
 无干扰<br />
 有干扰</p>
<p>图 3-5  干扰对机器学习训练速度的影响 <br />
最后，集群中的资源和任务实际上处于动态变化中。即使分配给某个任务的<br />
计算资源不变，实际上当集群中还有其他任务在运行的时候，它们会相互竞争<br />
PCIe、通信网络、存储等资源，也就是其他的任务会对该任务造成干扰。不同的<br />
机器学习任务对于其他任务的干扰的敏感程度也是不一样的。举个例子，有两台<br />
服务器通过千兆以太网相连，每台服务器有两块 1080  Ti  GPU，在这两台服务器<br />
上的两块 GPU 上（两块 GPU 分别分布在两台服务器上）在下面两种情况下：1）20 <br />
 电子科技大学硕士学位论文 <br />
22 </p>
<p>另外两块 GPU 没有在运行任何任务；2）另外两块 GPU 同时在训练 ResNet-50 模<br />
型；在 TensorFlow 平台上分别训练 VGG16、ResNet-50、Inception-v1 模型。如图<br />
3-5 所示，三个模型的训练速度在有干扰的情况下都会下降，并且三个模型的训<br />
练速度下降的程度不一样，即干扰对它们造成的影响不同。因为集群中的任务处<br />
于不断的变化中并且难以预测，所以来自其他任务的干扰也是非常难以建模的。<br />
同样的，前面提到的文献建模的时候也没有考虑其他任务的干扰。</p>
<h1 id="4">4. 参考资料</h1>
<ol>
<li>
<p><a href="https://picture.iczhiku.com/weixin/message1608012039669.html">机器之心：TensorFlow 2.4来了：上线对分布式训练和混合精度的新功能支持<br />
</a></p>
</li>
<li>
<p><a href="https://www.tensorflow.org/tutorials/distribute/input">Tensorflow 官网: Distributed Input</a></p>
</li>
</ol>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>