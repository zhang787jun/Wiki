<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>[测试]Tensorflow Profiler 性能优化 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript" async src="https://cdn.bootcss.com/mathjax-mhchem/3.3.2/mhchem.js">
    </script>

</head>

<body>
    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Data_Scince\Library_Platform\Tensorflow">Data_Scince\Library_Platform\Tensorflow</a>&nbsp;»&nbsp;[测试]Tensorflow Profiler 性能优化</div>
</div>
<div class="clearfix"></div>
<div id="title">[测试]Tensorflow Profiler 性能优化</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1">1. 工具</a><ul>
<li><a href="#tensorflow-profiler-ui">Tensorflow Profiler UI</a><ul>
<li><a href="#1-installation">1. 安装 Installation</a></li>
<li><a href="#2-profiler">2. 准备profiler 文件</a></li>
<li><a href="#3-ui">3. 启动 UI</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2">2.策略</a><ul>
<li><a href="#_1">硬件</a></li>
<li><a href="#jit">JIT 编译</a></li>
</ul>
</li>
<li><a href="#_2">配置开启即时编译</a></li>
<li><a href="#_3">参考资料</a></li>
</ul>
</div>
<h1 id="1">1. 工具</h1>
<h2 id="tensorflow-profiler-ui">Tensorflow Profiler UI</h2>
<p>??<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup></p>
<h3 id="1-installation">1. 安装 Installation</h3>
<ol>
<li>下载文件 profiler-ui 的源文件</li>
</ol>
<div class="hlcode"><pre><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="o">:</span><span class="c1">//github.com/tensorflow/profiler-ui.git</span>
</pre></div>


<p>1.Install Python dependencies. </p>
<div class="hlcode"><pre><span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">user</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="p">.</span><span class="n">txt</span>
</pre></div>


<p>2.Install pprof.</p>
<p>[1] 下载 安装go语言包 </p>
<p>安装包下载地址为：https://golang.org/dl/。<br />
如果打不开可以使用这个地址：https://golang.google.cn/dl/</p>
<p>参考：https://www.runoob.com/go/go-environment.html</p>
<p>装 pprof 的时候会有个坑点，CentOS 库中可以找到 gperftools 这个工具，也是 Google 提供的，yum 装上之后可执行文件的名字也叫 pprof ！！但是跟这里用到的 pprof 不是一个玩意！！</p>
<p>[2] 安装pprof</p>
<div class="hlcode"><pre><span class="k">go</span> <span class="nx">get</span> <span class="o">-</span><span class="nx">u</span> <span class="nx">github</span><span class="p">.</span><span class="nx">com</span><span class="o">/</span><span class="nx">google</span><span class="o">/</span><span class="nx">pprof</span>
</pre></div>


<h3 id="2-profiler">2. 准备profiler 文件</h3>
<p>3.Create a profile context file using the tf.contrib.tfprof.ProfileContext class. </p>
<p>生成持久化文件 <code>/path/to/your/profile.context</code></p>
<h3 id="3-ui">3. 启动 UI</h3>
<p>进入到  profiler-ui 的文件夹下 </p>
<div class="hlcode"><pre><span class="n">python</span> <span class="n">ui</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">profile_context_path</span><span class="o">=/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">your</span><span class="o">/</span><span class="n">profile</span><span class="o">.</span><span class="n">context</span>
</pre></div>


<h1 id="2">2.策略</h1>
<p><sup id="fnref:2"><a class="footnote-ref" href="#fn:2" rel="footnote">2</a></sup></p>
<h2 id="_1">硬件</h2>
<h2 id="jit">JIT 编译</h2>
<p>使用即时编译<br />
注意: 为了支持 XLA（加速线性代数），TensorFlow 必须从源文件编译。</p>
<p>为什么使用即时编译？<br />
TensorFlow / XLA 即时编译器通过 XLA 编译和运行 TensorFlow 图的各个部分。与标准的 TensorFlow 实现相比，XLA 的好处是可以将多个运算符（内核融合）融合到少量的编译内核中。与 TensorFlow 逐个运行操作相比，融合运算能减少对<strong>内存带宽</strong>的要求，同时提升性能。</p>
<p>通过 XLA 运行 TensorFlow 图<br />
有两种方式通过 XLA 运行 TensorFlow 计算图：一是用 CPU 或 GPU 设备上的即时编译操作，二是把操作放到 XLA_CPU 或 XLA_GPU TensorFlow 设备上。将操作直接放到一个 TensorFlow XLA<br />
设备上强制执行，因此这种方法主要用于测试。</p>
<p>注意：XLA CPU 后端会生成快速、单线程的代码，但是不会如 TensorFlow CPU 后端一样并行化。 XLA GPU 后端与标准的 TensorFlow 后端充分竞争，运行速度时快时慢。</p>
<p>开启即时编译<br />
即时编译可以在会话层开启，或手动进行选择操作。两种方式都是零拷贝 --- 数据在同台设备的已编译 XLA 内核和 TensorFlow 操作之间传递时，无需另行复制。</p>
<p>会话<br />
在会话层开启即时编译时，系统将尽可能把所有操作编译成 XLA 计算。每个 XLA 计算将被编译成单个或多个设备底层内核。</p>
<p>受某些限制影响，如果图模型中有两个相邻的操作都要使用 XLA，它们将会被编译成单个 XLA 计算。</p>
<p>通过将 global_jit_level 设置成tf.OptimizerOptions.ON_1，并在会话初始化阶段传入配置，就可以在会话层开启即时编译。</p>
<h1 id="_2">配置开启即时编译</h1>
<p>config = tf.ConfigProto()<br />
config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1</p>
<p>sess = tf.Session(config=config)<br />
注意：在会话层开启即时编译将不会导致为 CPU 编译操作。CPU 运算的即时编译必须通过下面描述的手动方法开启，原因在于 CPU 后端是单线程的。</p>
<p>手动开启<br />
对于单个或多个操作，可以手动开启即时编译，通过对运算进行标记以使用属性 _XlaCompile=true 来进行编译。最简单的方法就是通过在 tensorflow/contrib/compiler/jit.py<br />
中定义的 tf.contrib.compiler.jit.experimental_jit_scope() 。<br />
使用范例：</p>
<div class="hlcode"><pre><span class="n">jit_scope</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">compiler</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">experimental_jit_scope</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">with</span> <span class="n">jit_scope</span><span class="p">()</span><span class="o">:</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="err">#</span> <span class="n">add</span> <span class="err">将被</span> <span class="n">XLA</span> <span class="err">编译</span>
</pre></div>


<p>_XlaCompile 属性目前是以最佳的方式支持的。如果一个操作无法编译，TensorFlow 将默认回退到常规实现。</p>
<p>将操作加载到 XLA 设备中<br />
通过 XLA 执行计算的另一种方法是将操作载入到特定的 XLA 设备上。这个方法通常只用于测试。有效设备包括 XLA_CPU 或 XLA_GPU。</p>
<p>with tf.device("/job:localhost/replica:0/task:0/device:XLA_GPU:0"):<br />
  output = tf.add(input1, input2)<br />
不同于标准 CPU 和 GPU 设备上的即时编译，这些设备在传输到设备上和关闭设备时，会生成一个数据副本。额外的拷贝导致在同一个图模型中混合使用 XLA 和 TensorFlow 操作的开销变得很大。</p>
<p>教程<br />
这个教程涵盖了一个简单版的 MNIST softmax 训练模型。在会话层开启了即时编译，只支持 GPU。</p>
<p>在开始本教程之前，先验证 LD_LIBRARY 环境变量或者 ldconfig 包含 $CUDA_ROOT/extras/CUPTI/lib64，其中包含 CUDA 分析工具接口库<br />
(CUPTI)。TensorFlow 使用 CUPTI 从 GPU 获取追踪信息。</p>
<p>步骤 #1: 准备代码范例<br />
下载或移动 mnist_softmax_xla.py 到 TensorFlow 源码之外的文件夹中。</p>
<p>步骤 #2: 无 XLA 运行<br />
执行 python 代码，不用 XLA 训练模型。</p>
<p>python mnist_softmax_xla.py --xla=''<br />
使用 Chrome 跟踪事件探查器 (导航到 chrome://tracing)，当代码执行完时打开时间线文件 timeline.ctf.json。呈现的时间线类似于下图，其中有多个绿色框，标记为 MatMul，可能跨多个 GPU 。</p>
<p>步骤 #3：用 XLA 运行代码<br />
执行 python 代码，用 XLA 训练模型，并打开 XLA 调试工具，用环境变量输出 XLA 图。</p>
<p>TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python mnist_softmax_xla.py<br />
打开时间线文件(timeline.ctf.json)。呈现的时间线类似于下图，其中有一个标有 _XlaLaunch<br />
的长块。</p>
<p>通过查看控制台类似下面的输出来了解在 _XlaLaunch 里到底发生了什么:</p>
<p>computation cluster_0[_XlaCompiledKernel=true,_XlaNumConstantArgs=1].v82 [CPU:<br />
pipeline start, before inline]: /tmp/hlo_graph_0.dot<br />
控制台显示了包含 XLA 创建的图模型信息的 hlo_graph_xx.dot 文件位置。XLA 融合操作的过程可以从 hlo_graph_0.dot 开始逐个查看分析图了解。</p>
<p>为了将 .dot 文件渲染成 png 格式，需安装 GraphViz 并运行:</p>
<p>dot -Tpng hlo_graph_80.dot -o hlo_graph_80.png<br />
结果如下图：</p>
<h1 id="_3">参考资料</h1>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>profiler-ui Github pages https://github.com/tensorflow/profiler-ui&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>美团基于TensorFlow Serving的深度学习在线预估 https://tech.meituan.com/2018/10/11/tfserving-improve.html&#160;<a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>Performance Analysis of Just-in-Time Compilation<br />
for Training TensorFlow Multi-Layer Perceptrons&#160;<a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div>
</div>
<div id="re">
  如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多
</div>>


        /Wiki
        /Wiki
        /Wiki
    </div>
    <div id="footer">
        <span>
            Copyright © 2019 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>
    
</body>

</html>