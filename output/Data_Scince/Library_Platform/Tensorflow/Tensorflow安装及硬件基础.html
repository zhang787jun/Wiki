<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="./static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="./static/css/tango.css">
    <link rel="shortcut icon" href="./favicon.ico" type="image/x-icon">
    <link rel="icon" href="./favicon.ico" type="image/x-icon">
    <title>[环境部署]Tensorflow的硬件基础及安装 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/javascript" async src="https://cdn.bootcss.com/mathjax-mhchem/3.3.2/mhchem.js">
    </script>



</head>

<body>
    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="./">Home</a>&nbsp;»&nbsp;<a href="./#Data_Scince\Library_Platform\Tensorflow">Data_Scince\Library_Platform\Tensorflow</a>&nbsp;»&nbsp;[环境部署]Tensorflow的硬件基础及安装</div>
</div>
<div class="clearfix"></div>
<div id="title">[环境部署]Tensorflow的硬件基础及安装</div>
  <div id="content">
  <h1 id="tensorflow">Tensorflow的硬件基础及安装</h1>
<div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#tensorflow">Tensorflow的硬件基础及安装</a><ul>
<li><a href="#1">1. 硬件基础</a><ul>
<li><a href="#11-cpu">1.1 CPU</a><ul>
<li><a href="#111">1.1.1 指令集问题</a></li>
<li><a href="#112">1.1.2 虚拟化问题</a></li>
<li><a href="#113-cpu">1.1.3 CPU硬件识别</a></li>
</ul>
</li>
<li><a href="#12-gpu">1.2 GPU</a><ul>
<li><a href="#121-nvidia">1.2.1 NVIDIA独立显卡</a><ul>
<li><a href="#1211">1.2.1.1 显卡驱动</a></li>
<li><a href="#1212-cuda">1.2.1.2 CUDA</a><ul>
<li><a href="#1-cudagpu">1. 支持CUDA平台的GPU硬件型号</a></li>
<li><a href="#2-cuda-cuda-toolkit">2. CUDA 核心工具包(CUDA Toolkit)</a></li>
<li><a href="#3-cudacudnn">3. CUDA深度神经网络库(cuDNN)</a></li>
</ul>
</li>
<li><a href="#1213">1.2.1.3 性能监控</a></li>
</ul>
</li>
<li><a href="#122-amd">1.2.2 AMD 独立显卡</a><ul>
<li><a href="#1221">1.2.2.1. 显卡驱动</a></li>
<li><a href="#1222-rocm">1.2.2.2. ROCm 平台</a></li>
<li><a href="#1223">1.2.2.3. 性能监控</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#13">1.3 网络通信层</a><ul>
<li><a href="#131-grpc">1.3.1 gRPC</a><ul>
<li><a href="#1311-rpc">1.3.1.1 RPC 系统</a><ul>
<li><a href="#1_1">1. 系统组成</a></li>
<li><a href="#2-rpc">2. rpc 过程</a></li>
<li><a href="#3">3. 服务开启</a></li>
</ul>
</li>
<li><a href="#1312-grpc">1.3.1.2 gRPC 系统</a></li>
</ul>
</li>
<li><a href="#132-rdma">1.3.2 RDMA</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#2-tensorflow">2. Tensorflow 安装</a><ul>
<li><a href="#21">2.1 安装准备</a><ul>
<li><a href="#211-python">2.1.1 python 安装</a></li>
</ul>
</li>
<li><a href="#22-pip">2.2 pip 安装</a></li>
<li><a href="#23-docker">2.3 docker 安装</a><ul>
<li><a href="#231cpu">2.3.1CPU版本</a></li>
<li><a href="#232-gpu">2.3.2 GPU版本</a></li>
</ul>
</li>
<li><a href="#24">2.4  集群中安装</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<h2 id="1">1. 硬件基础</h2>
<h3 id="11-cpu">1.1 CPU</h3>
<h4 id="111">1.1.1 指令集问题</h4>
<blockquote>
<p><strong>背景</strong><br />
<code>指令集</code>是存储在CPU内部，对CPU运算进行指导和优化的硬程序。拥有这些指令集，CPU就可以更高效地运行。<br />
Intel主要有 [x86，EM64T，MMX，SSE，SSE2，SSE3，SSSE3 (Super SSE3)，SSE4A，SSE4.1，SSE4.2，AVX，AVX2，AVX-512，VMX] <strong>（时间排序）</strong> 等指令集。<br />
AMD主要是x86，x86-64，3D-Now!指令集。</p>
<p><strong>为兼容低版本CPU,从pip 源 安装的 tensorlfow 默认不适用高级指令集，使得运算没有充分利用硬件</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th align="left">厂家</th>
<th align="left">指令集</th>
<th align="left">说明</th>
<th align="left"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Intel</td>
<td align="left">SSE2</td>
<td align="left">Streaming SIMD Extensions</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">Intel</td>
<td align="left">AVX</td>
<td align="left">高级矢量扩展,Intel Advanced Vector Extensions (Intel AVX)</td>
<td align="left">AVX引入了融合乘法累加（FMA）运算，加速了线性代数计算，即点积，矩阵乘法，卷积</td>
</tr>
</tbody>
</table>
<h4 id="112">1.1.2 虚拟化问题</h4>
<p><strong><code>虚拟化</code></strong> 就是由位于下层的软件模块，根据上层的软件模块的期待，抽象（虚拟）出一个虚拟的软件或硬件模块，使上一层软件直接运行在这个与自己期待完全一致的虚拟环境上。</p>
<p>CPU 虚拟化 主要指 intel 的 VT-x 和 AMD 的 AMD-V 为主的硬件辅助的 CPU 虚拟化技术<br />
其中，Intel VT 包括 VT-x （支持 CPU 虚拟化）、EPT（支持内存虚拟化）和 VT-d（支持 I/O 虚拟化）</p>
<p>VMM 全称是 Virtual Machine Monitor，虚拟机监控系统，也叫 Hypervisor，是虚拟化层的具体实现。主要是<strong><code>以软件的方式，实现一套和物理主机环境完全一样的虚拟环境</code></strong>，物理主机有的所有资源，包括 CPU、内存、网络 IO、设备 IO等等</p>
<p>KVM 是一种硬件辅助的虚拟化技术，支持 Intel VT-x 和 AMD-v 技术，怎么知道 CPU 是否支持 KVM 虚拟化呢？可以通过如下命令查看：</p>
<p>CPU 是否支持虚拟化关系到是否能使用docker 等虚拟化工具 </p>
<h4 id="113-cpu">1.1.3 CPU硬件识别</h4>
<p>使用 <strong>英特尔® 处理器识别实用程序</strong> 产看 intel 系列 CPU 数据</p>
<p>https://www.intel.cn/content/www/cn/zh/support/products/5982/processors/processor-utilities-and-programs/intel-processor-identification-utility.html</p>
<h3 id="12-gpu">1.2 GPU</h3>
<p>集成显卡: 是指集成在CPU内部的GPU模块<br />
独立显卡: </p>
<p>查看设备GPU情况<br />
<em> For window<br />
设备管理器-&gt;显示适配器<br />
</em> For Linux</p>
<h4 id="121-nvidia">1.2.1 NVIDIA独立显卡</h4>
<h5 id="1211">1.2.1.1 显卡驱动</h5>
<p>下载地址：<br />
https://www.nvidia.cn/Download/index.aspx?lang=cn</p>
<h5 id="1212-cuda">1.2.1.2 CUDA</h5>
<p>CUDA（Compute Unified Device Architecture），是显卡厂商NVIDIA推出的运算平台。</p>
<h6 id="1-cudagpu">1. 支持CUDA平台的GPU硬件型号</h6>
<p>https://developer.nvidia.com/cuda-gpus</p>
<h6 id="2-cuda-cuda-toolkit">2. CUDA 核心工具包(CUDA Toolkit)</h6>
<p>下载地址：<br />
https://developer.nvidia.com/cuda-downloads<br />
* 注意平台及版本号，最新版本的cuda 工具包未必能和最新的tensorflow 对应上</p>
<p>Windows 平台下安装指南<br />
https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html<br />
其他平台安装指南<br />
https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html</p>
<p>安装要求</p>
<ol>
<li>A CUDA-capable GPU(硬件)</li>
<li>A supported version of Microsoft Windows（操作系统）</li>
<li>A supported version of Microsoft Visual Studio（注意非常重要!）</li>
<li>the NVIDIA CUDA Toolkit （安装包）</li>
</ol>
<p>安装成功检验：<br />
Fro windows:</p>
<div class="hlcode"><pre><span class="n">nvcc</span> <span class="o">--</span><span class="n">version</span> <span class="err">#</span> <span class="err">产看版本号</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="nl">nvcc:</span> <span class="n">NVIDIA</span> <span class="p">(</span><span class="n">R</span><span class="p">)</span> <span class="n">Cuda</span> <span class="n">compiler</span> <span class="n">driver</span>
<span class="n">Copyright</span> <span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="mi">2005</span><span class="o">-</span><span class="mi">2017</span> <span class="n">NVIDIA</span> <span class="n">Corporation</span>

<span class="n">Built</span> <span class="n">on</span> <span class="n">Fri_Sep__1_21</span><span class="o">:</span><span class="mi">08</span><span class="o">:</span><span class="mi">32</span><span class="n">_Central_Daylight_Time_2017</span>
<span class="n">Cuda</span> <span class="n">compilation</span> <span class="n">tools</span><span class="p">,</span> <span class="n">release</span> <span class="mf">9.0</span><span class="p">,</span> <span class="n">V9</span><span class="mf">.0.176</span>
</pre></div>


<p>For Linux:</p>
<div class="hlcode"><pre><span class="n">cat</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="n">version</span><span class="p">.</span><span class="n">txt</span>
</pre></div>


<h6 id="3-cudacudnn">3. CUDA深度神经网络库(cuDNN)</h6>
<p>The NVIDIA CUDA® Deep Neural Network library (cuDNN) 是主要适用于深度神经网络的GPU加速器， cuDNN 为一些标准的神经网络（如前反馈、后反馈、卷积、池化、活化等 ） 提供了高适应性的基础设施。<br />
下载地址：<br />
https://developer.nvidia.com/cudnn （需要注册）</p>
<p>For Linux</p>
<div class="hlcode"><pre><span class="n">cat</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="n">include</span><span class="o">/</span><span class="n">cudnn</span><span class="p">.</span><span class="n">h</span> <span class="o">|</span> <span class="n">grep</span> <span class="n">CUDNN_MAJOR</span> <span class="o">-</span><span class="n">A</span> <span class="mi">2</span>
</pre></div>


<h5 id="1213">1.2.1.3 性能监控</h5>
<p>nvidia-smi简称NVSMI，提供监控GPU使用情况和更改GPU状态的功能，是一个跨平台工具，它支持所有标准的NVIDIA驱动程序支持的Linux发行版以及从WindowsServer 2008 R2开始的64位的系统。该工具是NVIDIA显卡卡驱动附带的，只要安装好驱动后就会有它。</p>
<div class="hlcode"><pre><span class="n">nvidia</span><span class="o">-</span><span class="n">smi</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="n">Sun</span> <span class="n">Jun</span> <span class="mi">09</span> <span class="mi">17</span><span class="o">:</span><span class="mi">19</span><span class="o">:</span><span class="mi">48</span> <span class="mi">2019</span>
<span class="o">+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">NVIDIA</span><span class="o">-</span><span class="n">SMI</span> <span class="mf">385.54</span>                 <span class="n">Driver</span> <span class="n">Version</span><span class="o">:</span> <span class="mf">385.54</span>                    <span class="o">|</span>
<span class="o">|-------------------------------+----------------------+----------------------+</span>
<span class="o">|</span> <span class="n">GPU</span>  <span class="n">Name</span>            <span class="n">TCC</span><span class="o">/</span><span class="n">WDDM</span> <span class="o">|</span> <span class="n">Bus</span><span class="o">-</span><span class="n">Id</span>        <span class="n">Disp</span><span class="p">.</span><span class="n">A</span> <span class="o">|</span> <span class="n">Volatile</span> <span class="n">Uncorr</span><span class="p">.</span> <span class="n">ECC</span> <span class="o">|</span>
<span class="o">|</span> <span class="n">Fan</span>  <span class="n">Temp</span>  <span class="n">Perf</span>  <span class="n">Pwr</span><span class="o">:</span><span class="n">Usage</span><span class="o">/</span><span class="n">Cap</span><span class="o">|</span>         <span class="n">Memory</span><span class="o">-</span><span class="n">Usage</span> <span class="o">|</span> <span class="n">GPU</span><span class="o">-</span><span class="n">Util</span>  <span class="n">Compute</span> <span class="n">M</span><span class="p">.</span> <span class="o">|</span>
<span class="o">|===============================+======================+======================|</span>
<span class="o">|</span>   <span class="mi">0</span>  <span class="n">GeForce</span> <span class="mi">940</span><span class="n">MX</span>      <span class="n">WDDM</span>  <span class="o">|</span> <span class="mo">00000000</span><span class="o">:</span><span class="mo">01</span><span class="o">:</span><span class="mf">00.0</span> <span class="n">Off</span> <span class="o">|</span>                  <span class="n">N</span><span class="o">/</span><span class="n">A</span> <span class="o">|</span>
<span class="o">|</span> <span class="n">N</span><span class="o">/</span><span class="n">A</span>   <span class="mi">49</span><span class="n">C</span>    <span class="n">P8</span>    <span class="n">N</span><span class="o">/</span><span class="n">A</span> <span class="o">/</span>  <span class="n">N</span><span class="o">/</span><span class="n">A</span> <span class="o">|</span>     <span class="mi">26</span><span class="n">MiB</span> <span class="o">/</span>  <span class="mi">1024</span><span class="n">MiB</span> <span class="o">|</span>      <span class="mi">0</span><span class="o">%</span>      <span class="n">Default</span> <span class="o">|</span>
<span class="o">+-------------------------------+----------------------+----------------------+</span>

<span class="o">+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">Processes</span><span class="o">:</span>                                                       <span class="n">GPU</span> <span class="n">Memory</span> <span class="o">|</span>
<span class="o">|</span>  <span class="n">GPU</span>       <span class="n">PID</span>   <span class="n">Type</span>   <span class="n">Process</span> <span class="n">name</span>                             <span class="n">Usage</span>      <span class="o">|</span>
<span class="o">|=============================================================================|</span>
<span class="o">|</span>  <span class="n">No</span> <span class="n">running</span> <span class="n">processes</span> <span class="n">found</span>                                                 <span class="o">|</span>
<span class="o">+-----------------------------------------------------------------------------+</span>
</pre></div>


<p>gpustat：基于nvidia-smi的监控GPU 小工具</p>
<div class="hlcode"><pre><span class="n">pip</span> <span class="n">install</span> <span class="n">gpustat</span> <span class="err">#</span> <span class="err">通过</span><span class="n">pip</span><span class="err">源安装</span>
<span class="n">watch</span> <span class="o">--</span><span class="n">color</span> <span class="o">-</span><span class="n">n1</span> <span class="n">gpustat</span> <span class="o">-</span><span class="n">cpu</span> <span class="err">#</span> <span class="n">Linux</span> <span class="n">only</span>
</pre></div>


<h4 id="122-amd">1.2.2 AMD 独立显卡</h4>
<h5 id="1221">1.2.2.1. 显卡驱动</h5>
<h5 id="1222-rocm">1.2.2.2. ROCm 平台</h5>
<h5 id="1223">1.2.2.3. 性能监控</h5>
<h3 id="13">1.3 网络通信层</h3>
<p>--（无需额外操作）<br />
Tensorflow 的网络通信层：基于GRPC和RDMA两种通信方式，实现组件间数据通信。</p>
<h4 id="131-grpc">1.3.1 gRPC</h4>
<p>grpc 是由 google 开发，是一款语言中立、平台中立、开源的远程过程调用(RPC)系统。</p>
<h5 id="1311-rpc">1.3.1.1 RPC 系统</h5>
<p>RPC 的全称是 Remote Procedure Call 是一种进程间通信方式。它<code>允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数</code>，而不用程序员显式编码这个远程调用的细节。</p>
<p><strong>即程序员无论是调用本地的还是远程的，本质上编写的调用代码基本相同。</strong></p>
<h6 id="1_1">1. 系统组成</h6>
<div class="hlcode"><pre><span class="n">rpc</span><span class="err">系统包括</span><span class="mi">5</span><span class="err">部分</span><span class="o">:</span>
    <span class="mi">1</span><span class="o">.</span> <span class="n">User</span>
    <span class="mi">2</span><span class="o">.</span> <span class="n">User</span><span class="o">-</span><span class="n">stub</span>
    <span class="mi">3</span><span class="o">.</span> <span class="n">RPCRuntime</span>
    <span class="mi">4</span><span class="o">.</span> <span class="n">Server</span><span class="o">-</span><span class="n">stub</span>
    <span class="mi">5</span><span class="o">.</span> <span class="n">Server</span>
</pre></div>


<p>这里 user 就是 client 端，当 user 想发起一个远程调用时，它实际是通过本地调用 user-stub。user-stub 负责将调用的接口、方法和参数通过约定的协议规范进行编码并通过本地的 RPCRuntime 实例传输到远端的实例。远端 RPCRuntime 实例收到请求后交给 server-stub 进行解码后发起本地端调用，调用结果再返回给 user 端。</p>
<div class="hlcode"><pre><span class="o">!</span><span class="p">[</span><span class="n">avatar</span><span class="p">](.</span><span class="o">/</span><span class="n">iamges</span><span class="o">/</span><span class="n">rpc_stucture</span><span class="p">.</span><span class="n">jpg</span><span class="p">)</span>
</pre></div>


<h6 id="2-rpc">2. rpc 过程</h6>
<p><img alt="avatar" src="https://cn.bing.com/th?id=OIP.ZxToE8_oOyXZw4aKKlrehgHaHH&amp;pid=Api&amp;rs=1&amp;p=0" /></p>
<h6 id="3">3. 服务开启</h6>
<blockquote>
<p>在windows 系统中：<br />
"开始"→"设置"→"控制面板"-&gt;"管理工具"→"服务"-&gt;"remote procedure call (rpc)"，开启rpc服务</p>
<p>在</p>
</blockquote>
<h5 id="1312-grpc">1.3.1.2 gRPC 系统</h5>
<p>理念：<br />
定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。在服务端实现这个接口，并运行一个 gRPC 服务器来处理客户端调用。在客户端拥有一个存根能够像服务端一样的方法。</p>
<p>特性：<br />
1. 基于HTTP/2 <br />
HTTP/2 提供了连接多路复用、双向流、服务器推送、请求优先级、首部压缩等机制。可以节省带宽、降低TCP链接次数、节省CPU，帮助移动设备延长电池寿命等。gRPC 的协议设计上使用了HTTP2 现有的语义，请求和响应的数据使用HTTP Body 发送，其他的控制信息则用Header 表示。<br />
2. IDL使用ProtoBuf <br />
gRPC使用ProtoBuf来定义服务，ProtoBuf是由Google开发的一种数据序列化协议（类似于XML、JSON、hessian）。ProtoBuf能够将数据进行序列化，并广泛应用在数据存储、通信协议等方面。压缩和传输效率高，语法简单，表达力强。<br />
3. 多语言支持（C, C++, Python, PHP, Nodejs, C#, Objective-C、Golang、Java） <br />
gRPC支持多种语言，并能够基于语言自动生成客户端和服务端功能库。目前已提供了C版本grpc、Java版本grpc-java 和 Go版本grpc-go，其它语言的版本正在积极开发中，其中，grpc支持C、C++、Node.js、Python、Ruby、Objective-C、PHP和C#等语言，grpc-java已经支持Android开发。<br />
远程过程调用系统（rpc）</p>
<p>TensorFlow Servering C/S通信约束<br />
TensorFlow Serving以Server方式提供模型能力服务，作为服务的使用者（Client）可以通过gRPC和RESTfull API两种方式来获取模型能力。虽然TensorFlow对C/S的通信约束做了说明，但感觉介绍的并不是特别的清晰易用，需要自己根据使用示例，并结合文档进行梳理和总结。</p>
<h4 id="132-rdma">1.3.2 RDMA</h4>
<p>RDMA是一个远程通讯技术，它通过Kernel bypass等方式降低数据传输中的延迟和CPU消耗。<br />
在分布式训练中，由于多个Worker之间或者Worker和Paramater Server 之间需要大量传输模型变量。当GPU到达一定数量后，受制于网络带宽以及TCP协议的延迟，通讯往往会成为计算性能的瓶颈，而在分布式训练中使用RDMA技术能够非常明显地提高训练速度。</p>
<p>Tensorflow是谷歌开源的深度学习框架，它有丰富的平台支持和API，也可以非常轻松地构建分布式模型训练。<br />
Tensorflow 在实现里支持RDMA作为其分布式场景的通讯协议，但是官方镜像默认没有支持RDMA。需要重新构建tensorflow，并开启RDMA相关的构建参数。 Tensorflow 对 RDMA的支持和实现协议</p>
<p>官方参考示例<br />
在文档中提到了两个参考示例，一个用于gRPC通信约束测试，一个用于RESTfull API通信约束测试。</p>
<p>server = tf.train.Server(cluster, job_name="local", task_index=0, protocol='grpc+verbs') # default protocol is 'grpc'</p>
<ol>
<li>gRPC示例<br />
示例使用说明：https://www.tensorflow.org/serving/serving_basic</li>
</ol>
<p>模型输出<br />
minist_save_model.py<br />
示例代码：https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_saved_model.py<br />
客户端验证<br />
mnist_client.py<br />
示例代码：https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py<br />
2. RESTfull API示例<br />
示例使用说明:https://www.tensorflow.org/serving/api_rest</p>
<p>模型输出<br />
export_half_plus_two.py<br />
示例代码：https://github.com/tensorflow/serving/blob/master/tensorflow_serving/servables/tensorflow/testdata/export_half_plus_two.py<br />
客户端验证<br />
Http请求，通过curl命令完成<br />
示例代码分析<br />
虽然官网提供了两个示例，但实际上RESTfull API的示例过于简单，并且做了封装，实际的参考价值不大。下面主要结合gRPC的示例代码进行分析。</p>
<p>客户端同服务端进行通信交互的核心是几个标识，我们分别介绍下。</p>
<p>有了 gRPC， 我们可以一次性的在一个 .proto 文件中定义服务并使用任何支持它的语言去实现客户端和服务器，反过来，它们可以在各种环境中，</p>
<h2 id="2-tensorflow">2. Tensorflow 安装</h2>
<h3 id="21">2.1 安装准备</h3>
<h4 id="211-python">2.1.1 python 安装</h4>
<p>python的版本由很多，但tensorflow是的操作可以简单认为由python驱动调用底层c++进行计算，对jit版本的python对Tensorflow程序的运行没有明显的影响。Anaconda版本的python 和原生的Cpython 都对Tensorflow 由很好的支持。（主要python版本）</p>
<p>1， Anaconda 安装<br />
https://repo.continuum.io/archive/</p>
<h3 id="22-pip">2.2 pip 安装</h3>
<ol>
<li>从pip官方源下载 安装</li>
</ol>
<div class="hlcode"><pre><span class="n">pip</span> <span class="n">install</span> <span class="n">tensorflow</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">tensorflow</span><span class="o">==</span><span class="mf">2.0.0</span><span class="o">-</span><span class="n">alpha0</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">tf</span><span class="o">-</span><span class="n">nightly</span>
</pre></div>


<ol>
<li>国内pip源加速<br />
在境内地区的国际站点的网络连接速度通常较慢，pip  可通过国内镜像加速</li>
</ol>
<p>清华：https://pypi.tuna.tsinghua.edu.cn/simple<br />
阿里云：http://mirrors.aliyun.com/pypi/simple/<br />
中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/<br />
华中理工大学：http://pypi.hustunique.com/<br />
山东理工大学：http://pypi.sdutlinux.org/ <br />
豆瓣：http://pypi.douban.com/simple/</p>
<div class="hlcode"><pre><span class="err">#</span> <span class="nx">一次性使用</span><span class="err">，</span><span class="nx">在pip</span> <span class="nx">后添加</span> <span class="na">-i</span> <span class="o">&lt;</span><span class="nx">pip</span> <span class="nx">源地址</span><span class="o">&gt;</span><span class="p">:</span>
<span class="err">#</span> <span class="nx">pip</span> <span class="nb">install</span> <span class="o">&lt;</span><span class="nx">package</span><span class="na">-name</span><span class="o">&gt;</span> <span class="na">-i</span> <span class="o">&lt;</span><span class="nx">url_address</span><span class="o">&gt;</span> 
<span class="nx">pip</span> <span class="nb">install</span> <span class="nx">tensorflow</span> <span class="na">-i</span> <span class="nx">https</span><span class="p">:</span><span class="c1">//pypi.tuna.tsinghua.edu.cn/simple</span>
<span class="err">#</span> <span class="ow">or</span>
<span class="nx">pip</span> <span class="nx">config</span> <span class="nb">set</span> <span class="bp">global.</span><span class="nb">index</span><span class="na">-url</span> <span class="nx">https</span><span class="p">:</span><span class="c1">//pypi.tuna.tsinghua.edu.cn/simple</span>
</pre></div>


<ol>
<li>本地whl包 安装</li>
</ol>
<div class="hlcode"><pre><span class="n">pip</span> <span class="n">install</span> <span class="n">xxx</span><span class="p">.</span><span class="n">whl</span>
</pre></div>


<p>在windows 环境下推荐 使用编译好的二进制文件进行安装<br />
参考 https://github.com/fo40225/tensorflow-windows-wheel</p>
<h3 id="23-docker">2.3 docker 安装</h3>
<p>参考：https://tensorflow.google.cn/install/docker</p>
<h4 id="231cpu">2.3.1CPU版本</h4>
<div class="hlcode"><pre><span class="n">docker</span> <span class="n">pull</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">:</span><span class="n">latest</span><span class="o">-</span><span class="n">devel</span><span class="o">-</span><span class="n">gpu</span><span class="o">-</span><span class="n">py3</span> 
</pre></div>


<h4 id="232-gpu">2.3.2 GPU版本</h4>
<p>Docker 是在 GPU 上运行 TensorFlow 的最简单方法，因为主机只需安装 NVIDIA® 驱动程序（无需安装 NVIDIA® CUDA® 工具包）。</p>
<p>检测nvidia GPU是否可用 </p>
<div class="hlcode"><pre><span class="n">lspci</span> <span class="o">|</span> <span class="n">grep</span> <span class="o">-</span><span class="n">i</span> <span class="n">nvidia</span>
</pre></div>


<p>若显卡驱动可用 </p>
<div class="hlcode"><pre><span class="n">docker</span> <span class="n">run</span> <span class="o">--</span><span class="n">runtime</span><span class="o">=</span><span class="n">nvidia</span> <span class="o">-</span><span class="n">it</span> <span class="o">-</span><span class="n">p</span> <span class="mi">8888</span><span class="o">:</span><span class="mi">8888</span> <span class="o">-</span><span class="n">p</span> <span class="mi">6001</span><span class="o">:</span><span class="mi">6001</span> <span class="o">-</span><span class="n">v</span> <span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">notebooks</span><span class="o">:/</span><span class="n">notebooks</span> <span class="o">--</span><span class="n">rm</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">:</span><span class="n">latest</span><span class="o">-</span><span class="n">gpu</span><span class="o">-</span><span class="n">py3</span>
</pre></div>


<h3 id="24">2.4  集群中安装</h3>
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2019 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>
    
</body>

</html>