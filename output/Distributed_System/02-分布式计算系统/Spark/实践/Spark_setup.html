<!DOCTYPE HTML>
<html>

<head>
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/Wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/Wiki/favicon.ico" type="image/x-icon">
    <title>Spark 安装与部署 - Jun's personal knowledge wiki</title>
    <meta name="keywords" content="Technology, MachineLearning, DataMining, Wiki" />
    <meta name="description" content="A wiki website" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
            }
        });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
</head>

<body>

    <div id="container">
        
<div id="header">
  <div id="post-nav"><a href="/Wiki/">Home</a>&nbsp;»&nbsp;<a href="/Wiki/#Distributed_System">Distributed_System</a>&nbsp;»&nbsp;<a href="/Wiki/#-02-分布式计算系统">02-分布式计算系统</a>&nbsp;»&nbsp;<a href="/Wiki/#-Spark">Spark</a>&nbsp;»&nbsp;<a href="/Wiki/#-实践">实践</a>&nbsp;»&nbsp;Spark 安装与部署</div>
</div>
<div class="clearfix"></div>
<div id="title">Spark 安装与部署</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#1">1. 常规部署</a><ul>
<li><a href="#11">1.1. 本地部署</a></li>
<li><a href="#12-spark-on-yarn">1.2. Spark On Yarn</a><ul>
<li><a href="#121">1.2.1. 概念</a><ul>
<li><a href="#1211-mesos">1.2.1.1. Mesos</a></li>
</ul>
</li>
<li><a href="#122-docker">1.2.2. 基于docker</a><ul>
<li><a href="#1221">1.2.2.1. 单击版</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#13-spark">1.3. 搭建spark集群</a><ul>
<li><a href="#13001">1.3.0.0.1. 设置系统路由转发功能</a></li>
<li><a href="#13002">1.3.0.0.2. 创建一个网络</a><ul>
<li><a href="#130021-spark-master">1.3.0.0.2.1. 启动 spark-master</a></li>
<li><a href="#130022-spark-worker">1.3.0.0.2.2. 启动 spark-worker</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#2-spark-on-k8s">2. Spark on k8s</a><ul>
<li><a href="#21-helm-spark-on-k8s">2.1. 通过 Helm 部署 Spark on k8s</a></li>
</ul>
</li>
<li><a href="#spark-on-google-colab">spark on google colab</a></li>
</ul>
</div>
<h1 id="1">1. 常规部署</h1>
<h2 id="11">1.1. 本地部署</h2>
<h2 id="12-spark-on-yarn">1.2. Spark On Yarn</h2>
<h3 id="121">1.2.1. 概念</h3>
<p>Spark on Yarn的两种运行模式：cluster和client;<br />
一句话概述两种的区别就是Spark driver到底运行再什么地方<br />
In cluster mode：Driver运行在NodeManage的AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行。<br />
In client mode:Driver运行在Client上，通过ApplicationMaster向RM获取资源。本地Driver负责与所有的executor container进行交互</p>
<h4 id="1211-mesos">1.2.1.1. Mesos</h4>
<h3 id="122-docker">1.2.2. 基于docker</h3>
<h4 id="1221">1.2.2.1. 单击版</h4>
<p>参考：https://hub.docker.com/r/sequenceiq/spark/</p>
<div class="hlcode"><pre><span class="n">docker</span> <span class="n">pull</span> <span class="n">sequenceiq</span><span class="o">/</span><span class="n">spark</span><span class="o">:</span><span class="mf">1.6.0</span>
</pre></div>


<h2 id="13-spark">1.3. 搭建spark集群</h2>
<h5 id="13001">1.3.0.0.1. 设置系统路由转发功能</h5>
<div class="hlcode"><pre><span class="err">修改配置文件</span> 
<span class="n">vim</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">sysctl</span><span class="p">.</span><span class="n">conf</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="cp">#新增一条：</span>
<span class="n">net</span><span class="p">.</span><span class="n">ipv4</span><span class="p">.</span><span class="n">ip_forward</span><span class="o">=</span><span class="mi">1</span> 
<span class="cp"># 0 时表示禁止进行IP转发</span>
<span class="cp"># 1 IP转发功能已经打开</span>

<span class="cp">#重启网络：</span>
<span class="n">systemctl</span> <span class="n">restart</span> <span class="n">network</span>
<span class="cp">#验证配置：</span>
<span class="n">sysctl</span> <span class="n">net</span><span class="p">.</span><span class="n">ipv4</span><span class="p">.</span><span class="n">ip_forward</span>
</pre></div>


<h5 id="13002">1.3.0.0.2. 创建一个网络</h5>
<p>为本地群集创建一个网络<br />
创建网络非常简单，可以通过运行以下命令来完成：<br />
桥接网络类似于默认bridge网络</p>
<div class="hlcode"><pre><span class="n">docker</span> <span class="n">network</span> <span class="n">create</span> <span class="n">spark_network</span>
</pre></div>


<h6 id="130021-spark-master">1.3.0.0.2.1. 启动 spark-master</h6>
<div class="hlcode"><pre><span class="n">docker</span> <span class="n">run</span> <span class="o">--</span><span class="n">rm</span> <span class="o">-</span><span class="n">it</span> <span class="o">--</span><span class="n">name</span> <span class="n">spark</span><span class="o">-</span><span class="n">master</span> <span class="o">--</span><span class="n">hostname</span> <span class="n">spark</span><span class="o">-</span><span class="n">master</span> \
    <span class="o">-</span><span class="n">p</span> <span class="mi">7077</span><span class="o">:</span><span class="mi">7077</span> <span class="o">-</span><span class="n">p</span> <span class="mi">8080</span><span class="o">:</span><span class="mi">8080</span> <span class="o">--</span><span class="n">network</span> <span class="n">spark_network</span> \
    <span class="o">&lt;</span><span class="n">spark</span><span class="o">-</span><span class="n">image</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span> <span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">sh</span>
</pre></div>


<h6 id="130022-spark-worker">1.3.0.0.2.2. 启动 spark-worker</h6>
<div class="hlcode"><pre><span class="n">docker</span> <span class="n">run</span> <span class="o">--</span><span class="n">rm</span> <span class="o">-</span><span class="n">it</span> <span class="o">--</span><span class="n">name</span> <span class="n">spark</span><span class="o">-</span><span class="n">worker1</span> <span class="o">--</span><span class="n">hostname</span> <span class="n">spark</span><span class="o">-</span><span class="n">worker1</span> \
    <span class="o">--</span><span class="n">network</span> <span class="n">spark_network</span> \
    <span class="o">&lt;</span><span class="n">spark</span><span class="o">-</span><span class="n">image</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span> <span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">sh</span>
<span class="cp"># 输入</span>
<span class="o">/</span><span class="n">spark</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">spark</span><span class="o">-</span><span class="n">class</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="n">deploy</span><span class="p">.</span><span class="n">worker</span><span class="p">.</span><span class="n">Worker</span> \
    <span class="o">--</span><span class="n">webui</span><span class="o">-</span><span class="n">port</span> <span class="mi">8080</span> <span class="n">spark</span><span class="o">:</span><span class="c1">//spark-master:7077</span>
</pre></div>


<h1 id="2-spark-on-k8s">2. Spark on k8s</h1>
<h2 id="21-helm-spark-on-k8s">2.1. 通过 Helm 部署 Spark on k8s</h2>
<p>下面我们将利用 Helm，来部署 Spark 以用于大数据处理。</p>
<p>输入如下命令。</p>
<div class="hlcode"><pre><span class="n">helm</span> <span class="n">install</span> <span class="n">stable</span><span class="o">/</span><span class="n">spark</span> <span class="o">--</span><span class="n">namespace</span> <span class="n">spark</span><span class="o">-</span><span class="n">on</span><span class="o">-</span><span class="n">k8s</span> <span class="o">--</span><span class="n">generate</span><span class="o">-</span><span class="n">name</span>

<span class="cp"># 得到如下的结果。</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="nl">NAME:</span>   <span class="n">myspark</span>
<span class="n">LAST</span> <span class="n">DEPLOYED</span><span class="o">:</span> <span class="n">Mon</span> <span class="n">Nov</span> <span class="mi">20</span> <span class="mi">19</span><span class="o">:</span><span class="mi">24</span><span class="o">:</span><span class="mi">22</span> <span class="mi">2017</span>
<span class="nl">NAMESPACE:</span> <span class="k">default</span>
<span class="nl">STATUS:</span> <span class="n">DEPLOYED</span>
<span class="p">...</span>
</pre></div>


<p>利用如下命令查看 Spark 的 release 和 service。</p>
<div class="hlcode"><pre><span class="n">helm</span> <span class="n">list</span>
<span class="n">kubectl</span> <span class="n">get</span> <span class="n">svc</span>
</pre></div>


<p>利用如下命令查看 Spark 相关的 Pod，并等待其状态变为 Running。因为 Spark 的相关镜像体积较大，所以拉取镜像需要一定的时间。</p>
<div class="hlcode"><pre><span class="n">kubectl</span> <span class="n">get</span> <span class="n">pod</span>
</pre></div>


<p>利用如下命令获得 Spark Web UI 的访问地址。</p>
<div class="hlcode"><pre><span class="n">echo</span> <span class="n">http</span><span class="o">:</span><span class="c1">//$(kubectl get svc myspark-webui -o jsonpath=&#39;{.status.loadBalancer.ingress[0].ip}&#39;):8080</span>
</pre></div>


<p>通过上面的 URL，可以在浏览器上看到 Spark 的 Web UI，上面显示 worker 实例当前为 3 个。</p>
<p>接下来，我们将利用如下命令，使用 Helm 对 Spark 应用做升级，将 worker 实例数量从 3 个变更为 4 个。请注意参数名称是大小写敏感的。</p>
<div class="hlcode"><pre><span class="n">helm</span> <span class="n">upgrade</span> <span class="n">myspark</span> <span class="o">--</span><span class="n">set</span> <span class="s">&quot;Worker.Replicas=4&quot;</span> <span class="n">stable</span><span class="o">/</span><span class="n">spark</span>
<span class="err">得到如下结果。</span>

<span class="n">Release</span> <span class="s">&quot;myspark&quot;</span> <span class="n">has</span> <span class="n">been</span> <span class="n">upgraded</span><span class="p">.</span> <span class="n">Happy</span> <span class="n">Helming</span><span class="o">!</span>
<span class="n">LAST</span> <span class="n">DEPLOYED</span><span class="o">:</span> <span class="n">Mon</span> <span class="n">Nov</span> <span class="mi">20</span> <span class="mi">19</span><span class="o">:</span><span class="mi">27</span><span class="o">:</span><span class="mi">29</span> <span class="mi">2017</span>
<span class="nl">NAMESPACE:</span> <span class="k">default</span>
<span class="nl">STATUS:</span> <span class="n">DEPLOYED</span>
</pre></div>


<p>...<br />
利用如下命令查看 Spark 新增的 Pod，并等待其状态变为 Running。</p>
<div class="hlcode"><pre><span class="n">kubectl</span> <span class="n">get</span> <span class="n">pod</span>
</pre></div>


<p>在浏览器上刷新 Spark 的 Web UI，可以看到此时 worker 数量已经变为 4 个。</p>
<p>如需彻底删除 Spark 应用，可输入如下命令。</p>
<div class="hlcode"><pre><span class="n">helm</span> <span class="n">delete</span> <span class="o">--</span><span class="n">purge</span> <span class="n">myspark</span>
</pre></div>


<h1 id="spark-on-google-colab">spark on google colab</h1>
<div class="hlcode"><pre><span class="err">如何在调试代码</span>

<span class="err">最近由于部门有个项目涉及到</span><span class="n">ETL</span><span class="err">，就想学一下</span><span class="n">spark</span><span class="err">，本来没打算处理环境安装的问题，就直接上了</span><span class="n">databricks</span><span class="err">上面的免费</span><span class="n">notebook</span><span class="err">环境，并且照着那上面的教程把</span><span class="n">spark</span> <span class="n">get</span> <span class="n">started</span><span class="err">走了一遍，后面想继续学习的时候越来越感觉到</span><span class="n">databricks</span><span class="err">的</span><span class="n">notebook</span><span class="err">不够友好，比</span><span class="n">jupyter</span> <span class="n">Notebook</span><span class="err">难操作。遂想到</span><span class="n">colab</span><span class="err">的</span><span class="n">notebook</span><span class="err">不错，便萌生想法在</span><span class="n">colab</span><span class="err">上运行</span><span class="n">spark</span><span class="err">代码，但是</span><span class="n">colab</span><span class="err">不像</span><span class="n">databricks</span><span class="err">的</span><span class="n">notebook</span><span class="err">预装了</span><span class="n">spark</span><span class="err">相关库，需要自己安装。在搜索了一通之后找了个方案调试了很久才调通，先把调通的</span><span class="n">notebook</span><span class="err">上预装环境的代码贴下来，目前可以直接使用，后续可能需要更改下面某些源程序的版本号。</span>

<span class="err">先放代码：</span>

<span class="o">!</span><span class="n">apt</span> <span class="n">install</span> <span class="n">openjdk</span><span class="o">-</span><span class="mi">8</span><span class="o">-</span><span class="n">jdk</span><span class="o">-</span><span class="n">headless</span>
<span class="o">!</span><span class="n">wget</span> <span class="o">-</span><span class="n">q</span> <span class="n">https</span><span class="o">:</span><span class="c1">//www-us.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz</span>
<span class="o">!</span><span class="n">tar</span> <span class="n">xf</span> <span class="n">spark</span><span class="o">-</span><span class="mf">2.4.3</span><span class="o">-</span><span class="n">bin</span><span class="o">-</span><span class="n">hadoop2</span><span class="mf">.7</span><span class="p">.</span><span class="n">tgz</span>

<span class="o">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">findspark</span>
<span class="n">import</span> <span class="n">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">&quot;JAVA_HOME&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s">&quot;/usr/lib/jvm/java-8-openjdk-amd64&quot;</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">&quot;SPARK_HOME&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s">&quot;/content/spark-2.4.3-bin-hadoop2.7&quot;</span>
<span class="o">!</span><span class="n">update</span><span class="o">-</span><span class="n">alternatives</span> <span class="o">--</span><span class="n">config</span> <span class="n">java</span>
<span class="cp">#select openjdk1.8  enter</span>

<span class="n">import</span> <span class="n">findspark</span>
<span class="n">findspark</span><span class="p">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">from</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">sql</span> <span class="n">import</span> <span class="n">SparkSession</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">&quot;local[*]&quot;</span><span class="p">).</span><span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>
</div>
<div id="renote">
  <HR style=" FILTER: alpha (opacity = 100, finishopacity =0 , style= 3 )" width="80%" color=#987 cb 9 SIZE=3>
  <p>如果你觉得这篇文章对你有帮助，不妨请我喝杯咖啡，鼓励我创造更多!</p>
  <img src="/Wiki/static/images/pay.jpg" width="25%">
</div>

    </div>
    <div id="footer">
        <span>
            Copyright © 2021 zhang787jun.
            Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        </span>
    </div>

    
</body>
<script>
    function changeImgurl(site_root_url) {
        var images = document.images;
        var site_root = site_root_url;
        for (i = 0, len = images.length; i < len; i++) {
            image = images[i];
            image_src = image.src;
            if (image_src.search("attach") >= 0) {
                re_image_src = image_src.slice(image_src.search("attach"));
                abs_image_src = (site_root.endsWith("/")) ? site_root + re_image_src : site_root + "/" +
                    re_image_src;
                image.src = abs_image_src;
            }
        }
    }
    var site_root_url = "/Wiki";
    changeImgurl(site_root_url);
    let isMathjaxConfig = false; // 防止重复调用Config，造成性能损耗
    const initMathjaxConfig = () => {
        if (!window.MathJax) {
            return;
        }
        window.MathJax.Hub.Config({
            showProcessingMessages: false, //关闭js加载过程信息
            messageStyle: "none", //不显示信息
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
                displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
                skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
            },
            "HTML-CSS": {
                availableFonts: ["STIX", "TeX"], //可选字体
                showMathMenu: false //关闭右击菜单显示
            }
        });
        isMathjaxConfig = true; //
    };
    if (isMathjaxConfig === false) {
        // 如果：没有配置MathJax
        initMathjaxConfig();
    };
</script>

</html>